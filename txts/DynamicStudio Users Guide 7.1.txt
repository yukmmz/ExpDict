User's Guide
DynamicStudio
This document may not be copied, photocopied, translated, modified, or reduced to any
electronic medium or machine-readable form, in whole, or in part, without the prior writ-
ten consent of Dantec Dynamics A/S.
Build no.: 7.1.0076. Publication no.: 9040U1872. Date: 9/11/2020. c by Dantec Dynamics
A/S, P.O. Box 121, Tonsbakken 18, DK?2740 Skovlunde, Denmark. All rights reserved.
All trademarks referred to in this document are registered by their owners.
Disposal of Electronic Equipment (WEEE)
Important
Electronic equipment should not be disposed of together
with other waste. The user must dispose electronic waste
according to local rules and regulations.
I
Table of Content
Table of Content                                                                                                                               i
1 Introduction                                                                                                                                 19
1.1 The DynamicStudio Software                                                                                                      19
1.1.1 Assumptions                                                                                                                          19
2 Related Documentation                                                                                                       21
3 Contacting Dantec Dynamics                                                                                          23
4 Software License Agreement                                                                                            25
5 Laser Safety                                                                                                                                  29
5.1 Danger of Laser Light                                                                                                                  29
5.2 Precautions                                                                                                                                   29
5.3 Laser Safety Poster                                                                                                                      30
6 Requirements                                                                                                                              31
6.1 Minimum requirements                                                                                                              31
6.2 Recommended configuration                                                                                                    31
7 Note to Administrators                                                                                                        33
7.0.1 Installation                                                                                                                             33
7.1 Windows Firewall                                                                                                                          33
7.2 BCDEDIT.exe (MicrosoftR Windowsc 10)                                                                                33
8 Software Update Agreement (SUA)                                                                              35
9 Acknowledgement third party software components                                    37
9.1 IPP and MKL from Intel                                                                                                               37
9.2 OpenCV                                                                                                                                          39
9.3 ANN University of Maryland, Sunil Arya and David Mount                                                    39
9.4 Boost C++ libraries                                                                                                                       48
9.5 NVIDIA CUDA Toolkit                                                                                                                   49
9.6 muParser from Ingo Berg                                                                                                          49
9.7 Tiflib                                                                                                                                                50
9.8 VTK                                                                                                                                                  51
9.9 Math.net Numerics                                                                                                                      52
10 Add-Ons and Options for DynamicStudio                                                             53
11 Legacy Options                                                                                                                        55
12 Getting Started                                                                                                                       57
12.1 Warranties and Disclaimers                                                                                                      57
12.1.1 Camera Sensor Warranty                                                                                                  57
12.2 Installation                                                                                                                                   57
12.2.1 Multiple installations of DynamicStudio                                                                          58
12.3 Using DynamicStudio                                                                                                                 58
12.3.1 User Interface                                                                                                                     58
II
12.3.2 First Time Use                                                                                                                      69
12.3.3 Image Buffer?resources                                                                                                    75
12.3.4 Normal Use                                                                                                                          76
12.3.5 Database Access                                                                                                                 87
12.3.6 Delete and Restore                                                                                                            90
12.3.7 Working with the Database                                                                                               93
12.3.8 Ensemble Linking                                                                                                                97
12.3.9 Calibration Images                                                                                                           102
12.3.10 AgentHost service                                                                                                         104
12.3.11 Known issues                                                                                                                  105
12.3.12 Database with Demo Data                                                                                            108
12.3.13 Combined PIV-DIC measurements                                                                             109
13 Acquisition                                                                                                                               115
13.1 Acquisition Manager                                                                                                                115
13.1.1 Opening the Acquisition Manger                                                                                   115
13.1.2 Description of possible settings                                                                                     115
13.1.3 Buttons in Acquisition Manager dialog                                                                         116
13.1.4 Working with rows in the Acquisition Manager                                                           117
13.1.5 Naming convention                                                                                                          118
13.1.6 Generate Grid                                                                                                                  118
13.1.7 Selecting Analysis seqeunce                                                                                           119
13.1.8 Troubleshooting                                                                                                               120
13.2 Device registration                                                                                                                  120
13.2.1 Registering 3rd party devices to unlock and enable full control of them via
DynamicStudio                                                                                                                              121
13.2.2 Requirements                                                                                                                   121
13.2.3 How to register                                                                                                                 121
13.2.4 Receiving the “RegisteredDeviceKeys.zip” file                                                            123
13.3 Reporting acquisition settings                                                                                               124
13.4 Storing and loading acquisition settings                                                                               125
13.5 Imaging Setup Assistant (ISA)                                                                                                126
13.5.1 ISA Step-by-Step instructions                                                                                         126
13.5.2 The Medium Bounds and Medium Refractive Index                                                  128
13.5.3 The Measurement Volume - Center and Size                                                              128
13.5.4 Hardware configuration                                                                                                  129
13.5.5 2 Examples how to use ISA to configure, plan for and setup an PIV experiment. 137
13.6 Remote Controlled Acquisition                                                                                              143
13.6.1 How to start Remote Controlled Acquisition                                                               144
13.6.2 Infomation on the Remote Control Driver                                                                  145
13.6.3 Commands                                                                                                                        147
13.7 PIV Simulator                                                                                                                            149
13.7.1 Running the experiment.                                                                                                149
III
13.7.2 Configuring                                                                                                                       150
13.7.3 Adjusting the particle engine properties                                                                     151
13.7.4 Adjusting the camera                                                                                                      152
13.7.5 Calibration images                                                                                                            153
13.8 Online Vectors                                                                                                                          154
13.8.1 Adding Online Vectors to be performed during Acquisition                                     155
13.8.2 Parameters for Online Vectors                                                                                      155
13.9 Adding the Focus Assist online analysis function to the setup                                          156
13.10 Understanding the resulting image from Online Focus Assist                                       157
13.11 Using Online Focus Assist to set up a stereo PIV system                                                160
13.12 Parameters for the device Online Focus Assist                                                                164
13.12.1 Pixel dilation Diameter                                                                                                  164
13.13 Online Light-Field                                                                                                                   164
13.13.1 Online data analysis                                                                                                       164
13.13.2 Acquiring data to disk                                                                                                    164
13.14 Synthetic image generator                                                                                                  165
13.14.1 Introduction:                                                                                                                   165
14 Devices                                                                                                                                        170
14.1 Lens Mounts                                                                                                                             170
14.1.1 Remote lens control                                                                                                        170
14.1.2 SpeedSense Ethernet camera Canon lens mount                                                       174
14.2 1-Axis Scheimpflug mounts                                                                                                    176
14.2.1 Instructions for mounting the FlowSense EO cameras                                              177
14.2.2 Scheimpflug 1-Axis                                                                                                           179
14.2.3 Instructions for mounting the SpeedSense 1040 cameras                                      181
14.2.4 Scheimpflug 1-Axis                                                                                                           184
14.3 Image Intensifiers                                                                                                                   186
14.3.1 Custom Image Intensifier                                                                                               186
14.3.2 Hamamatsu Image Intensifier                                                                                       189
14.4 Analog Input                                                                                                                             190
14.4.1 Analog Input                                                                                                                     190
14.4.2 Installation                                                                                                                         190
14.4.3 Connecting hardware for acquisition                                                                            191
14.4.4 Preparing software for acquisition                                                                                192
14.4.5 Acquiring data                                                                                                                   195
14.5 Cameras                                                                                                                                    195
14.5.1 HiSense MkI                                                                                                                      195
14.5.2 HiSense MkII Camera                                                                                                      196
14.5.3 HiSense NEO and Zyla Camera                                                                                       197
14.5.4 FlowSense 2M Camera                                                                                                    203
14.5.5 FlowSense 4M Camera                                                                                                    205
14.5.6 FlowSense CM 12M-70                                                                                                    208
IV
14.5.7 FlowSense CX 4M-563                                                                                                     210
14.5.8 Parameters for the camera                                                                                            212
14.5.9 FlowSense CX 12M-160                                                                                                   216
14.5.10 Parameters for the camera                                                                                         218
14.5.11 FlowSense CX 25M-80                                                                                                   221
14.5.12 Parameters for the camera                                                                                         223
14.5.13 FlowSense EO Camera series                                                                                       226
14.5.14 FlowSense USB 2M-165                                                                                                232
14.5.15 HiSense 4M Camera                                                                                                      236
14.5.16 HiSense 11M Camera                                                                                                    236
14.5.17 HiSense 6XX cameras                                                                                                    236
14.5.18 Image buffer recourses                                                                                               238
14.5.19 Streaming to disk                                                                                                           240
14.5.20 NanoSense Camera Series                                                                                           243
14.5.21 Photron Camera Series                                                                                                 247
14.5.22 SpeedSense 10XX series                                                                                              259
14.5.23 SpeedSense 1040 camera                                                                                            263
14.5.24 Parameters for the camera                                                                                         264
14.5.25 SpeedSense Ethernet Cameras                                                                                   267
14.5.26 PCO?Dimax cameras                                                                                                      284
14.6 Synchronizers                                                                                                                           285
14.6.1 High Performance Synchronizer and Performance Synchronizer                           286
14.6.2 Setting up the Synchronizer for PIV?measurements                                                 317
14.6.3 Synchronization setup                                                                                                     319
14.6.4 Running two cameras synchronized but at different trigger rate                           323
14.6.5 Additional settings                                                                                                           326
14.6.6 How to use the TrueTime Add-on and the Light Trap                                                326
14.6.7 Scanning Light Sheet controller                                                                                     330
14.6.8 Synchronization                                                                                                                333
14.6.9 USB Timing HUB                                                                                                               336
14.6.10 Start options                                                                                                                   337
14.6.11 Mode options                                                                                                                 338
14.6.12 Limitations                                                                                                                       339
14.6.13 High Resolution Synchronizer                                                                                      340
14.6.14 Timer Box                                                                                                                        344
14.6.15 Installing the TimerBox                                                                                                 344
14.6.16 Connecting the timer box                                                                                             345
14.6.17 Synchronization setup                                                                                                   347
14.6.18 Synchronizing two TimerBoxes                                                                                   348
14.6.19 Additional settings                                                                                                         349
14.6.20 Cyclic Synchronizer and Linear Synchronizer                                                            350
14.6.21 Using Two Synchronizers                                                                                              366
V
14.6.22 Pulse Receiver                                                                                                                367
14.7 Illumination Systems                                                                                                               369
14.7.1 Brilliant B laser                                                                                                                  369
14.7.2 DualPower Lasers                                                                                                            371
14.7.3 Time resolved lasers                                                                                                        372
14.7.4 Evergreen Laser                                                                                                               373
14.7.5 Lee Lasers                                                                                                                         375
14.7.6 Microstrobe                                                                                                                       375
14.7.7 New Wave Lasers                                                                                                             376
14.7.8 Pulsed Lasers                                                                                                                    376
14.7.9 Dual Cavity pulsed lasers                                                                                                 376
14.7.10 Shutter Devices                                                                                                              377
14.7.11 Tunable Dye Laser                                                                                                         377
14.7.12 Quantel Q-smart laser                                                                                                   379
14.8 Microscopes                                                                                                                              382
14.8.1 Leica M165FC microscope                                                                                               382
14.9 Traverse systems                                                                                                                     386
14.9.1 Traverse Control                                                                                                              386
14.10 Other Devices                                                                                                                        400
14.10.1 Active Target                                                                                                                  400
14.10.2 DualScope                                                                                                                       408
15 Analysis                                                                                                                                      411
15.1 Analysis Sequence                                                                                                                   411
15.1.1 "Pickup" an Analysis Sequence                                                                                       411
15.2 Analysis Sequence Library                                                                                                      411
15.3 Using Analysis Sequences                                                                                                      412
15.4 Predefined Analysis Sequences                                                                                            413
15.4.1 Use of Analysis Sequences                                                                                             413
15.5 Context menu                                                                                                                          413
15.6 Distributed Analysis                                                                                                                 414
15.6.1 Installing Analysis Agent software                                                                                 414
15.6.2 Configuring Distributed Analysis.                                                                                  414
15.6.3 Analyzing using Distributed Analysis                                                                             415
15.7 Distributed Database                                                                                                              417
15.7.1 Collecting remotely stored data                                                                                    418
15.7.2 Troubleshooting Distributed Database                                                                        418
15.8 Custom Properties                                                                                                                  419
15.8.1 Example                                                                                                                              419
15.9 Timestamp                                                                                                                                419
15.10 Selection (Input to Analysis)                                                                                                 419
15.10.1 Example                                                                                                                           419
15.11 Fixed Selection (Input to Analysis)                                                                                      419
VI
16 Analysis methods                                                                                                                421
16.1 2D Least squares matching (LSM)                                                                                         421
16.1.1 Background information                                                                                                 421
16.1.2 Usage                                                                                                                                 422
16.1.3 References                                                                                                                        423
16.2 2-Frame 2D PTV                                                                                                                       424
16.2.1 1. How it works                                                                                                                 424
16.2.2 2. Graphical Use Interface                                                                                              424
16.2.3 3. Typical use                                                                                                                     425
16.3 Time-Resolved 2D PTV                                                                                                            426
16.3.1 Applying the Time-Resolved 2D PTV                                                                             427
16.3.2 Interface and settings                                                                                                     427
16.4 3D Particle Tracking Velocimetry                                                                                          428
16.4.1 Applying the 3D Particle Tracking Velocimetry                                                           429
16.4.2 Interface and settings                                                                                                     430
16.4.3 Analysis process                                                                                                               431
16.5 Adaptive PIV                                                                                                                             433
16.5.1 Interrogation areas                                                                                                         434
16.5.2 Windowing and Filtering                                                                                                  434
16.5.3 Validation                                                                                                                           436
16.5.4 Adaptivity                                                                                                                           436
16.5.5 Diagnostic                                                                                                                          438
16.5.6 Reference Vector Map                                                                                                    440
16.6 Calibrate Analog inputs                                                                                                           440
16.6.1 How to set calibration values?                                                                                        440
16.6.2 Calibration                                                                                                                         441
16.7 Auto-Mask?Generator                                                                                                             442
16.7.1 Theory                                                                                                                                442
16.7.2 Automask Generator implemented in DynamicStudio                                              443
16.7.3 Pitfalls, Tips &?Tricks                                                                                                         445
16.7.4 References                                                                                                                        447
16.8 When to use Average Correlation?                                                                                       447
16.8.1 Using Average Correlation to look at average PIV signal conditions                       447
16.8.2 Using an offset vector map to improve the results                                                    448
16.8.3 Using preconditioning to minimize influence from out of focus particles              449
16.8.4 Schematic of Average Correlation                                                                                449
16.9 Average Filter                                                                                                                           452
16.10 Calibration refinement                                                                                                         454
16.10.1 Required input                                                                                                                455
16.10.2 Refinement Area                                                                                                           456
16.10.3 Frame                                                                                                                               457
16.10.4 Interrogation Area Size                                                                                                457
VII
16.10.5 Disparity Map                                                                                                                 458
16.10.6 Dewarped Particle Images                                                                                           458
16.10.7 Average Correlation Map                                                                                             459
16.10.8 Interpreting correlation maps                                                                                    460
16.10.9 Interpreting Disparity Vectors                                                                                    460
16.10.10 Change of Coordinate System                                                                                   462
16.11 Coherence Filter                                                                                                                    463
16.12 Combine Tracks                                                                                                                     465
16.12.1 Inputs:                                                                                                                              466
16.12.2 Outputs:                                                                                                                           467
16.12.3 Settings and how to:                                                                                                      468
16.13 Combustion LIF processing                                                                                                  468
16.13.1 Image analysis                                                                                                                468
16.13.2 Interpretation of the results                                                                                       470
16.14 Cross-Correlation (Legacy method)                                                                                    470
16.14.1 Cross-correlating single images                                                                                  471
16.15 Curve fit processing                                                                                                              471
16.15.1 Curve fitting procedure                                                                                                471
16.15.2 Open data fit as numeric                                                                                               473
16.15.3 Overview on non-linear fit models                                                                              473
16.16 Cylindrical coordinate transform                                                                                        474
16.17 Define Mask                                                                                                                           474
16.17.1 Adding shapes to the mask                                                                                          476
16.17.2 Deleting shapes                                                                                                             477
16.17.3 Selecting multiple shapes                                                                                             477
16.17.4 Ordering shapes                                                                                                            477
16.17.5 Preview the final mask                                                                                                  477
16.17.6 Vector map Overlay                                                                                                       477
16.18 Diameter Statistics                                                                                                                477
16.19 Extract                                                                                                                                     480
16.19.1 Examples                                                                                                                         480
16.20 Feature Tracking                                                                                                                    483
16.21 FeaturePIV                                                                                                                             486
16.22 FlexPIV processing                                                                                                                488
16.22.1 Defining grid points                                                                                                       490
16.22.2 Defining vector analysis                                                                                                491
16.23 Grid Interpolation                                                                                                                 491
16.24 Histogram                                                                                                                               493
16.24.1 The Recipe dialog                                                                                                           494
16.25 Image Arithmetic                                                                                                                  496
16.25.1 Image arithmetic with another image                                                                        498
16.25.2 Image arithmetic with a constant value                                                                      498
VIII
16.25.3 Thresholding                                                                                                                   499
16.26 Image Balancing                                                                                                                    499
16.27 Image Dewarping                                                                                                                 501
16.27.1 Dewarping image maps                                                                                                501
16.27.2 Recipe dialog: Imaging Model Fit (camera calibration)                                             503
16.27.3 Recipe dialog: Re-sampling grid                                                                                   503
16.27.4 Recipe dialog: Re-sampling scheme                                                                            504
16.27.5 Recipe dialog: Fill color outside image                                                                        504
16.27.6 Recipe dialog: Z-coordinate                                                                                          505
16.28 Image Histogram                                                                                                                  505
16.28.1 Using ROI                                                                                                                         505
16.28.2 Manipulating the ROI rectangle using the mouse.                                                   506
16.28.3 Setting the ROI rectangle using the property dialog.                                              507
16.28.4                                                                                                                                           507
16.28.5 Input Image types                                                                                                         507
16.28.6 Using a Mask                                                                                                                   507
16.29 Image Masking                                                                                                                      508
16.30 Image & Volume Math                                                                                                          512
16.30.1 Inputs                                                                                                                              513
16.30.2 Scalars                                                                                                                              514
16.30.3 Functions                                                                                                                         514
16.30.4 Operators                                                                                                                        516
16.30.5 Error description                                                                                                            517
16.30.6 Output Selection                                                                                                            518
16.30.7 Example                                                                                                                           518
16.30.8 Advanced processing                                                                                                    520
16.31 Image Mean                                                                                                                           520
16.31.1 Application example                                                                                                      520
16.32 Image Min/Mean/Max                                                                                                          521
16.33 Image Processing Library (IPL)                                                                                           525
16.33.1 Low-pass filters                                                                                                               526
16.33.2 High-pass filters                                                                                                             528
16.33.3 Morphology filters                                                                                                         531
16.33.4 Thresholding                                                                                                                   533
16.33.5 Utility filters                                                                                                                    534
16.33.6 Signal processing                                                                                                           539
16.33.7 Custom filter                                                                                                                   539
16.34 Image Resampling                                                                                                                 545
16.34.1 Re-sampling window                                                                                                      545
16.34.2 Re-sampled maps                                                                                                           546
16.35 Image Resolution                                                                                                                  546
16.36 Image RMS                                                                                                                             547
IX
16.37 Image Stitching                                                                                                                      548
16.38 Image Distortion Correction                                                                                               550
16.38.1 Determine transformation grid from target image                                                 550
16.38.2 Use transformation grid to correct image distortion                                              556
16.39 Imaging model fit (Legacy Method)                                                                                   556
16.39.1 Normal use and background                                                                                        557
16.39.2 Target library & Custom targets                                                                                  560
16.39.3 Acquiring calibration images                                                                                        563
16.39.4 The recipe for Imaging Model Fit                                                                                566
16.39.5 Displaying imaging model parameters                                                                       569
16.39.6 Direct Linear Transform (DLT)                                                                                     571
16.39.7 3'rd order XYZ polynomial imaging model fit                                                            572
16.39.8 Pinhole camera model                                                                                                  573
16.39.9 Telecentric camera model                                                                                            576
16.39.10 Adjusting parameters for finding the dot matrix target                                       580
16.39.11 Image Processing Parameters                                                                                  582
16.39.12 Imaging model fit equals Camera calibration                                                          585
16.40 Multi Camera Calibration                                                                                                      585
16.40.1 Target Types & Parameters                                                                                         589
16.40.2 Dual level and dual sided targets                                                                                590
16.40.3 Automatic/Semi-automatic Calibration                                                                       591
16.40.4 Active Target Calibration                                                                                              592
16.40.5 Manual Calibration                                                                                                         592
16.41 Imaging Model Fit Import                                                                                                    596
16.42 IPI Processing                                                                                                                        597
16.42.1 Content                                                                                                                            597
16.42.2 User Interface                                                                                                                598
16.42.3 Calibration                                                                                                                       599
16.42.4 Recipe <IPI Processing>                                                                                               600
16.42.5 General                                                                                                                            600
16.42.6 Optical Setup                                                                                                                   601
16.42.7 Velocity Setup                                                                                                                 602
16.42.8 Advanced Settings                                                                                                         603
16.42.9 Region Of Interest (ROI)/ Validation                                                                           604
16.42.10 Window Setup                                                                                                              605
16.42.11 Filter                                                                                                                               606
16.42.12 Laser setup                                                                                                                   608
16.42.13 Processing and Presentation                                                                                     608
16.42.14 Post processing                                                                                                            613
16.42.15 Example                                                                                                                         614
16.42.16 Trouble shooting guide                                                                                               615
16.43 IPI Spatial Histogram                                                                                                            622
X
16.43.1 Process                                                                                                                            623
16.44 3D Least Squares Matching                                                                                                 624
16.44.1 Introduction                                                                                                                    624
16.44.2 The Least Squares Matching Recipe                                                                            625
16.44.3 Results of the analysis                                                                                                   630
16.44.4 References                                                                                                                     631
16.45 LIEF Processing                                                                                                                      631
16.45.1 1. LIEF spray analysis                                                                                                     631
16.45.2 1.2 Spatial calibration of two cameras                                                                        634
16.45.3 1.3 Launching the LIEF Processing analysis method                                                634
16.45.4 1.4 Determining and correcting for the cross-talk                                                   635
16.46 LIF Calibration                                                                                                                        636
16.46.1 Custom properties of the calibration images                                                            636
16.46.2 Performing the calibration                                                                                           639
16.47 LIF Processing                                                                                                                        642
16.48 1. Mie-LIF SMD - General                                                                                                     646
16.49 2. SMD Calibration                                                                                                                 647
16.50 3. SMD Process                                                                                                                      653
16.51 LII Calibration                                                                                                                         656
16.52 LII Gas composition calibration                                                                                           657
16.53 LII Processing                                                                                                                        659
16.53.1 LII data processing by Region-of-Interest <ROI> methodology                            659
16.53.2 LII data processing by Line-of-Sight (LoS) methodology                                         661
16.54 Make Double Frame                                                                                                              662
16.55 Make Single Frame                                                                                                                663
16.56 Make Reverse Frame                                                                                                            663
16.57 MATLAB Link                                                                                                                          664
16.57.1 Contents                                                                                                                          664
16.57.2 Recipe for the MATLAB Link                                                                                         664
16.57.3 Selecting data for transfer to MATLAB                                                                      667
16.57.4 DynamicStudio data in MATLAB's workspace                                                            668
16.57.5 Parameter String                                                                                                           672
16.57.6 General                                                                                                                            672
16.57.7 The Output variable                                                                                                       672
16.58 Troubleshooting, Tips &?Tricks                                                                                            675
16.59 Moving Average Validation                                                                                                  678
16.59.1 Using the <Moving-average validation > method                                                    678
16.60 N-Sigma Validation                                                                                                                 680
16.61 Octave Link                                                                                                                             684
16.61.1 Contents                                                                                                                          684
16.61.2 Recipe for the Octave Link                                                                                           684
16.61.3 Selecting data for transfer to Octave                                                                         688
XI
16.61.4 DynamicStudio data in Octave's workspace                                                               688
16.61.5 The Output variable                                                                                                       688
16.62 Oscillating Pattern Decomposition                                                                                     689
16.62.1 Numeric Export of OPD results                                                                                   695
16.62.2 References                                                                                                                     698
16.63 Particle Tracking Velocimetry (PTV)                                                                                    699
16.64 Peak Validation                                                                                                                       700
16.64.1 Interactive setting and finding good parameters                                                    701
16.64.2 Example using the peak validation for phase separation                                        701
16.65 Phase Boundary Detection                                                                                                  703
16.65.1 A) Preview settings                                                                                                        703
16.65.2 B) Global threshold                                                                                                        704
16.65.3 C) Local threshold                                                                                                           704
16.65.4 D) Filters                                                                                                                          704
16.65.5 E) Output                                                                                                                         705
16.65.6 F) Phase label                                                                                                                  706
16.65.7 Publications:                                                                                                                    706
16.66 Dynamic Masking                                                                                                                   706
16.67 PIV Uncertainty                                                                                                                      707
16.67.1 Rayleigh distribution                                                                                                      709
16.67.2 Methodology                                                                                                                  710
16.67.3 Error messages                                                                                                              711
16.67.4 Example                                                                                                                           712
16.67.5 Expanded uncertainties                                                                                                717
16.68 Proper Orthogonal Decomposition (POD Analysis)                                                         718
16.68.1 Theory                                                                                                                             718
16.68.2 Supported input                                                                                                             719
16.68.3 Handling the Mean                                                                                                        719
16.68.4 Handling Outliers and Masked out data                                                                     719
16.68.5 Mode Count                                                                                                                    720
16.68.6 Recipe settings                                                                                                               720
16.68.7 Input requirements                                                                                                      737
16.68.8 Numeric Export of POD results                                                                                   737
16.69 POD Reconstruction                                                                                                              742
16.70 Pressure from PIV                                                                                                                 746
16.70.1 Define Objects                                                                                                                747
16.70.2 Pressure from PIV                                                                                                         749
16.70.3 Solver                                                                                                                               752
16.70.4 Important considerations                                                                                             754
16.71 Probability Distribution                                                                                                        755
16.71.1 Define and apply a mask                                                                                               755
16.71.2 Distribution processing                                                                                                 755
XII
16.71.3 More about ROI data analysis                                                                                      756
16.72 Profile plot                                                                                                                              757
16.72.1 Detailed description                                                                                                      758
16.72.2 A handy shortcut                                                                                                            761
16.72.3 Obtaining the numerical values                                                                                   762
16.72.4 Examples                                                                                                                         763
16.73 Range Validation                                                                                                                    764
16.74 Rayleigh Thermometry                                                                                                         766
16.74.1 Rayleigh theory                                                                                                              766
16.74.2 Rayleigh Thermometry analysis in DynamicStudio                                                   767
16.74.3 Species and Mixture Library                                                                                         774
16.75 Reynolds Flux                                                                                                                         775
16.75.1 Image re-sampling                                                                                                         776
16.75.2 Reynolds flux calculations                                                                                             776
16.76 Rigid Object Tracking and Stabilization                                                                               776
16.76.1 Introduction                                                                                                                    776
16.76.2 Rigid Object Tracking                                                                                                     777
16.76.3 Rigid Object Stabilization                                                                                               778
16.77 Region of Interest (ROI) Extract                                                                                          779
16.77.1 Manipulating the ROI rectangle using the mouse.                                                   780
16.77.2 Setting the ROI rectangle using the property dialog.                                              781
16.77.3 Using the image view.                                                                                                   781
16.78 Scalar Conversion                                                                                                                  782
16.79 Scalar derivatives                                                                                                                   783
16.79.1 Calculating the gradients of U, V and W in the x and y direction                            784
16.79.2 Scalar derivatives that can be calculated                                                                    785
16.79.3 Propagating uncertainties                                                                                            787
16.80 Scalar Map                                                                                                                               788
16.80.1 Visualization methods…                                                                                                 790
16.81 Scalar statistics                                                                                                                       792
16.82 Shadow Combine                                                                                                                   795
16.83 Shadow Histogram                                                                                                                796
16.83.1 Variable to process                                                                                                        797
16.83.2 Process data from                                                                                                          798
16.83.3 Display                                                                                                                             798
16.83.4 Scaling                                                                                                                              798
16.83.5 Region                                                                                                                              799
16.83.6 Histogram display properties                                                                                      799
16.84 Shadow Processing (Legacy method)                                                                                 800
16.84.1 Content                                                                                                                            801
16.84.2 Field of view and calibration                                                                                         801
16.84.3 Recipe <Shadow Sizing>                                                                                                802
XIII
16.84.4 Data visualization                                                                                                           810
16.85 Shadow Resample                                                                                                                  811
16.86 Shadow Spatial Histogram                                                                                                    812
16.86.1 Calculation                                                                                                                       813
16.86.2 Number of cells                                                                                                              813
16.86.3 Process                                                                                                                            813
16.87 Shadow Tracking                                                                                                                    813
16.87.1 General description of the algorithm                                                                         813
16.87.2 Dialog window (recipe)                                                                                                  814
16.87.3 Data Visualization                                                                                                           817
16.88 Shadow Validation                                                                                                                 817
16.89 Size-velocity correlation                                                                                                       818
16.90 Spectrum                                                                                                                                820
16.90.1 Example: Spectrum in the wake of a cylinder                                                            821
16.91 Spray Geometry                                                                                                                    826
16.91.1 Introduction                                                                                                                    826
16.91.2 Setup window                                                                                                                 827
16.91.3 Spray nozzle properties                                                                                                828
16.91.4 Region of interest                                                                                                          830
16.91.5 Cone Geometry - Plume geometry                                                                             830
16.91.6 Spray Pattern                                                                                                                 832
16.91.7 Spray geometry processing - Temporal evolution                                                   834
16.91.8 Trouble shooting                                                                                                            837
16.92 Stereo-PIV                                                                                                                              838
16.92.1 Method and formulas                                                                                                    839
16.92.2 Input Required                                                                                                               841
16.92.3 Recipe for Stereo PIV?processing                                                                                841
16.92.4 Displaying results                                                                                                           843
16.93 Subpixel Analysis                                                                                                                   845
16.94 Temporal Smoothing                                                                                                            847
16.94.1 A) Smoothing settings                                                                                                   847
16.94.2 B) Values to smooth                                                                                                       848
16.94.3 C) Preview                                                                                                                       848
16.95 Tomographic Particle Tracking Velocimetry                                                                      848
16.95.1 Tomographic PTV                                                                                                           849
16.95.2 Working principle:                                                                                                          857
16.96 Track Smoothing                                                                                                                    858
16.96.1 Inputs                                                                                                                              858
16.96.2 Outputs                                                                                                                            859
16.96.3 Settings and how to                                                                                                       860
16.97 Two-Color Pyrometry                                                                                                            860
16.97.1 Introduction                                                                                                                    860
XIV
16.97.2 Overview of the procedure                                                                                          861
16.97.3 Spatial calibration                                                                                                           861
16.97.4 Two-Color Pyrometry Calibration                                                                                862
16.97.5 Two-Color Pyrometry Processing                                                                                867
16.98 Universal Outlier Detection                                                                                                 873
16.99 UV Scatter plot Range Validation                                                                                         877
16.100 Vector Arithmetic                                                                                                                878
16.101 Vector Dewarping                                                                                                               880
16.101.1 Setting the z-value and w-value                                                                                 881
16.102 Vector Interpolation                                                                                                           884
16.103 Vector Masking                                                                                                                    885
16.104 Vector Resampling                                                                                                              891
16.104.1 Automatic re-sampling                                                                                                892
16.104.2 User-defined re-sampling                                                                                          893
16.104.3 Edit data                                                                                                                        894
16.105 Vector Rotation/Mirroring                                                                                                 895
16.106 Vector/Scalar subtraction                                                                                                  896
16.107 Vector Statistics                                                                                                                   900
16.107.1 Visualization methods                                                                                                 903
16.107.2 Numeric data display                                                                                                   903
16.107.3 Formulas used                                                                                                              904
16.108 Vector Stitching                                                                                                                   905
16.109 Volume Grid                                                                                                                         906
16.109.1 Inputs                                                                                                                            908
16.109.2 Outputs                                                                                                                         908
16.109.3 Settings and how to                                                                                                     909
16.110 Volumetric calibration refinement                                                                                   909
16.110.1 Introduction:                                                                                                                909
16.110.2 Theory of the Volumetric Calibration Refinement.                                                910
16.110.3 The Static Volumetric Calibration Refinement in DynamicStudio                         912
16.110.4 Example of the VV calibration refinement                                                               918
16.110.5 The Temporal Volumetric Calibration Refinement in DynamicStudio                 924
16.110.6 Side effects of the Volumetric Calibration Refinement                                         924
16.111 Volumetric Velocimetry (Legacy method)                                                                       925
16.111.1 References                                                                                                                   926
16.112 Voxel Reconstruction                                                                                                          926
16.112.1 Introduction                                                                                                                 926
16.112.2 The different reconstruction techniques in theory                                               927
16.112.3 The Voxel Reconstruction in DynamicStudio                                                           930
16.113 Waveform Calculation                                                                                                        936
16.113.1 Formulas                                                                                                                       936
16.113.2 Built-in Functions                                                                                                         937
XV
16.113.3 Built-in Operators                                                                                                        938
16.113.4 Naming Conventions                                                                                                   939
16.113.5 Syntax and Numerical Errors                                                                                     940
16.113.6 Examples                                                                                                                       940
16.114 Waveform Extract                                                                                                               940
16.114.1 Extracting Data                                                                                                            940
16.115 Waveform Statistics                                                                                                            941
16.115.1 Statistical Values                                                                                                           941
16.115.2 Example                                                                                                                         942
16.116 Waveform Stitch                                                                                                                  942
16.116.1 Stitching Data                                                                                                               942
16.116.2 Analog Stitching                                                                                                           944
16.117 Correlation option Window/Filter                                                                                     944
16.117.1 Window functions                                                                                                        944
16.117.2 Filter functions                                                                                                             947
16.118 Adaptive Correlation (Legacy method)                                                                            948
16.118.1 Interrogation areas                                                                                                    949
16.118.2 Window and Filter                                                                                                        950
16.118.3 Validation methods                                                                                                      951
16.118.4 Interrogation area offset                                                                                           952
16.118.5 High Accuracy and Deforming Windows                                                                  953
16.119 Proper Orthogonal Decomposition, POD (Legacy Method)                                         957
16.119.1 POD Snapshot                                                                                                              957
16.119.2 POD Projection                                                                                                             960
16.119.3 References                                                                                                                   964
17 Data  Exchange                                                                                                                      965
17.1 Image Import                                                                                                                           965
17.1.1 Formats                                                                                                                              965
17.1.2 How to Import Images                                                                                                    966
17.1.3 Note                                                                                                                                    970
17.2 Image Export                                                                                                                            970
17.2.1 Formats                                                                                                                              971
17.2.2 File Format                                                                                                                         972
17.2.3 Enhanced image quality                                                                                                   972
17.2.4 How to Export Data                                                                                                          973
17.3 Numeric Export                                                                                                                        973
18 Displays                                                                                                                                      978
18.1 General Display Interaction                                                                                                   978
18.2 Zoom and Pan                                                                                                                          978
18.2.1 Zooming                                                                                                                            979
18.2.2 Panning                                                                                                                              980
18.3 Display Rulers                                                                                                                           981
XVI
18.4 Using the display from within an analysis method                                                              982
18.4.1 Zoom                                                                                                                                  983
18.4.2 Pan                                                                                                                                      983
18.4.3 Magnifying glass                                                                                                               983
18.4.4 Color map                                                                                                                          984
18.4.5 Adjusting the ellipse                                                                                                         984
18.4.6 Adjusting the polygon                                                                                                     985
18.4.7 Adjusting the rectangle                                                                                                   987
18.5 Image Map Display                                                                                                                  988
18.5.1 Color map and Histogram                                                                                               992
18.5.2 Particle Density Probe                                                                                                     997
18.5.3 Correlation Map Display                                                                                                1001
18.6 Vector Map Display                                                                                                                1004
18.6.1 Vector Map display options                                                                                          1004
18.6.2 Examples of realizable displays                                                                                    1012
18.7 Scalar Map Display                                                                                                                 1014
18.7.1 More visualization methods…                                                                                      1014
18.8 Overlays                                                                                                                                  1017
18.9 3D Display                                                                                                                               1024
18.9.1 3D?Voxel Display                                                                                                             1025
18.9.2 Interacting with the voxel volume display                                                                 1026
18.9.3 The display options for voxel volumes                                                                       1026
18.9.4 Images                                                                                                                             1029
18.9.5 Vectors                                                                                                                            1030
18.9.6 Iso-surfaces                                                                                                                    1034
18.9.7 Contours                                                                                                                          1036
18.9.8 Streamlines                                                                                                                     1038
18.9.9 Stereo Rendering                                                                                                          1039
18.9.10 Animation                                                                                                                      1039
18.9.11 Camera                                                                                                                          1039
18.9.12 Export                                                                                                                            1040
18.10 XY Display                                                                                                                             1040
18.10.1 Graphical user interface                                                                                             1040
18.10.2 Legend                                                                                                                          1041
18.10.3 Info Box                                                                                                                        1042
18.10.4 Zooming                                                                                                                        1043
18.10.5 Probe                                                                                                                             1043
18.10.6 Default setup                                                                                                               1043
18.10.7 Display Options                                                                                                            1044
18.10.8 Data Selection                                                                                                              1044
18.10.9 Plot Setup                                                                                                                      1045
18.10.10 Axis Setup                                                                                                                   1046
XVII
18.10.11 Line Style                                                                                                                     1047
18.11 Numeric Display                                                                                                                  1048
19 Dialogs                                                                                                                                     1055
19.1 Define Axes                                                                                                                            1055
19.1.1 to change the coordinate system for a project within a DynamicStudio Data-
base.                                                                                                                                             1055
19.2 Measure Scale Factor                                                                                                            1056
19.2.1 Scaling of measurements to metric units.                                                                  1056
19.3 Sort                                                                                                                                          1058
19.3.1 Sorting by Timestamp                                                                                                   1058
19.3.2 Sorting by Data Column Value                                                                                      1058
19.3.3 Sort by Property Value                                                                                                  1059
19.4 Split                                                                                                                                          1060
19.4.1 Split at Index                                                                                                                   1060
19.4.2 Split at Sort Property Value                                                                                          1060
19.4.3 Automatic                                                                                                                        1060
19.4.4 Custom                                                                                                                            1060
19.5 Merge                                                                                                                                      1061
PAGE | 19
1 Introduction
The DynamicStudio User Guide is split in 2 volumes:
Volume 1 is about DynamicStudio in general plus all about hardware and acquisition.
Volume 2 is about data handling, analysis and display.
1.1 The  DynamicStudio Software
DynamicStudio is the main software package for image acquisition and analysis in for example
the PIV, LIF, LII, and Particle Sizing areas. It contains tools for configuration, acquisition, ana-
lysis, post-processing of acquired data:
l  The acquisition system includes auto-detection of devices, cable connection diagrams and
supports distributed acquisition over networks.
l  The innovative and secure ensemble database gives a simple intuitive display of large
amounts of data.
l  The built-in presentation and analysis modules give you several possibilities and com-
binations of processing and display of data.?
Please read and accept our "Software License Agreement" (on page?25) before using this
product!
We encourage you to sign up for a Software Update Agreement, so that you can download
new versions of the software.
Please do not hesitate to contact Dantec Dynamics if you run in to issues with this product, and
please visit our homepage www.dantecdynamics.com for new information on products and
events.
Contents  of this  Manual
This manual consists of sections presenting the system and software and how to use it.
l  If you are new to DynamicStudio you might want to go to Normal Use to get an intro-
duction to DynamicStudio.
l  If you are looking for help on a particular dialog, most dialogs in DynamicStudio will bring
up help information for the dialog if you, Press F1.
On-line Help
The On-line Help is a context sensitive Help system built into the DynamicStudio application
software. It provides you with a quick reference for procedures, keystroke sequences and
commands that you will need in order to run the Imaging System. The Help can be used within
the Application Software.
1.1.1   Assumptions
It is assumed that you have a basic knowledge about measurement techniques in Fluid Mech-
anics and that you are familiar with the concept of Imaging Analysis, PIV, LIF, LII, Particle Siz-
ing etc.
It is also assumed that you are familiar with Windows Terminology.
PAGE | 20
PAGE | 21
2 Related Documentation
The following printed manuals, user guides and documents contain information that you
might find helpful as you use this online help file. Some of these manuals are only delivered
for specific applications and systems
l  2D PIV Reference Manual - 9040U1752 This manual describes the fundamental techniques
of PIV. Including seeding, light sources, cameras, the mathematics of PIV data processing
and using the software.
l  Stereoscopic PIV Reference Manual - 9040U4115 This manual describes the fundamental
techniques of Stereoscopic PIV. Including laser safety, principles in Stereoscopic PIV, cal-
ibrations, system components, and using the software.
l  Planar-LIF Software Installation & User's Guide - 9040U3652 This manual describes the fun-
damental techniques of liquid LIF. Including theory and practice with planar LIF, using the
software for calibration, re-sampling and Reynolds fluxes, and the quality of planar LIF
data.
l  LIF Application Guide - 9040U3041 This manual describes the fundamental techniques of
gas and combustion LIF processes. Including how to handle gas LIF tracers, using the soft-
ware, and the quality of gas and combustion LIF data.
l  Inteferometric Particle Imaging (IPI) Reference Manual - 9040U1191 This manual describes
the IPI particle sizing techniques. Including camera setup, particle light scattering and
detection, calibration and post-processing using the software.
l  A Practical Guide to Laser Safety and Risk Assessment (Safety Guide) - 9040U4031 This safety
guide includes safety instruction in working with lasers. Including risk assessment, per-
sonal safety, creating safe environments, and a check list.
l  Handling Fluorescence Dyes (Safety Guide) - 9040U3671 This safety guide includes instruc-
tions in working with fluorescence dyes for planar LIF. Including interpretation of MSDS
standards, risks and safety phrases.
l  OEM Documentation - A number of OEM supplied manuals and guides are delivered
together with the devices delivered for your system. This includes cameras, lasers, frame
grabbers, AD boards, etc.
PAGE | 23
3 Contacting Dantec Dynamics
Please contact Dantec Dynamics' sales staff for information about these products. Feel free to
comment on the application and send ideas and suggestions to your local Dantec Dynamics
representative, so we can help improve your work with the Dantec Dynamics products. Also,
visit our web site at http://www.dantecdynamics.com for support and information regarding
hardware and software products. As always, we thank you for your continued interest in our
products. If you ever have any questions or comments, please feel free to contact us.
Address  Information
Dantec Dynamics A/S
Tonsbakken 16-18
P.O. Box 121
DK-2740 Skovlunde
Denmark
Telephone: +45 44 57 80 00
Fax: +45 44 57 80 01
For international contacts and sales representatives please visit: Contact Us.
PAGE | 25
4 Software License Agreement
This software end user license agreement ("License Agreement") is concluded between you
(either an individual or a corporate entity) and Dantec Dynamics A/S ("Dantec Dynamics").
Please read all terms and conditions of this License Agreement before installing the Software.
When you install the Software, you agree to be bound by the terms of this License Agree-
ment. If you cannot agree to the terms of this License Agreement you may not install or use
the software in any manner.
Grant of License
This License Agreement together with the Software package including eventual media, user's
guide and documentation and the invoice constitutes your proof of license and the right to
exercise the rights herein and must be retained by you.
One license permits you to use one installation at a time of the Dantec Dynamics software
product supplied to you (the "Software") including documentation in written or electronic
form and solely for your own internal business purposes.
You may not rent, lease, sublicense or otherwise distribute, assign, transfer or make the Soft-
ware available to any third party without the express consent of Dantec Dynamics except to
the extent specifically permitted by mandatory applicable law.
Updates
Updates, new releases, bug fixes, etc. of the Software which are supplied to you (if any), may
be used only in conjunction with versions of the Software legally acquired and licensed by you
that you have already installed, unless such update etc. replaces that former version in its
entirety and such former version is destroyed.
Copyright
The Software (including text, illustrations and images incorporated into the Software) and all
proprietary rights therein are owned by Dantec Dynamics or Dantec Dynamics' suppliers, and
are protected by the Danish Copyright Act and applicable international law. You may not
reverse assemble, decompile, or otherwise modify the Software except to the extent spe-
cifically permitted by mandatory applicable law. You are not entitled to copy the Software or
any part thereof except as otherwise expressly set out above. However you may make a copy
of the Software solely for backup or archival purposes. You may not copy the user's guide
accompanying the Software, nor distribute copies of any user documentation provided in “on-
line” or electronic form, without Dantec Dynamics' prior written permission.
License and Maintenance Fees
You must pay any and all licensee and maintenance fees in accordance with the then-current
payment terms established by Dantec Dynamics.
Limited Warranty
You are obliged to examine and test the Software immediately upon your receipt thereof.
Until 30 days after delivery of the Software, Dantec Dynamics will deliver a new copy of the
Software if the medium on which the Software was supplied is not legible.
A defect in the Software shall be regarded as material only if it has a material effect on the
proper functioning of the Software as a whole, or if it prevents operation of the Software in
its entirety. If until 90 days after the delivery of the Software, it is established that there is a
material defect in the Software, Dantec Dynamics shall, at Dantec Dynamics' discretion, either
deliver a new version of the Software without the material defect, or remedy the defect free
of charge or terminate this License Agreement and repay the license fee received against the
return of the Software. In any of these events the parties shall have no further claims against
each other. Dantec Dynamics shall be entitled to remedy any defect by indicating procedures,
methods or uses ("work-arounds") which result in the defect not having a significant effect on
the use of the Software.
Software is inherently complex and the possibility remains that the Software contains bugs,
defects and inexpediencies which are not covered by the warranty set out immediately
above. Such bugs, defects and inexpediencies etc. shall not constitute due ground for ter-
mination and shall not entitle you to any remedial action including refund of fees or payment
of damages or costs. Dantec Dynamics will endeavour to correct bugs, defects etc. in sub-
sequent releases of the Software.
The Software is licensed "as is" and without any warranty, obligation to take remedial action or
the like thereof in the event of breach other than as stipulated above. It is therefore not war-
ranted that the operation of the Software will be without interruptions, free of bugs or
defects, or that bugs or defects can or will be remedied.
Indemnification
Dantec Dynamics will indemnify you against any claim by an unaffiliated third party that the
Software infringes such unaffiliated third party's intellectual property rights and shall pay to
you the amount awarded to such unaffiliated third party in a final judgment (or settlement to
which Dantec Dynamics has consented) always subject however to the limitations and exclu-
sions set out in this paragraph.
You must notify Dantec Dynamics promptly in writing of any such claim and allow Dantec Dyna-
mics to take sole control over its defense. You must provide Dantec Dynamics with all reas-
onable assistance in defending the claim.
Dantec Dynamics' obligation to indemnify you shall not apply to the extent that any claim com-
prised by this paragraph is based in whole or in part on (i) any materials provided directly or
indirectly by you; (ii) your exploitation of the Software for other purposes than those
expressly contemplated in this License Agreement; and/or (iii) combining of the Software with
third party products. You shall reimburse Dantec Dynamics for any costs or damages that res-
ult from such actions.
If Dantec Dynamics receives information of an alleged infringement of third party intellectual
property rights or a final adverse judgment is passed by a competent court or a final set-
tlement consented to by Dantec Dynamics is reached regarding an intellectual property right
infringement claim related to the Software, Dantec Dynamics may (but shall not under any
obligation to do so), either (i) procure for you the right to continue to use the Software as con-
templated in this License Agreement; or (ii) modify the Software to make the Software non
infringing; (iii) replace the relevant portion of the Software with a non infringing functional
equivalent; or (iv) terminate with immediate effect your right to install and use the Software
against a refund of the license fees paid by you prior to termination. You acknowledge and
agree that Dantec Dynamics is entitled to exercise either of the aforesaid options in Dantec
Dynamics' sole discretion and that this constitutes your sole and exclusive remedy in the
event of any infringement of third party intellectual property rights.
PAGE | 26
PAGE | 27
Limitation of Liability
Neither Dantec Dynamics nor its distributors shall be liable for any indirect damages including
without limitation loss of profits and loss of data or restoration hereof, or any other incidental,
special or other consequential damages, and even if Dantec Dynamics has been informed of
their possibility. Further, Dantec Dynamics disclaims and excludes any and all liability based on
Dantec Dynamics' simple or gross negligent acts or omissions. In addition to any other lim-
itations and exclusions of liability, Dantec Dynamics' total aggregate liability to pay any dam-
ages and costs to you shall in all events be limited to a total aggregated amount equal to the
license fee paid by you for the Software.
Product Liability
Dantec Dynamics shall be liable for injury to persons or damage to tangible items caused by
the Software in accordance with those rules of the Danish Product Liability Act, which cannot
be contractually waived. Dantec Dynamics disclaims and excludes any liability in excess
thereof.
Assignment
Neither party shall be entitled to assign this License Agreement or any of its rights or oblig-
ations pursuant this Agreement to any third party without the prior written consent of the
other party. Notwithstanding the aforesaid, Dantec Dynamics shall be entitled to assign this
License Agreement in whole or in part without your consent to (i) a company affiliated with
Dantec Dynamics or (ii) an unaffiliated third party to the extent that such assignment takes
place in connection with a restructuring, divestiture, merger, acquisition or the like.
Term and Termination
Subject to and conditional upon your compliance with the terms and conditions of this License
Agreement may install and use the Software as contemplated herein.
We may terminate this License Agreement for breach at any time with immediate effect by
serving notice in writing to you, if you commit any material breach of any terms and conditions
set out in this License Agreement. Without limiting the generality of the aforesaid, any failure
to pay fees and amounts due to Dantec Dynamics and/or any infringement of Dantec Dyna-
mics' intellectual property rights shall be regarded a material breach that entitles Dantec
Dynamics to terminate this License Agreement for breach.
Governing Law and Proper Forum
This License Agreement shall be governed by and construed in accordance with Danish law.
The sole and proper forum for the settlement of disputes hereunder shall be that of the
venue of Dantec Dynamics. Notwithstanding the aforesaid, Dantec Dynamics shall forthwith
be entitled to file any action to enforce Dantec Dynamics' rights including intellectual property
rights, in any applicable jurisdiction using any applicable legal remedies.
Questions
Should you have any questions concerning this License Agreement, or should you have any
questions relating to the installation or operation of the Software, please contact the author-
ized Dantec Dynamics distributor serving your country. You can find a list of current Dantec
Dynamics distributors on our web site: www.dantecdynamics.com.
Dantec Dynamics A/S
Tonsbakken 16-18 - DK-2740 Skovlunde, Denmark
Tel: +45 4457 8000 - Fax: + 45 4457 8001
www.dantecdynamics.com
PAGE | 28
PAGE | 29
5 Laser Safety
All equipment using lasers must be labeled with the safety information. Dantec systems are
using Class III and Class IV lasers whose beams are safety hazards. Please read the laser
safety sections of the documentation for the lasers and optics carefully. Furthermore, you
must instigate appropriate laser safety measures and abide by local laser safety legislation.
Use protective eye wear when the laser is running.
Appropriate laser safety measures must be implemented when aligning and using lasers and
illumination systems. You are therefore urged to follow the precautions below, which are gen-
eral safety precautions to be observed by anyone working with illumination systems to be
used with a laser. Again, before starting, it is recommended that you read the laser safety
notices in all documents provided by the laser manufacturer and illumination system supplier
and follow these as well as your local safety procedures.
Precautions
As general precautions to avoid eye damage, carefully shield any reflections so that they do
not exit from the measurement area.
5.1 Danger  of Laser  Light
Laser light is a safety hazard and may harm eyes and skin. Do not aim a laser beam at any-
body. Do not stare into a laser beam. Do not wear watches, jewellery or other blank objects
during alignment and use of the laser. Avoid reflections into eyes. Avoid accidental exposure
to specular beam reflections. Avoid all reflections when using a high-power laser. Screen laser
beams and reflections whenever possible. Follow your local safety regulations. You must wear
appropriate laser safety goggles during laser alignment and operation.
During alignment, run the laser at low power whenever possible.
Since this document is concerned with the BSA Flow Software, there is no direct instructions in
this document regarding laser use. Therefore, before connecting any laser to the system,
you must consult the laser safety section of the laser instruction manual.
5.2 Precautions
As general precautions to avoid eye damage, carefully shield any reflections so that they do
not exit from the measurement area.
5.3 Laser  Safety Poster
Displays the essential precautions for ensuring a safe laboratory environment when using
lasers in your experiments. (Wall poster 70 x 100 cm). Available from http://www.dantec-
dynamics.com, or through Dantec Dynamics' sales offices and representatives.
PAGE | 30
PAGE | 31
6 Requirements
6.1 Minimum  requirements
l  64-bit PC with a modern multi-core Intel processor
l  64-bit MicrosoftR Windows 10? with latest service packs and Windows updates
l  MicrosoftR Windowsc Installer v3.5 or later
l  MicrosoftR Internet Explorerc 11 or later with latest security updates
l  MicrosoftR .NET 4.6.1
l  8 GB of RAM
l  SXGA (1280x1024) or higher-resolution monitor with millions of colors
l  Mouse or compatible pointing device
l  DVD drive x8
6.2 Recommended  configuration
l  16+ GB of RAM
l  1 TB RAID0 configured hard-disk space
l  OS?Disk should be an SSD or better
l  GigaBit Base-T Ethernet or faster adapter with RJ-45 connector
l  UXGA monitor (1920x1080 pixel)
MicrosoftR Windowsc 95, 98, 98SE, Me, NT, 2000, XP, Vista, Windowsc 7 and 32-bit Win-
dowsc 10 operating systems are not supported.
Notice: Installation must always be done as administrator, if necessary contact your network
administrator.
Notice: Some basic Windows system files can be missing during the installation on a clean Win-
dows PC. Therefore we always recommend having all the latest Windows service packs and
updates installed along with the latest Internet Explorer. Utility files and useful links can also
be found on the DVD in the “…\software\util\” folder.
Warning: All USB dongles must be removed during the installation!
During the installation, additional third-party products are installed:
Rainbow Sentinel System Driver: This driver can be updated or removed using the add/re-
move pane in the Windows Control Panel. For more information or driver updates please visit
www.rainbow.com.
DynamicStudio and remote agents require Microsoft .NET Framework Version 4.6.1 or later
which will be installed as part of the DynamicStudio installation.
If your Imaging system includes timer boards, frame grabbers or other computer boards for
insertion in the PC, the appropriate drivers must be installed before installing DynamicStudio.
Please follow the instructions provided in the Unpacking and Installation Guide on the Dynam-
icStudio DVD, or on the download section http://www.dantecdynamics.com/download-login
PAGE | 32
PAGE | 33
7 Note to Administrators
7.0.1 Installation
All installation scripts from Dantec Dynamics A/S contain a digital signature (code signing cer-
tificate) issued by (CA) VeriSign, Inc. (http://www.verisign.com) ensuring correct identity.
Installation requires administrative privileges, but running can also be done under restric-
ted/limited (non-admin) user-accounts. When installing under a restricted/limited user
account, the user is requested to run (Run As...) the installation as an administrative user.
Installation under Windows 10 requires disabling of User Access Control (UAC). Go to the Win-
dows Control Panel and search for UAC to turn it off before installing (requires reboot). It is
however highly recommended to turn it on again after the installation.
To accomplish restricted/limited user account support, the following permissions are altered
during installation:
l  All users (“Everyone”) are permitted full file access to the “All Users\Shared Docu-
ments\Dantec Dynamics” folder including all sub folders.
l  All XML configuration and library files are installed in the “All Users\Shared Docu-
ments\Dantec Dynamics\Dynamic Studio” folder, allowing shared access for all users.
7.1 Windows  Firewall
The following ports are opened during installation in the Windows Firewall.
l  5007 Dantec Dynamics - Multicast Agent Hosts
l  5008 Dantec Dynamics - Agent Configuration Multicast
l  5011 Dantec Dynamics - Agent Master
l  5012 Dantec Dynamics - Agent Host
l  5013 Dantec Dynamics - Acquisition Agent
l  5014 Dantec Dynamics - Traverse Agent
7.2 BCDEDIT.exe  (MicrosoftR  Windowsc  10)
The following boot parameter is added to the Windows boot configuration information, for
specifying physical memory that Windows cannot use, when the image buffer is changed for
frame grabber cameras:
"truncatememory".
The parameter can be removed (and thereby the allocated memory freed) by entering the fol-
lowing command in a Command Prompt:
"bcdedit.exe /deletevalue truncatememory".
PAGE | 35
8 Software Update Agreement (SUA)
We encourage you to sign up for a Software Update Agreement (SUA)
Please consider registering your product and signing up for a Software Update Agreement
with us, giving you accesses to free software updates, manuals and tools. For more inform-
ation see: Software Registration.
PAGE | 37
9 Acknowledgement third party software
components
DynamicStudio uses the following third part software components:
"IPP and MKL from Intel" below
"OpenCV" on page?39
"ANN University of Maryland, Sunil Arya and David Mount" on page?39
"Boost C++ libraries" on page?48
"NVIDIA CUDA Toolkit" on page?49
"muParser from Ingo Berg" on page?49
"Tiflib" on page?50
"VTK" on page?51
"Math.net Numerics" on page?52
9.1 IPP and  MKL  from  Intel
Copyright (c) 2018 Intel Corporation.
Use and Redistribution. You may use and redistribute the software (the “Software”), without
modification, provided the following conditions are met:
* Redistributions must reproduce the above copyright notice and the following terms of use
in the Software and in the documentation and/or other materials provided with the dis-
tribution.
* Neither the name of Intel nor the names of its suppliers may be used to endorse or pro-
mote products derived from this Software without specific prior written permission.
* No reverse engineering, decompilation, or disassembly of this Software is permitted.
Limited patent license. Intel grants you a world-wide, royalty-free, non-exclusive license
under patents it now or hereafter owns or controls to make, have made, use, import, offer to
sell and sell (“Utilize”) this Software, but solely to the extent that any such patent is necessary
to Utilize the Software alone. The patent license shall not apply to any combinations which
include this software. No hardware per se is licensed hereunder.
Third party and other Intel programs. “Third Party Programs” are the files listed in the “third-
party-programs.txt” text file that is included with the Software and may include Intel pro-
grams under separate license terms. Third Party Programs, even if included with the dis-
tribution of the Materials, are governed by separate license terms and those license terms
solely govern your use of those programs.
DISCLAIMER. THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT ARE
DISCLAIMED. THIS SOFTWARE IS NOT INTENDED FOR USE IN SYSTEMS OR APPLICATIONS
WHERE FAILURE OF THE SOFTWARE MAY CAUSE PERSONAL INJURY OR DEATH AND YOU
AGREE THAT YOU ARE FULLY RESPONSIBLE FOR ANY CLAIMS, COSTS, DAMAGES, EXPENSES,
AND ATTORNEYS’ FEES ARISING OUT OF ANY SUCH USE, EVEN IF ANY CLAIM ALLEGES THAT
INTEL WAS NEGLIGENT REGARDING THE DESIGN OR MANUFACTURE OF THE MATERIALS.
LIMITATION OF LIABILITY. IN NO EVENT WILL INTEL BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
OF THE POSSIBILITY OF SUCH DAMAGE. YOU AGREE TO INDEMNIFY AND HOLD INTEL
HARMLESS AGAINST ANY CLAIMS AND EXPENSES RESULTING FROM YOUR USE OR
UNAUTHORIZED USE OF THE SOFTWARE.
No support. Intel may make changes to the Software, at any time without notice, and is not
obligated to support, update or provide training for the Software.
Termination. Intel may terminate your right to use the Software in the event of your breach
of this Agreement and you fail to cure the breach within a reasonable period of time.
Feedback. Should you provide Intel with comments, modifications, corrections, enhance-
ments or other input (“Feedback”) related to the Software Intel will be free to use, disclose,
reproduce, license or otherwise distribute or exploit the Feedback in its sole discretion
without any obligations or restrictions of any kind, including without limitation, intellectual
property rights or licensing obligations.
Compliance with laws. You agree to comply with all relevant laws and regulations governing
your use, transfer, import or export (or prohibition thereof) of the Software.
Governing law. All disputes will be governed by the laws of the United States of America and
the State of Delaware without reference to conflict of law principles and subject to the exclus-
ive jurisdiction of the state or federal courts sitting in the State of Delaware, and each party
agrees that it submits to the personal jurisdiction and venue of those courts and waives any
objections. The United Nations Convention on Contracts for the International Sale of Goods
(1980) is specifically excluded and will not apply to the Software.
*Other names and brands may be claimed as the property of others.
PAGE | 38
PAGE | 39
9.2 OpenCV
By downloading, copying, installing or using the software you agree to this license. If you do
not agree to this license, do not download, install, copy or use the software.
License Agreement
For Open Source Computer Vision Library
(3-clause BSD License)
Copyright (C) 2000-2019, Intel Corporation, all rights reserved.
Copyright (C) 2009-2011, Willow Garage Inc., all rights reserved.
Copyright (C) 2009-2016, NVIDIA Corporation, all rights reserved.
Copyright (C) 2010-2013, Advanced Micro Devices, Inc., all rights reserved.
Copyright (C) 2015-2016, OpenCV Foundation, all rights reserved.
Copyright (C) 2015-2016, Itseez Inc., all rights reserved.
Third party copyrights are property of their respective owners.
Redistribution and use in source and binary forms, with or without modification, are per-
mitted provided that the following conditions are met:
l  Redistributions of source code must retain the above copyright notice, this list of con-
ditions and the following disclaimer.
l  Redistributions in binary form must reproduce the above copyright notice, this list of con-
ditions and the following disclaimer in the documentation and/or other materials provided
with the distribution.
l  Neither the names of the copyright holders nor the names of the contributors may be
used to endorse or promote products derived from this software without specific prior
written permission.
This software is provided by the copyright holders and contributors “as is” and any express or
implied warranties, including, but not limited to, the implied warranties of merchantability and
fitness for a particular purpose are disclaimed. In no event shall copyright holders or con-
tributors be liable for any direct, indirect, incidental, special, exemplary, or consequential dam-
ages (including, but not limited to, procurement of substitute goods or services; loss of use,
data, or profits; or business interruption) however caused and on any theory of liability,
whether in contract, strict liability, or tort (including negligence or otherwise) arising in any
way out of the use of this software, even if advised of the possibility of such damage.
9.3 ANN University of Maryland,  Sunil  Arya  and  David
Mount
----------------------------------------------------------------------
The ANN Library (all versions) is provided under the terms and
conditions of the GNU Lesser General Public Library, which is stated
below. It can also be found at:
http://www.gnu.org/copyleft/lesser.html
----------------------------------------------------------------------
GNU LESSER GENERAL PUBLIC LICENSE
Version 2.1, February 1999
Copyright (C) 1991, 1999 Free Software Foundation, Inc.
59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.
[This is the first released version of the Lesser GPL. It also counts
as the successor of the GNU Library Public License, version 2, hence the
version number 2.1.]
Preamble
The licenses for most software are designed to take away your freedom to
share and change it. By contrast, the GNU General Public Licenses are
intended to guarantee your freedom to share and change free software--to
make sure the software is free for all its users.
This license, the Lesser General Public License, applies to some
specially designated software packages--typically libraries--of the Free
Software Foundation and other authors who decide to use it. You can use
it too, but we suggest you first think carefully about whether this
license or the ordinary General Public License is the better strategy to
use in any particular case, based on the explanations below.
When we speak of free software, we are referring to freedom of use, not
price. Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
this service if you wish); that you receive source code or can get it if
you want it; that you can change the software and use pieces of it in
new free programs; and that you are informed that you can do these
things.
To protect your rights, we need to make restrictions that forbid
distributors to deny you these rights or to ask you to surrender these
rights. These restrictions translate to certain responsibilities for you
if you distribute copies of the library or if you modify it.
For example, if you distribute copies of the library, whether gratis or
for a fee, you must give the recipients all the rights that we gave you.
You must make sure that they, too, receive or can get the source code.
If you link other code with the library, you must provide complete
object files to the recipients, so that they can relink them with the
library after making changes to the library and recompiling it. And you
must show them these terms so they know their rights.
We protect your rights with a two-step method: (1) we copyright the
PAGE | 40
PAGE | 41
library, and (2) we offer you this license, which gives you legal
permission to copy, distribute and/or modify the library.
To protect each distributor, we want to make it very clear that there is
no warranty for the free library. Also, if the library is modified by
someone else and passed on, the recipients should know that what they
have is not the original version, so that the original author's
reputation will not be affected by problems that might be introduced by
others.
Finally, software patents pose a constant threat to the existence of any
free program. We wish to make sure that a company cannot effectively
restrict the users of a free program by obtaining a restrictive license
from a patent holder. Therefore, we insist that any patent license
obtained for a version of the library must be consistent with the full
freedom of use specified in this license.
Most GNU software, including some libraries, is covered by the ordinary
GNU General Public License. This license, the GNU Lesser General Public
License, applies to certain designated libraries, and is quite different
from the ordinary General Public License. We use this license for
certain libraries in order to permit linking those libraries into
non-free programs.
When a program is linked with a library, whether statically or using a
shared library, the combination of the two is legally speaking a
combined work, a derivative of the original library. The ordinary
General Public License therefore permits such linking only if the entire
combination fits its criteria of freedom. The Lesser General Public
License permits more lax criteria for linking other code with the
library.
We call this license the "Lesser" General Public License because it does
Less to protect the user's freedom than the ordinary General Public
License. It also provides other free software developers Less of an
advantage over competing non-free programs. These disadvantages are the
reason we use the ordinary General Public License for many libraries.
However, the Lesser license provides advantages in certain special
circumstances.
For example, on rare occasions, there may be a special need to encourage
the widest possible use of a certain library, so that it becomes a
de-facto standard. To achieve this, non-free programs must be allowed to
use the library. A more frequent case is that a free library does the
same job as widely used non-free libraries. In this case, there is
little to gain by limiting the free library to free software only, so we
use the Lesser General Public License.
In other cases, permission to use a particular library in non-free
programs enables a greater number of people to use a large body of free
software. For example, permission to use the GNU C Library in non-free
programs enables many more people to use the whole GNU operating system,
as well as its variant, the GNU/Linux operating system.
Although the Lesser General Public License is Less protective of the
users' freedom, it does ensure that the user of a program that is linked
with the Library has the freedom and the wherewithal to run that program
using a modified version of the Library.
The precise terms and conditions for copying, distribution and
modification follow. Pay close attention to the difference between a
"work based on the library" and a "work that uses the library". The
former contains code derived from the library, whereas the latter must
be combined with the library in order to run.
TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
0. This License Agreement applies to any software library or other
program which contains a notice placed by the copyright holder or other
authorized party saying it may be distributed under the terms of this
Lesser General Public License (also called "this License"). Each
licensee is addressed as "you".
A "library" means a collection of software functions and/or data
prepared so as to be conveniently linked with application programs
(which use some of those functions and data) to form executables.
The "Library", below, refers to any such software library or work which
has been distributed under these terms. A "work based on the Library"
means either the Library or any derivative work under copyright law:
that is to say, a work containing the Library or a portion of it, either
verbatim or with modifications and/or translated straightforwardly into
another language. (Hereinafter, translation is included without
limitation in the term "modification".)
"Source code" for a work means the preferred form of the work for making
modifications to it. For a library, complete source code means all the
source code for all modules it contains, plus any associated interface
definition files, plus the scripts used to control compilation and
installation of the library.
Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope. The act of running
a program using the Library is not restricted, and output from such a
program is covered only if its contents constitute a work based on the
Library (independent of the use of the Library in a tool for writing
it). Whether that is true depends on what the Library does and what the
program that uses the Library does.
1. You may copy and distribute verbatim copies of the Library's complete
source code as you receive it, in any medium, provided that you
conspicuously and appropriately publish on each copy an appropriate
PAGE | 42
PAGE | 43
copyright notice and disclaimer of warranty; keep intact all the notices
that refer to this License and to the absence of any warranty; and
distribute a copy of this License along with the Library.
You may charge a fee for the physical act of transferring a copy, and
you may at your option offer warranty protection in exchange for a fee.
2. You may modify your copy or copies of the Library or any portion of
it, thus forming a work based on the Library, and copy and distribute
such modifications or work under the terms of Section 1 above, provided
that you also meet all of these conditions:
a) The modified work must itself be a software library.
b) You must cause the files modified to carry prominent notices
stating that you changed the files and the date of any change.
c) You must cause the whole of the work to be licensed at no
charge to all third parties under the terms of this License.
d) If a facility in the modified Library refers to a function or a
table of data to be supplied by an application program that uses
the facility, other than as an argument passed when the facility
is invoked, then you must make a good faith effort to ensure that,
in the event an application does not supply such function or
table, the facility still operates, and performs whatever part of
its purpose remains meaningful.
(For example, a function in a library to compute square roots has
a purpose that is entirely well-defined independent of the application.
Therefore, Subsection 2d requires that any application-supplied function
or table used by this function must be optional: if the application does
not supply it, the square root function must still compute square
roots.)
These requirements apply to the modified work as a whole. If
identifiable sections of that work are not derived from the Library, and
can be reasonably considered independent and separate works in
themselves, then this License, and its terms, do not apply to those
sections when you distribute them as separate works. But when you
distribute the same sections as part of a whole which is a work based on
the Library, the distribution of the whole must be on the terms of this
License, whose permissions for other licensees extend to the entire
whole, and thus to each and every part regardless of who wrote it.
Thus, it is not the intent of this section to claim rights or
contest your rights to work written entirely by you; rather, the intent
is to exercise the right to control the distribution of derivative or
collective works based on the Library.
In addition, mere aggregation of another work not based on the
Library with the Library (or with a work based on the Library) on a
volume of a storage or distribution medium does not bring the other work
under the scope of this License.
3. You may opt to apply the terms of the ordinary GNU General Public
License instead of this License to a given copy of the Library. To do
this, you must alter all the notices that refer to this License, so that
they refer to the ordinary GNU General Public License, version 2,
instead of to this License. (If a newer version than version 2 of the
ordinary GNU General Public License has appeared, then you can specify
that version instead if you wish.) Do not make any other change in these
notices.
Once this change is made in a given copy, it is irreversible for that
copy, so the ordinary GNU General Public License applies to all
subsequent copies and derivative works made from that copy.
This option is useful when you wish to copy part of the code of the
Library into a program that is not a library.
4. You may copy and distribute the Library (or a portion or derivative
of it, under Section 2) in object code or executable form under the
terms of Sections 1 and 2 above provided that you accompany it with the
complete corresponding machine-readable source code, which must be
distributed under the terms of Sections 1 and 2 above on a medium
customarily used for software interchange.
If distribution of object code is made by offering access to copy from a
designated place, then offering equivalent access to copy the source
code from the same place satisfies the requirement to distribute the
source code, even though third parties are not compelled to copy the
source along with the object code.
5. A program that contains no derivative of any portion of the Library,
but is designed to work with the Library by being compiled or linked
with it, is called a "work that uses the Library". Such a work, in
isolation, is not a derivative work of the Library, and therefore falls
outside the scope of this License.
However, linking a "work that uses the Library" with the Library creates
an executable that is a derivative of the Library (because it contains
portions of the Library), rather than a "work that uses the library".
The executable is therefore covered by this License. Section 6 states
terms for distribution of such executables.
When a "work that uses the Library" uses material from a header file
that is part of the Library, the object code for the work may be a
derivative work of the Library even though the source code is not.
Whether this is true is especially significant if the work can be linked
without the Library, or if the work is itself a library. The threshold
for this to be true is not precisely defined by law.
If such an object file uses only numerical parameters, data structure
layouts and accessors, and small macros and small inline functions (ten
PAGE | 44
PAGE | 45
lines or less in length), then the use of the object file is
unrestricted, regardless of whether it is legally a derivative work.
(Executables containing this object code plus portions of the Library
will still fall under Section 6.)
Otherwise, if the work is a derivative of the Library, you may
distribute the object code for the work under the terms of Section 6.
Any executables containing that work also fall under Section 6, whether
or not they are linked directly with the Library itself.
6. As an exception to the Sections above, you may also combine or link a
"work that uses the Library" with the Library to produce a work
containing portions of the Library, and distribute that work under terms
of your choice, provided that the terms permit modification of the work
for the customer's own use and reverse engineering for debugging such
modifications.
You must give prominent notice with each copy of the work that the
Library is used in it and that the Library and its use are covered by
this License. You must supply a copy of this License. If the work during
execution displays copyright notices, you must include the copyright
notice for the Library among them, as well as a reference directing the
user to the copy of this License. Also, you must do one of these things:
a) Accompany the work with the complete corresponding
machine-readable source code for the Library including whatever
changes were used in the work (which must be distributed under
Sections 1 and 2 above); and, if the work is an executable linked
with the Library, with the complete machine-readable "work that
uses the Library", as object code and/or source code, so that the
user can modify the Library and then relink to produce a modified
executable containing the modified Library. (It is understood that
the user who changes the contents of definitions files in the
Library will not necessarily be able to recompile the application
to use the modified definitions.)
b) Use a suitable shared library mechanism for linking with the
Library. A suitable mechanism is one that (1) uses at run time a
copy of the library already present on the user's computer system,
rather than copying library functions into the executable, and (2)
will operate properly with a modified version of the library, if
the user installs one, as long as the modified version is
interface-compatible with the version that the work was made with.
c) Accompany the work with a written offer, valid for at least
three years, to give the same user the materials specified in
Subsection 6a, above, for a charge no more than the cost of
performing this distribution.
d) If distribution of the work is made by offering access to copy
from a designated place, offer equivalent access to copy the above
specified materials from the same place.
e) Verify that the user has already received a copy of these
materials or that you have already sent this user a copy.
For an executable, the required form of the "work that uses the Library"
must include any data and utility programs needed for reproducing the
executable from it. However, as a special exception, the materials to be
distributed need not include anything that is normally distributed (in
either source or binary form) with the major components (compiler,
kernel, and so on) of the operating system on which the executable runs,
unless that component itself accompanies the executable.
It may happen that this requirement contradicts the license restrictions
of other proprietary libraries that do not normally accompany the
operating system. Such a contradiction means you cannot use both them
and the Library together in an executable that you distribute.
7. You may place library facilities that are a work based on the Library
side-by-side in a single library together with other library facilities
not covered by this License, and distribute such a combined library,
provided that the separate distribution of the work based on the Library
and of the other library facilities is otherwise permitted, and provided
that you do these two things:
a) Accompany the combined library with a copy of the same work
based on the Library, uncombined with any other library
facilities. This must be distributed under the terms of the
Sections above.
b) Give prominent notice with the combined library of the fact
that part of it is a work based on the Library, and explaining
where to find the accompanying uncombined form of the same work.
8. You may not copy, modify, sublicense, link with, or distribute the
Library except as expressly provided under this License. Any attempt
otherwise to copy, modify, sublicense, link with, or distribute the
Library is void, and will automatically terminate your rights under this
License. However, parties who have received copies, or rights, from you
under this License will not have their licenses terminated so long as
such parties remain in full compliance.
9. You are not required to accept this License, since you have not
signed it. However, nothing else grants you permission to modify or
distribute the Library or its derivative works. These actions are
prohibited by law if you do not accept this License. Therefore, by
modifying or distributing the Library (or any work based on the
Library), you indicate your acceptance of this License to do so, and all
its terms and conditions for copying, distributing or modifying the
Library or works based on it.
PAGE | 46
PAGE | 47
10. Each time you redistribute the Library (or any work based on the
Library), the recipient automatically receives a license from the
original licensor to copy, distribute, link with or modify the Library
subject to these terms and conditions. You may not impose any further
restrictions on the recipients' exercise of the rights granted herein.
You are not responsible for enforcing compliance by third parties with
this License.
11. If, as a consequence of a court judgment or allegation of patent
infringement or for any other reason (not limited to patent issues),
conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License. If you cannot distribute
so as to satisfy simultaneously your obligations under this License and
any other pertinent obligations, then as a consequence you may not
distribute the Library at all. For example, if a patent license would
not permit royalty-free redistribution of the Library by all those who
receive copies directly or indirectly through you, then the only way you
could satisfy both it and this License would be to refrain entirely from
distribution of the Library.
If any portion of this section is held invalid or unenforceable under
any particular circumstance, the balance of the section is intended to
apply, and the section as a whole is intended to apply in other
circumstances.
It is not the purpose of this section to induce you to infringe any
patents or other property right claims or to contest validity of any
such claims; this section has the sole purpose of protecting the
integrity of the free software distribution system which is implemented
by public license practices. Many people have made generous
contributions to the wide range of software distributed through that
system in reliance on consistent application of that system; it is up to
the author/donor to decide if he or she is willing to distribute
software through any other system and a licensee cannot impose that
choice.
This section is intended to make thoroughly clear what is believed to be
a consequence of the rest of this License.
12. If the distribution and/or use of the Library is restricted in
certain countries either by patents or by copyrighted interfaces, the
original copyright holder who places the Library under this License may
add an explicit geographical distribution limitation excluding those
countries, so that distribution is permitted only in or among countries
not thus excluded. In such case, this License incorporates the
limitation as if written in the body of this License.
13. The Free Software Foundation may publish revised and/or new versions
of the Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.
Each version is given a distinguishing version number. If the Library
specifies a version number of this License which applies to it and "any
later version", you have the option of following the terms and
conditions either of that version or of any later version published by
the Free Software Foundation. If the Library does not specify a license
version number, you may choose any version ever published by the Free
Software Foundation.
14. If you wish to incorporate parts of the Library into other free
programs whose distribution conditions are incompatible with these,
write to the author to ask for permission. For software which is
copyrighted by the Free Software Foundation, write to the Free Software
Foundation; we sometimes make exceptions for this. Our decision will be
guided by the two goals of preserving the free status of all derivatives
of our free software and of promoting the sharing and reuse of software
generally.
NO WARRANTY
15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE LIBRARY "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE
ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE LIBRARY IS WITH
YOU. SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL
NECESSARY SERVICING, REPAIR OR CORRECTION.
16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN
WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
AND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE LIBRARY
(INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF
THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF SUCH HOLDER OR
OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
9.4 Boost C++ libraries
Boost Software License - Version 1.0 - August 17th, 2003
PAGE | 48
PAGE | 49
Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:
The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
9.5 NVIDIA  CUDA  Toolkit
Parts of DynamicStudio uses NVIDIA CUDA Toolkit
License terms can be found https://docs.nvidia.com/cuda/eula/index.html.
9.6 muParser  from  Ingo Berg
#######################################################################
#                                                                                   #
#                                                                                   #
#                    __________                                                  #
#     _____    __ __\______    \_____  _______  ______  ____ _______      #
#    /      \ |  |  \|      ___/\__  \ \_  __ \/  ___/_/ __ \\_  __ \     #
#  |  Y Y  \|  |  /|     |      / __ \_|  | \/\___ \ \  ___/ |  | \/     #
#  |__|_|  /|____/ |____|     (____  /|__|  /____  > \___  >|__|        #
#          \/                            \/              \/       \/              #
#                                                 Fast math parser Library     #
#                                                                                   #
#  Copyright (C) 2011 Ingo Berg                                               #
#                                                                                   #
#  Web:      muparser.beltoforion.de                                          #
#  e-mail:  muparser@beltoforion.de                                          #
#                                                                                   #
#                                                                                   #
#######################################################################
Permission is hereby granted, free of charge, to any person obtaining a
copy of this
software and associated documentation files (the "Software"), to deal in
the Software
without restriction, including without limitation the rights to use,
copy, modify,
merge, publish, distribute, sublicense, and/or sell copies of the Soft-
ware, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
The above copyright notice and this permission notice shall be included
in all copies or
substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT
NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM,
DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
IN THE SOFTWARE.
OR OTHER DEALINGS IN THE SOFTWARE.
9.7 Tiflib
Copyright (c) 1988-1997 Sam Leffler
Copyright (c) 1991-1997 Silicon Graphics, Inc.
Permission to use, copy, modify, distribute, and sell this software and
its documentation for any purpose is hereby granted without fee,
provided
that (i) the above copyright notices and this permission notice appear
PAGE | 50
PAGE | 51
in
all copies of the software and related documentation, and (ii) the names
of
Sam Leffler and Silicon Graphics may not be used in any advertising or
publicity relating to the software without the specific, prior written
permission of Sam Leffler and Silicon Graphics.
THE SOFTWARE IS PROVIDED "AS-IS" AND WITHOUT WARRANTY OF ANY KIND,
EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY
WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
IN NO EVENT SHALL SAM LEFFLER OR SILICON GRAPHICS BE LIABLE FOR
ANY SPECIAL, INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND,
OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY
OF
LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE
OF THIS SOFTWARE.
9.8 VTK
VTK is an open-source toolkit licensed under the BSD license.
Copyright (c) 1993-2008 Ken Martin, Will Schroeder, Bill Lorensen
All rights reserved.
Redistribution and use in source and binary forms, with or without modification, are per-
mitted provided that the following conditions are met:
l  Redistributions of source code must retain the above copyright notice, this list of con-
ditions and the following disclaimer.
l  Redistributions in binary form must reproduce the above copyright notice, this list of con-
ditions and the following disclaimer in the documentation and/or other materials provided
with the distribution.
l  Neither name of Ken Martin, Will Schroeder, or Bill Lorensen nor the names of any con-
tributors may be used to endorse or promote products derived from this software
without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS”
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHORS OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
9.9 Math.net Numerics
Math.NET Numerics License (MIT/X11)
Copyright (c) 2002-2019 Math.NET
Permission is hereby granted, free of charge, to any person obtaining a copy of this software
and associated documentation files (the "Software"), to deal in the Software without restric-
tion, including without limitation the rights to use, copy, modify, merge, publish, distribute,
sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or sub-
stantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
PAGE | 52
PAGE | 53
10 Add-Ons and Options for Dynam-
icStudio
Item                                            Description                                                                                               Item No
Main Software Platform    DynamicStudio Base Package.                                                 80S57
3D-LSM                                Volumetric Least Squares Matching.                                      80S21
Volumetric Particle
Tracking
Determination of 3D?particle histories from time-
resolved images acquired from two cameras.
80S29
3D?TOMO PTV                     Tomographic Particle Tracking Velocimetry                          80S20
2D PIV                                  Including standard 2D PIV auto-, and cross-correlation
calculations, together with advanced adaptive-, and
average correlations. Validation routines of PIV cov-
ering peak-, range-, filter- and moving average val-
idations. The add-on also includes basic vector displays
and statistics as well as vector subtractions.
80S58
Stereoscopic PIV
(requires 2D PIV)
Allows computation of the third (out-of-plane) velocity
component, by using data from two cameras.
80S45
Optical Flow and Motion
Tracking
Particle tracking add-on for detecting particle tra-
jectories in double images.
80S46
FlexPIV                                 FlexPIV is a unique add-on to DynamicStudio. It allows
you to define exactly what part of an image map you
want to use for vector calculation, and how you want to
do it. It introduces advanced grid generation FlexGRID
and variable processing schemes.
80S43
Oscillating Pattern
Decomposition
Oscillating Pattern Decomposition identify spatial struc-
tures (modes) with corresponding frequencies and
growth/decay rates.
80S13
Spray Geometry module  Spray geometry determines the spatial geometry of a
spray nozzle with one or more nozzle exits. The analysis
can characterize the geometry of a spray seen from the
side as a plume or seen from above/below as a pattern.
The temporal development of the spray can be eval-
uated at different time delays, or time averaged.
80S87
Shadow Sizing                     Shadow sizing analysis add-on for detecting arbitrary
particle types in captured images. This package includes
the following modules: shadow processing, shadow his-
togram, shadow spatial histogram/statistics, and shadow
validation.
80S48
Particle Characterization   Interferometric Particle Imaging (IPI) add-on for detect-
ing spherical droplets or bubbles in an image. This pack-
age includes the following modules: IPI processing,
diameter histogram, spatial histogram together with
mean, and RMS statistics.
80S38
LIEF                                       Laser Induced Exciplex Fluorescence                                     80S111
Combustion LIF                  Laser Induced Fluorescence (LIF) add-on for com-
bustion processes. This package includes the following
modules: Analog time-stamp calibration and rescale, LIF
species processing, resampling (grid, vector map), dis-
tribution processing, scalar map conversion, curve fit
analysis together with mean-, and RMS statistics.
80S55
Liquid/Gas LIF                     Laser Induced Fluorescence (LIF) add-on for measuring
instant whole-field concentration, temperature or pH
maps in both liquid and gas flows. The LIF package con-
tains the following modules: LIF tracer calibration, LIF
processing (concentration, temperature, pH), res-
ampling (grid, vector map), together with mean-, RMS-,
instant Reynolds fluxes-, and vector statistics.
80S85
Rayleigh Thermometry     Temperature measurements in flames using Rayleigh
scattering.
80S89
LII                                         Laser Induced Incandescense (LII) add-on for soot emis-
sions analysis. This package includes the following mod-
ules: analog time-stamp calibration and rescale, LII
calibration, LII gas-composition calibration, LII pro-
cessing, resampling (grid, vector map) together with
mean, and RMS statistics.
80S59
Distributed Analysis           Distributed Analysis use redundant network computer
capacity and speed up massive image data analysis.
80S84
Traverse Option                 Enables DynamicStudio to control a traverse system.         80S76
TecplotR Data Loader       An add-on data loader for Tecplot to simplify loading of
Dynamic Studio data.
80S79
LIF-Mie Sizing                      ...                                                                                                   80S56
Volumetric Velocimetry    Volumetric Velocimetry Add-on for DynamicStudio con-
tains calibration routines, Volumetric Particle Tracking,
Tomographic Particle Tracking and Least Squares Match-
ing.
80S83
PAGE | 54
PAGE | 55
11 Legacy Options
The following Add-Ons and Options have been discontinued, merged with other options or
made part of the main package. For technical reasons they may however still appear in Dynam-
icStudio's 'About'-box as enabled in the dongle:
Advanced PIV
(requires 2D PIV Add-
On)
80S33      The Advanced PIV add-on includes the
possibility for high accuracy processing
and definition of deforming windows.
Merged with 80S58,
2D PIV
Image Processing
Library
80S39      Basic image manipulations and trans-
forms including: rotation, shifting, scal-
ing, high and low pass filtering and more.
Also including more advanced routines
like morphing and Fourier transforms.
Included in 80S57,
DynamicStudio Base
Package.
Proper Orthogonal
Decomposition
80S74      Proper Orthogonal Decomposition
(POD): Using the method of snapshots
on a time-series of scalar or vector maps.
Included in 80S57,
DynamicStudio Base
Package.
Data Visualization              80S69      Using embedded graphics engine               Discontinued
Advanced Graphics           80S75      The Advanced Graphics opens up for
advanced vector data processing and dis-
plays. The add-on includes vector map
resampling, rotation and mirroring. It
also includes streamline, LIC, scalar map,
spectrum and vorticity displays.
(formerly known as Vector Processing
Library)
Included in 80S57,
DynamicStudio Base
Package.
MATLABR Link                   80S77      Transfer of setup information and data
from DynamicStudio to MATLAB's work-
space and auto execution of a user-sup-
plied script.
Included in 80S57,
DynamicStudio Base
Package.
1 Camera Plug-in
2 Camera Plug-in
3+ Camera Plug-in
80S80
80S81
80S82
The simple and flexible device handling
in DynamicStudio opens up for an infinite
number of camera combinations. Cam-
eras are auto detected and enabled loc-
ally or remote in the system. Different
add-ons opens up for standard 1 or 2
camera combinations, and for advanced
use of 3 or more cameras can be con-
nected anywhere in the system.
Included in 80S57,
DynamicStudio Base
Package.
Analog Waveform Ana-
lysis
80S91                                                                                 Included in 80S57,
DynamicStudio Base
Package.
Dynamic Mode Decom-
position
80S90      Dynamic Mode Decomposition extracts
modal structures based on temporal or
spatial linear evolution dynamics.
Replaced by 80S13,
Oscillating Pattern
Decomposition.
Light-field Velocimetry     80S99      Volumetric Particle Dis-
placement/Velocity using a Light-Field
camera
Discontinued
PAGE | 56
PAGE | 57
12 Getting Started
The DynamicStudio Image Acquisition System comprises the DynamicStudio software includ-
ing on-line image acquisition. The acquisition system can be configured with the desired para-
meter settings and data can be acquired, pre-viewed and saved to disk at the operator needs.
A comprehensive number of data processing methods are available for analyzing the data
after the acquisition has been made and the data are saved to disk.
Normally the acquisition hardware used to control the camera and illumination system (typ-
ically a laser), hardware synchronization and data acquisition/transfer are installed in the same
PC as the DynamicStudio software.
The system supports a multitude of different cameras and can be configured for one or more
cameras (typically using multiple cameras of the same type).
A synchronization unit is used to generate trigger pulses for all cameras, light sources and/or
a shutter devices.
For more information about supported devices please see "Cameras" on page?195, "Syn-
chronizers" on page?285, and "Illumination Systems" on page?369.
12.1 Warranties  and  Disclaimers
12.1.1 Camera  Sensor Warranty
Direct or reflected radiation from Argon or Nd:YAG lasers can damage the sensor of the cam-
era. This may happen with or without power to the camera and with or without the lens moun-
ted. Therefore when setting up and aligning for measurements, take extreme care to
prevent this from happening.
Laser damage may turn up as white pixels in the vertical direction, or as isolated white pixels
anywhere in the image. This shows up clearly when acquiring an image with the lens cap on.
The manufacturer has identified all sensor defects into classes. Often the character and loc-
ation of all defects are on record. Additional defects arising from laser-induced damage may
void the sensor warranty.
Precautions
1.   Cap the lens whenever the camera is not in use.
2.   Cap the lens during set-up and alignment of the light sheet. Before removing the cap,
make sure that reflections off the surface of any objects inside the light sheet do not hit
the lens by observing where reflections go.
3.   As general precautions to avoid eye damage, carefully shield any reflections so that they
do not exit from the measurement area. You must wear appropriate laser safety goggles
during laser alignment and operation.
12.2 Installation
Please follow the instruction found in "Unpacking and installation guide" on the installation
DVD.
12.2.1 Multiple installations of DynamicStudio
With DynamicStudio 7 it is possible to have several installations available on the same machine.
This can be useful for evaluation purpose when upgrading DynamicStudio to a new version.
This makes it possible to e.g. have both version 6 and 7 of DynamicStudio installed end switch-
ing between these versions.
Note: that it is not possible to have two different version of DynamicStudio 7 installed at the
same time.
Note: the installed versions configuration files are separated to avoid any conflict between
the installed versions.
Note: that when creating a new sequence library in version 6 this new entry will now also exist
in version 7 if both versions are installed on the same machine.
Note: DynamicStudio 7 comes with a new data structure, see "The new data structure intro-
duced with DynamicStudio 7" for more information.
12.3 Using  DynamicStudio
12.3.1 User Interface
This helps contents information about the "Layout, tool bar and file menu" below and provides
a step by step getting started procedure for the "Normal Use" on page?60 (data analysis)?and
for "Acquisition Mode" on page?61 (acquiring images).
Layout, tool bar and file menu
Layout and windows
The layout can be customized differently for acquisition mode and normal mode. Different
windows can be displayed from the View menu:
l  Devices: to add devices and to see the one (cameras, frame grabbers, agent,...) are use.
For more information see "Automatic device detection" on page?62 or "Adding devices
manually" on page?63.
l  Device Properties: to read and change the devices properties displayed in the device win-
dow.
l  System Control: to acquire data and to set acquisition parameters. For more information
see "Acquiring Images" on page?64.
l  Acquired Data: to display the database tree.
l  Synchronization Cables: to tell DynamicStudio how devices are connected. For more
information "Device connection" on page?64.
l  Log: to display error and warning messages. It is recommended to keep this window.
l  Database: display and handle the database.
l  Record Properties: view and edit properties of individual database records.
l  Agents: state of acquisition agents.
PAGE | 58
PAGE | 59
Tool bar
Below the main menu there is a series of icon buttons forming a toolbar. These buttons
provide shortcuts to a number of the functions in the system as described below.
Delete record or set-up.
Delete all records under a set up. Make sure you want to push this button before
using it.
DynamicStudio can be used in two modes: the normal mode for data analysis and
the acquisition mode. Swapping from one mode to the other one is done by click-
ing on the green lightning icon in the tool bar.
Show Recipe.
Analysis.
Perform analysis again.
Open as numeric. When pressing this feature on a data set the underlying data
are displayed in a spread sheet format. The columns may be formatted and pas-
ted onto Windows clip board from where it may be pasted into other programs
like Microsoft Excel.
Open current record.
Open database: Pressing this button will close the current database, and prompt
you to select a new one from a list of existing databases. You may of course re-
open the original database, by selecting it from the list.
Opens a data set as XY-Plot.
Unselects all records in the database.
Open the timing diagram
Menu
The menu represents the main menu of the DynamicStudio software and include the points:
File: selecting database, new/open/save/import and general system components.
Edit: rename/delete and select/unselect and clear log.
View: selecting device, record/device properties, system control, acquired data, syn-
chronization cables, agents, log windows and UI (user-interface) level. For more information
see "Normal Use" on the next page and "Acquisition Mode" on page?61. From this menu, the
layout can be saved, loaded or reset to default. The layout can be customized differently for
acquisition mode and normal mode.
Run:?leaving or entering the acquisition mode, resetting the acquisition system and con-
trolling the traverse system.
Analysis:?performing a calibration or data analysis
Tools:?launching the configuration wizard see "First Time Use" on page?69 and light source wiz-
ard "First Time Use" on page?69.
Window: standard control of the appearances of windows and jumping to another window
Help: online help contents, and version number of the software and gives access to dongle
key code.
It is possible to add your own entries to the Tools menu, e,g, to launch other software from
within DynamicStudio. Perform the following steps to add an entry:
1.   Right click onto the program that you want to create a shortcut for and select "Create
Shortcut".
2.   Let the name of the shortcut start with "Tool.". Example: Tool.BEI Device Interface
3.   Move the shortcut to the DynamicStudio installation path
(typically C:\Program Files (x86)\Dantec Dynamics\DynamicStudio
4.   Start or restart DynamicStudio, and the shortcut will appear in the list of tools for Dynam-
icStudio.
All entries that you add to the Tool menu will still be present after installation of a new Dynam-
icStudio version. In general, all files added manually to the DynamicStudio folder wil not be
deleted by installing or re-installing DynamicStudio.
Normal Use
When you start DynamicStudio you will see the default database view:
PAGE | 60
PAGE | 61
You can change the screen layout, but this is the default and will be used throughout this
manual.
The top of the screen features a normal menu and toolbar. Everything in the right hand side is
related to the acquisition of images, while the left hand side is intended for managing both
images that has already been acquired and other data derived from them. The gray area in
the middle is the working area where you can display and examine acquired images as well
data derived from them.
Creating a Database
From the File menu you can open existing DynamicStudio databases or create a new one to
hold the images that you acquire. To create a new one click File/New Database... and specify
name and location for a new database.
With DynamicStudio 7 databases have a new format as described in section "The new data
structure introduced with DynamicStudio 7". Read this section to understand how to work
with databases and keep your data safe.
Acquisition Mode
To enter Acquisition Mode press Run/Enter Acquisition Mode or click the little green button
on the toolbar.
When you enter acquisition mode DynamicStudio will search for Acquisition Agents on the
local PC and/or other PC's on the local network. The agents found will be prompted for inform-
ation about various hardware devices connected to the PC in question.
Automatic device detection
Cameras and synchronization devices are auto-detected by DynamicStudio and appear auto-
matically in the 'Devices' list in the middle right-hand part of the screen: The auto-detection
will also detect if connection is lost to either of them and indicate this by changing the text
color to red.
PAGE | 62
PAGE | 63
The auto-detection may take a few seconds, but you need not do anything to tell the system
e.g. what kind of camera(s) are connected.
Adding devices manually
Only 'intelligent' devices can be auto-detected, while e.g. lasers or other light sources typically
cannot. These have to be added manually, by right clicking in the device list below the last
device shown. In the context menu select Add New Device... and select your laser from the
list:
Having selected a laser, the device list should look like this:
Device connection
When all devices in your system are listed it is time to tell DynamicStudio how they are con-
nected. Some synchronization units are very flexible with respect to where e.g. the camera
trigger cable should be connected, while other synchronization units allow only one specific
way of connecting the other devices.
From the View menu select Synchronization Cables to display a diagram of connected devices.
At first you should just see the various devices without any connections between them. If you
right click e.g. the arrow at camera trig you can Restore Default Connections..., which will sug-
gest a possible connection to the synchronization unit.
Provided of course devices really are connected as shown in the diagram, you are now ready
to start acquiring images.
Acquiring Images
In the default screen layout of DynamicStudio the main system control is in the upper right
corner of the screen. If Acquired Data is shown click the tab labeled System Control.
PAGE | 64
PAGE | 65
You can specify how many images you wish to acquire (limited by the size of the image buffer).
You can also specify the time between pulses to use if double-frames (or double-exposed
single frames) are to be acquired.
You can specify the desired trigger rate, limited by the capabilities of the camera.
You can specify whether to acquire single- or double-frame images.
Here you can also specify if you want to change the coordinate system from the beginning of
your project, by selecting " Axes" on the next page
There are three different ways of acquiring images with DynamicStudio, corresponding to the
three topmost buttons in the right hand side of the system control window:
Free Run
In free run the camera is running freely, i.e. not synchronized with other hardware devices.
This means e.g. that in this mode the laser (or other light source) is not flashing, and the
frame rate of the camera does not necessarily correspond to the nominal value from the sys-
tem control. If ambient light is strong enough compared to exposure time you should be able
to see images and focus the camera.
The camera will continue acquiring data until you press Stop continually overwriting the oldest
images in the buffer. If you set the system up to acquire say 20 images, the last 20 acquired
will remain in the buffer after you press Stop so you can browse through them and store
them if you wish.
Preview
In preview mode all devices are synchronized, the laser is flashing and camera triggers
acquires images at the rate specified in the System Control panel. It will not stop acquiring
images when the requested number of images have been acquired, but simply overwrite the
oldest image with the most recent one. It will continue acquiring images until you press Stop
and again the last images acquired remain in the buffer for evaluation and possibly storage.
Acquire
Acquire does exactly the same as Preview with the one exception that it stops when the
requested number of images have been acquired. You can of course stop it earlier by
pressing the Stop button, in which case the allocated buffer will not be full, but hold the
images acquired until you stopped the acquisition.
Note Common to all run modes is that acquired data can be saved to the database.
After selecting "Acquire" or "Preview" it might be needed that cameras or other sensors
might need calibration. Therefore DynamicStudio automatically checks if the calibration of one
or more devices is needed and will warn the user, in case not all sensors are calibrated which is
described in the following section.
Semi Automatic Sensor calibration
For high image quality and noise removal a camera sensor calibration might be necessary
after changing the exposure time, the frequency, the operation mode form single- to double-
frame (or the other way round). When the user tries to start a preview or an acquisition after
a change of parameters and the software reminds him to perform a calibration and the fol-
lowing pop up will occur:
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration. Please note that for many
cameras without an internal mechanical shutter the lens has to be covered during the pro-
cess.
This "Semi automatic calibration" can also be started from the device settings under "Image
Sensor Calibration" in case of cameras or other devices.
Axes
when selecting "Define axes" the following dialog will appear. In this dialog you can assign the
axes and the direction of a Cartesian coordinate system. Note that left handed coordinate sys-
tems are applicable but may cause issues when traverses or similar are used. Also, the soft-
ware only lets you proceed when all three different axes are assigned.
PAGE | 66
PAGE | 67
When axes are reassigned, it is of importance that in record properties, and in all other parts
of DynamicStudio where coordinate systems are affected, one takes great care to be con-
sistent with the axes' definition. If you want to change you axis definition later you can also get
to the settings window by a right mouse click on the calibration record and then choose
"Define axes"
Storing and Evaluating Acquired Images
Whether acquisition stops automatically or because you click Stop, the last images acquired will
remain in the buffer for evaluation and storage.
To evaluate and/or store the images in the buffer click the tab labeled Acquired Data:
Evaluating images
When acquisition stops the very last image acquired will typically be shown on the screen and
with controls in the Acquired Data window you can browse through the images in the buffer
while the display window updates accordingly. You can even play back as a movie specifying
the time between each new frame in the dialog Time interval.
You need not play back all images, but can start part way into the buffer and stop before
reaching the end. To do this, click and drag the pink dots at either end of the green line in the
window above. The blue arrow indicates which image is presently shown on the screen and
can also be moved around by clicking and dragging with the mouse.
Storing images
While in the buffer images is still only stored temporarily in RAM. In the window with Acquired
Data you can choose to store the images in the database, meaning also that they are trans-
ferred to the hard disk for permanent storage.
As with the playback described above you need not store all of the images, but may opt to
store just some of them using the same controls as used for the movie playback.
There are two buttons for storing images, Save in Database and Save for Calibration.
The first one is for normal data, while the second, as the name implies, is intended for cal-
ibration images of various kinds. If for example you've taken a picture of a ruler to determine
scaling, you would store the image for calibration.
Specifically identifying calibration images to the system is an advantage for later processing,
since such images are inherently different from other images and are generally processed
and used very different from 'normal' images.
When images are saved in the database they will appear in the database view in the left hand
side of the screen, from where you can access them the same as you would access old data
from an existing database.
PAGE | 68
PAGE | 69
12.3.2 First Time Use
Content
This help contents information about
Agent DynamicStudio
Traverse Installation
Light Source Wizard to add, remove or edit a light source
Agents  DynamicStudio
Agents DynamicStudio rely on so-called 'Agents' to handle external hardware such as frame
grabbers, cameras, timer boards and so on. An 'Agent' is a program, which may run on the
same PC as DynamicStudio itself or on another PC communicating with DynamicStudio via a
normal network. DynamicStudio can handle multiple agents running on several PC's, but the
most common setup handles everything from a single PC.
A configuration wizard is supplied with DynamicStudio to help you set up the agent(s); either
click the shortcut which the installation created on the desktop or click Start/All Pro-
grams/Dantec Dynamics/DynamicStudio.
When DynamicStudio is running click Tools/Configuration Wizard and go through the wizard.
Traverse Installation
Click on tools and select the configuration wizard. Select Networked system/traversing system
Check the traverse agent to be installed:
PAGE | 70
PAGE | 71
The traverse agent is successfully installed
Light Source Wizard
To add, remove and configure a new light source such as laser, flash lamp or stroboscope,
select from tools the light source wizard. If you use a standard light source you may skip this
part.
Follow the instructions from the wizard.
Press next and check the operation you want to perform
PAGE | 72
PAGE | 73
Adding a light source
After giving a name to the light source, it has to be configured. Select the number of cavities
and light source type between:
According to the selected light source, you need to type different properties
The following window appears when the light source is created.
PAGE | 74
PAGE | 75
Your new light source is now available form the dive list (right lick on the agent icon and select
add new device):
12.3.3 Image Buffer?resources
Unless your camera has onboard RAM for temporary image storage you will need to allocate
RAM in the computer to use as a buffer when acquiring images. DynamicStudio will warn you
as shown below if this is the case:
You will now have to chose which type of memory you will use during acquisition:
1.   Right click the in the Devices view and set a check mark at "Advanced view"
2.   Select "Image buffer resources" in the Device tree view.
3.   Select the Memory mode to use.
4.   In the Device Properties view set the property "Buffer size" to ex. 1024 MBytes
5.   If you have chosen "Reserved memory" you will have to reboot the PC, otherwise the sys-
tem is ready for acquisition.
"Image buffer recourses" on page?238
12.3.4 Normal Use
When you start DynamicStudio 7 you will see the default database view:
You can change the screen layout, but this is the default and will be used throughout this
manual.
The top of the screen features a normal menu and toolbar. Everything in the right hand side is
related to the acquisition of images, while the left hand side is intended for managing both
images that has already been acquired and other data derived from them. The gray area in
the middle is the working area where you can display and examine acquired images as well as
data derived from them.
When you start DynamicStudio 7 you will see the StartPage at the right hand side. The
StartPage can be closed/opended under the View Tab or the Help Tab in the menu. The
StartPage contains practical short-cuts e.g. to create or open a database.
PAGE | 76
PAGE | 77
The new data structure introduced with DynamicStudio 7
With DynamicStudio 7 a new data structure (appearance in Windows file explorer) was intro-
duced to improve the data safety and to simplify the handling of data stored on disks. A new
icon is also introduced, when DynamicStudio 7 is installed, all databases present on the
machine will be shown with the new icon. In addition to this, the length of the file names of
e.g. stored raw images/data has been shortened to improve data safety. See the image below
showing the old and the new data structure as seen in Windows file explorer.
How to work with DynamicStudio 7 and the new data structure is described below. How to
open an old database, created with DynamicStudio 6 or earlier versions, and work with it in
DynamicStudio 7 is described below.
Top: Old database structure as seen in Windows file explorer. Bottom: The new data structure in
DynamicStudio 7.
Creating a Database
From the File menu you can open existing DynamicStudio databases or create a new one. To
create a new one click File/New Database... and specify name and location for a new database.
With DynamicStudio 7 databases have the new format as shown above.
Databases and how these are organized in Windows file explorer
With DynamicStudio 7 databases have the new format as shown in the bottom figure above.
Here, only a Folder with the database name is seen. The Folder holds the data and by double-
clicking on the database folder DynamicStudio 7 is starting up and loading the selected data-
base.
In DynamicStudio 6 or earlier versions the databases have the old format (structure) as shown
in the top figure above. In the folder in Windows explorer were the data is stored both a
Folder (holding the data) and a *.dynamix File (starting DynamicStudio and loading the selec-
ted database) with the same name are found.
Note: It is very important to 1) use the Tool inside DynamicStudio Handle databases to move,
copy or delete databases created with DynamicStudio. 2) For locally stored data (on the local
machine) use the DynamicStudio dedicated items found in Windows file explorer drop-down
list appearing when right-click on a DynamicStudio database. If databases are moved or
copied directly with Windows explorer data can be lost!
How to open old databases in DynamicStudio 7
To open an old database, created with DynamicStudio 6 or earlier versions, and work with it in
DynamicStudio 7 one of the following actions needs to be performed:
Recommended method:
PAGE | 78
PAGE | 79
Use the Convert Database Tool found in DynamicStudio 7 and convert the old database to the
new format. The converter can be found in the task bar of DynamicStudio under “ >Tools >
Convert Database” as described below.
Alternative method 1;
For locally stored data (on the local machine) use the DynamicStudio dedicated tools (see
image below) found in windows file explorer appearing when right-click on a DynamicStudio 6
(or older) database<name>.dynamix file. It is recommended to use "Convert DynamicStudio
database to latest format". This tool converts the database and asks if you want to open the
database. If "Open with DynamicStudio" is applied the database will be opened in Dynam-
icStudio 7 but remain its data structure in windows file explorer.
Alternative method 2;
Locate the database <name>.dynamix file from windows file explore and double-click on this.
The database will directly be opened in DynamicStudio 7. Note that the database will remain
its old structure (appearance in windows file explorer).
The Convert Database Tool
Use the Convert Database Tool found in DynamicStudio 7 to convert an old database to the
new format. The converter is found under the Tools Tab found in the task bar of Dynam-
icStudio under “ >Tools > Convert Database” as shown below:
When the Convert Database Tool is used the dialog below is shown. Locate the database to be
converted, select the<name>.dynamix and press open. (Several databases can be converted
at once as shown below).
Click on Open; the following dialog will be shown. You can directly Open the converted data-
base in DynamicStudio 7.
After conversion, the databases are found at the same location as before the conversion. The
structure is now updated to the new format.
Converting multiple old databases at once
To convert multiple databases: In DynamicStudio 7 open the Convert Database Tool. Select
(mark) all the <name>.dynamix files you want to convert as shown below. Click Open to con-
vert all the selected databases. The converted databases will be stored in the same location as
the original (old) databases. A dialog opens to confirm that the conversion has been per-
formed. The user is also asked it the last selected database should be opened in Dynam-
icStudio 7.
Acquisition Mode
To enter Acquisition Mode press Run/Enter Acquisition Mode or click the little green button
on the toolbar.
When you enter acquisition mode DynamicStudio will search for Acquisition Agents on the
local PC and/or other PC's on the local network. The agents found will be prompted for inform-
ation about various hardware devices connected to the PC in question.
PAGE | 80
PAGE | 81
Automatic device detection
Cameras and synchronization devices are auto-detected by DynamicStudio and appear auto-
matically in the 'Devices' list in the middle right-hand part of the screen: The auto-detection
will also detect if connection is lost to either of them and indicate this by changing the text
color to red.
The auto-detection may take a few seconds, but you need not do anything to tell the system
e.g. what kind of camera(s) are connected.
Adding devices manually
Only 'intelligent' devices can be auto-detected, while e.g. lasers or other light sources typically
cannot. These have to be added manually, by right clicking in the device list below the last
device shown. In the context menu select Add New Device... and select your laser from the
list:
Having selected a laser, the device list should look like this:
Device Properties
Each added device has specific device properties, these are shown in the “Device Property”
window. In the example shown below the device properties of a simulated camera are shown.
Please note that there are many different settings. For clarification the different properties
are normally grouped in different sections which can be expended.
PAGE | 82
PAGE | 83
Furthermore, these properties have different levels of depending on the users experience
and complexity of the task. With a right mouse-click in the device property window and UI
Level one can select the different modes and thereby can switch from the most important to
all possible settings in the view. For the given example the settings of the “UI Level: Expert” is
given below:
Device connection
When all devices in your system are listed it is time to tell DynamicStudio how they are con-
nected. Some synchronization units are very flexible with respect to where e.g. the camera
trigger cable should be connected, while other synchronization units allow only one specific
way of connecting the other devices.
From the View menu select Synchronization Cables to display a diagram of connected devices.
PAGE | 84
PAGE | 85
At first you should just see the various devices without any connections between them. If you
right click e.g. the arrow at camera trig you can Restore Default Connections..., which will sug-
gest a possible connection to the synchronization unit.
Provided of course devices really are connected as shown in the diagram, you are now ready
to start acquiring images.
Acquiring Images
In the default screen layout of DynamicStudio the main system control is in the upper right
corner of the screen. If Acquired Data is shown click the tab labeled System Control.
The system control is used to define some of the acquisition parameters :
l  Time between pulses : time between the two laser pulses, i.e. the time difference
between the two particle images
l  Trigger rate : Sampling frequency of the PIV setup
l  Number of images : specifies how many images are required in the acquisition
l  Single of Double frame PIV  mode :?in order to reach the minimum Time between
pulses, the camera has to tun in double frame mode.
Once those parameters are changed, they must be validated in order to be used by Dynam-
icStudio. This can be done by pressing either TAB or Enter.
If the validation is not done, and the user tries to click on one the acquisition button, the soft-
ware will not start the acquisition. A second click on the
button will be necessary to validate the parameters and start the process.
There are three different ways of acquiring images with DynamicStudio, corresponding to the
three topmost buttons in the right hand side of the system control window:
Free Run
In free run the camera is running freely, i.e. not synchronized with other hardware devices.
This means e.g. that in this mode the laser (or other light source) is not flashing, and the
frame rate of the camera does not necessarily correspond to the nominal value from the sys-
tem control. If ambient light is strong enough compared to exposure time you may be able to
see images and perhaps focus the camera.
The camera will continue acquiring data until you press Stop continually overwriting the oldest
images in the buffer. If you set the system up to acquire say 20 images, the last 20 acquired
will remain in the buffer after you press Stop so you can browse through them and store
them if you wish.
Preview
In preview mode all devices are synchronized, the laser is flashing and the camera is triggered
to acquire images at the rate specified in the System Control panel. It will not stop acquiring
images when the requested number of images have been acquired, but simply overwrite the
oldest image(s) with the most recent one(s). It will continue acquiring images until you press
Stop and the latest images acquired remain in the buffer for evaluation and possibly storage.
Note: For some cameras - especially High-Speed cameras - preview mode will result in bursts
of measurements. While each burst will have the correct frame rate, the overall data rate may
not be the same as when using the "acquire" mode.
Acquire
Acquire does exactly the same as Preview with the one exception that it stops when the
requested number of images have been acquired. You can of course stop it earlier by press-
ing the Stop button, in which case the allocated buffer will not be full, but hold the images
acquired until you stopped the acquisition.
Note: In all run modes acquired data can be saved to the database.
Storing and evaluating and Acquired Images
Whether acquisition stops automatically or because you click Stop, the last images acquired will
remain in the buffer for evaluation and storage.
To evaluate and/or store the images in the buffer click the tab labeled Acquired Data:
Evaluating images
When acquisition stops the very last image acquired will typically be shown on the screen and
with controls in the Acquired Data window you can browse through the images in the buffer
PAGE | 86
PAGE | 87
while the display window updates accordingly. You can even play back as a movie specifying
the time between each new frame in the dialog Time interval.
You need not play back all images, but can start part way into the buffer and stop before
reaching the end. To do this, click and drag the pink dots at either end of the green line in the
window above. The blue arrow indicates which image is presently shown on the screen and
can also be moved around by clicking and dragging with the mouse.
Storing images
While in the buffer images are still only stored temporarily in RAM. In the window with
Acquired Data you can choose to store the images in the database, meaning also that they are
transferred to the hard disk for permanent storage.
As with the playback described above you need not store all of the images, but may opt to
store just some of them using the same controls as used for the movie playback.
There are two buttons for storing images, Save in Database and Save for Calibration.
The first one is for normal data, while the second, as the name implies, is intended for cal-
ibration images of various kinds. If for example you've taken a picture of a ruler to determine
scaling, you would store the image for calibration.
Specifically identifying calibration images to the system is an advantage for later processing,
since such images are inherently different from other images and are generally processed
and used very different from 'normal' images.
When images are saved in the database they will appear in the database view in the left hand
side of the screen, from where you can access them the same as you would access old data
from an existing database.
12.3.5 Database Access
This help provides information about
Database Structure
Copying and Moving Ensembles
Database structure
DynamicStudio store acquired data in a nested tree structure. At the root of the tree is the
database, branching out into one or more project folders. Each project folder can hold both
calibration images and normal images stored in so-called 'Runs'. Every time you transfer data
from the image buffer to the database by saving, a new Run is added to the latest project and
time stamped for later reference. All project, run and ensemble labels can be later changed.
You can create new projects e.g. to separate different experiment configurations.
By right clicking on the different levels of the database, you can access to different operations
and properties
Ensembles
A core feature of DynamicStudio is the concept of ensembles, which represent a whole series
of data as a single icon in the database. Typically an ensemble corresponds to a Run and can
hold anywhere from a single to thousands of images or derived datasets. This greatly helps
you navigate among the very large amounts of data that can easily be acquired with modern
imaging systems.
The ensemble is thus the main data type in a DynamicStudio database and in connection with
data analysis we default to process entire ensembles producing entire ensembles of derived
data. If you wish to access and/or process individual datasets you can of course do so.
PAGE | 88
PAGE | 89
In the figure in the previous section the topmost ensemble has been renamed to 'Gain 0' and
the text right next to the name indicate that this ensemble contain 50 datasets. From the icon
we can see that the ensemble contain single frames and if we just double-click on the icon the
1st image in the ensemble will be shown on the screen. If we wish to have a closer look at indi-
vidual images in an ensemble, we can right click it and select Contents from the resulting con-
text menu:
Browsing inside the contents list the display window will update accordingly to show individual
images in the ensemble.
Project
The database can be fragmented into projects. For example a project can correspond to a spe-
cific flow setup configuration.
Calibration
If calibration images have been acquired and stored, a calibration folder will be created includ-
ing calibration images. Only one calibration folder per project can be created. From the
ensemble under the calibration, the specific operations are:
l  To Measure scale factor
l  To perform a calibration
l  To pre-process images (Image Processing Library and Image arithmetic)
Run
Each time, new images are stored, a new run will be created. From the ensemble inside a run,
the different analysis tools are available (mouse right click)
Copying and Moving ensembles  and Deleting images
Warning: Moving and copying ensembles may lead to loose image information and may cor-
rupt the database. The moving and copying of ensembles is the users responsibility. Such
operation must be carried out only in extreme necessity.
By right clicking on an ensemble, you can copy or move an ensemble. The ensemble will be
copied in the actual folder and named "copy of ensemble name". It is then possible to move it
into another run. To move an ensemble make a drag and drop with the mouse.
The run settings (such as dt between frames, cameras and so on) MUST be the same to move
ensemble from one run to another one.
Subsequently to expanding an ensemble, it is possible to delete images from it. Right hand
click the image you want to delete, and select "Delete". It is only possible to delete raw images
from ensembles, which has no child ensembles (as a result of a data analysis)
12.3.6 Delete and Restore
When enabled, the database feature 'Delete and Restore' will enable a 'Waste Bin', where
items deleted from the database are temporarily stored, providing the option to restore the
data to their original location in the database.
By default 'Delete and Restore' is enabled, but it can be disabled via DynamicStudio Options.
If 'Delete and Restore' is enabled, data deleted from DynamicStudio databases are not imme-
diately deleted, but moved to a Waste Bin named 'Deleted Items'. If the deletion was a mis-
take it is possible to restore the deleted items to their original location in the database.
Deleted Items  record
When 'Delete and Restore' is enabled a record named 'Deleted Items' will appear in the data-
base tree just below the root database icon.
PAGE | 90
PAGE | 91
When the feature is first enabled the waste bin will be empty as shown by the icon in the left
of the two figures above. Once data have been deleted (and thus moved to the waste bin) the
icon will change to the one shown on the right, indicating that the waste bin contains items,
which can potentially be restored.
The properties of the Deleted Items record will indicate how much data is in the waste bin:
Indicate parents  to deleted items
When data is deleted the parent record is marked in the database tree with a small indicator.
This tells the user that deleted child items are in the waste bin, from where they can be
restored.
The indicators can be toggled on and off by pressing Ctrl-I, or by right-clicking the waste bin
and selecting "Indicate parents to deleted items" or "Do not Indicate parents to deleted
items". By default the indicators are on.
Restoring deleted items
To restore deleted items right-click the parent and select 'Restore deleted items (recursive)'.
This will restore child datasets and possibly entire database branches descending from the
record you right-clicked. The record from where you start need not itself have an indicator
that deleted child data are in the waste bin. The system will scan through all descendant data-
sets restoring all deleted items recursively. This means also that in the extreme case you may
right-click the database icon itself, select 'Restore deleted items (recursive)', and the system
will restore all deleted items in the waste bin back to their original location in the database.
Emptying the waste bin
To empty Deleted Items right click the Deleted Items record and select 'Empty Deleted
items'. All data in the waste bin will now be permanently deleted.
To limit the amount of data in the waste bin you are also prompted to clean it up when a data-
base is closed:
This prompt will not appear if 'Delete and Restore' is disabled or if the waste bin is empty.
The first 3 buttons are self-explaining, you may delete all items, leave all items, or delete old
items, keeping only items deleted within the last week.
To prevent this message from appearing again click 'Show alternatives...', which will open the
'Database' tab of the 'Options' dialog:
With respect to the handling of deleted items you have three possibilities here. The last one is
to be prompted as shown above every time a database is closed, the other two will per-
manently delete all or delete old items without prompting the user. If the prompt has been
disabled and you wish to enable it again you can access the Options dialog via the 'Tools' menu
in DynamicStudio.
Note: Even if you set up the system to delete items older than 7 days the waste bin may in fact
contain items that are older. This is because the clean-up takes place when the database is
closed, not when it is opened. Imagine that you close a database and decide to keep deleted
PAGE | 92
PAGE | 93
items that are less than 7 days old. If two weeks pass before the database is opened again it
may then contain deleted items that are 3 weeks old.
12.3.7 Working with  the Database
DynamicStudio databases are based on ensembles. An ensemble is a collection of data, which
logically behaves like one single dataset. If - for example - the user has acquired 100 PIV
images from 1 camera, these images will appear in the database as one ensemble. If two cam-
eras had been used, the images would be represented by two ensembles - one for each cam-
era.
Working on ensembles
Most data processing is performed directly on the ensembles. Lets say that you want to per-
form a PIV analysis of the two above-mentioned ensembles. By selecting -> "Analyse" from
the context menu of an ensemble and then "PIV Signal" -> "Adaptive Correlation" , the PIV ana-
lysis is being carried out on all 37 image pairs in the ensembles. The result is another
ensemble - containing 37 Vector maps
Methods, which require a long range of successive input, are easily handled via the
ensembles. For instance calculating the average of the above-mentioned 37 Vector maps.
From the context menu of the vector map ensemble select "Analysis"-> "Statistics" -> "Vector
Statistics" and the average of all 37 Vector maps will be calculated automatically.
Expanding ensembles
It is also possible to work on individual elements of an ensemble. From the context menu of
the ensemble select "Contents", the ensemble will open and you can see the individual data.
Double clicking any data in this list will open op this particular dataset of the ensemble.
Analyzing a part of an ensemble
Selecting "Analyze.." from the context menu of a dataset will make it possible to perform an
analysis on this dataset only. The result will be a new ensemble containing one dataset only.
Before this ensemble is created you will be asked if you want to carry out the calculation on
the remaining datasets in the parent ensemble first
PAGE | 94
PAGE | 95
User Selected Data
Some analysis methods require data from more than one ensemble to be specified. An
example is the masking function. Here it is necessary to select the mask before the masking is
carried out. The selection method is used for this. Data selected by this method is called "User
Selected data". When trying to perform an analysis, which require a user selection, without
having performed the selection, this method's info box will contain a statement in red, that
the selection is missing.
Another example of how to use the select method is when performing an average cor-
relation. Here it is not mandatory to have a selected dataset. However, if a vector map has
been selected before the analysis is called, this vector map will be used to offset the inter-
rogation areas.
Pairing ensembles
It is possible to pair multiple datasets from two ensembles. This could for instance be neces-
sary when adding to sets of image maps. If the analysis method: "Image Arithmetic" - "add" is
carried out on an ensemble containing 37 image maps, while another ensemble containing
another 37 image maps has been "selected", the add function will add image 1 from the input
ensemble to image 1 from the selected ensemble, image 2 from the input ensemble to image
2 from the selected ensemble and so on.
PAGE | 96
PAGE | 97
Multi selection
It is possible to select multiple ensembles and perform the same action on all the selected
ensembles. There are three ways to select multiple ensembles:
l  Hold down the Ctrl key and click on the ensembles that are to be selected
l  Select the first ensemble and while holding down the Shift key select the last ensemble of
a series.
l  Use the arrow keys to move to the first ensemble, hold down the shift key an move to the
last ensemble to be selected
To perform the same analysis on the selected ensembles, then from the context menu of the
selected ensembles, select analysis. The following analysis will be carried out on all the selec-
ted ensembles.
Multi selection works on other database operations such as delete, open etc..
12.3.8 Ensemble Linking
As implied by the name Ensemble Linking will allow you to 'Link' two or more DynamicStudio
data ensembles such that anything you do to one of the linked ensembles is automatically
done to the others as well. This can be used for Analysis, but is most often used when dis-
playing multiple datasets side by side for comparison; The Linked Ensemble display could for
example show Input and Output of some analysis, or represent images from several cameras
that were acquired simultaneously. If you zoom in on a detail in one display the other display
(s) will automatically zoom in on the same area and if you pan one display the other(s) will auto-
matically pan also.
Image pairs acquired with the DualScope will for example be stored as Linked Ensembles by
default so opening (or closing) the display of one will automatically open (or close) the display
of the other also. (Read more about "DualScope" on page?408).
To establish the link first select (Ctrl-Click) each of the ensembles that you wish to include in
the link, then right-click either of them and pick 'Link ensembles' in the context menu:
Small chain symbols will now appear on each ensemble icon in the database to visualize that
they have been linked.
When you open either of the linked ensembles for display the others will automatically be
opened also and display windows be distributed and scaled for synchronized display. Similarly
closing either of these displays will automatically close the others as well. You can in principle
link any number of ensembles, but in most cases you will be comparing 2, 3, 4 or perhaps 6 dis-
plays side by side.
In the example used here, we've linked 4 ensembles for side-by-side comparison; One input
image, one intermediate result and two outputs.
The images are from measurements on arc welding, where PIV?seeding has been added to
the shielding gas and droplets of molten metal is also flying around.
Image processing is applied in an attempt to separate the small PIV?seeding particles from the
much larger metal droplets so the two can subsequently be analyzed separately.
Despite narrow band filtering the welding process itself generates a lot of background noise
so the first step attempts background removal, leading to an intermediate result where seed-
ing particles and metal droplets remain mixed, but background intensity has been reduced.
Further processing of this leads to separate images with large and small objects respectively.
Ensemble linking is used to quickly and easily zoom in on different image regions to assess
how well the analysis performs and perhaps see where it fails and thus needs modification.
Top left is the input image, bottom left has the background removed. Top right is the large
objects (i.e. the metal droplets) and bottom right is the small objects (i.e. PIV?seeding particles
in the shielding gas):
PAGE | 98
PAGE | 99
There is a foreground object in the bottom left corner of the image. Zooming in here con-
firms that the background removal takes care of the secondary scattering from this object:
From there panning to the top right corner shows that the large objects are indeed in the top
right image, while the bottom right one contain only small objects:
PAGE | 100
PAGE | 101
Please note that Ensemble Linking is stored in the database along with the data and thus
remain even after shutting down and restarting DynamicStudio and reloading the database.
The ensembles will remain linked until you actively cancel the linking by right-clicking either of
the linked ensembles and selecting 'Unlink ensembles' in the context menu:
12.3.9 Calibration  Images
Acquiring and saving calibration images
The acquisition of calibration images is no different from any other image acquisition, but
when you store the images you need to tell the system that they are intended for calibration
rather than measurements. This is done by selecting 'Save for Calibration' rather than 'Save in
Database' in the 'Acquired Data' window:
This will open a dialog prompting you to select one or more custom properties that needs to
be stored along with the images.
If for example the images are to be used for an Imaging Model Fit, the relevant custom prop-
erty is 'Z', describing the position of the calibration target relative to the lightsheet.
PAGE | 102
PAGE | 103
Press 'OK' and you're finally requested to specify a value for the custom property. Continuing
the example of Imaging Model Fits Z=0 is typically in the center of the lightsheet, while Z=-2 in
the example below may represent a position behind the lightsheet as seen from the cameras
point of view.
In other contexts such as the calibration of a concentration-LIF experiment, you will of course
choose the custom property 'Concentration' and then type in the known concentration that
apply for the calibration images you are about to save in the database.
Press OK and images will be stored and appear in the database in a group named 'Calibration':
If you right-click an ensemble of calibration images and select 'Contents' you will be able to
browse through the images in the ensemble and for each the custom property you entered
will be shown in the record properties. You can also assign custom properties to the ensemble
as a whole and whatever value you enter here will apply for any member of the ensemble
that does not have a specific property value of its own. Custom properties can also be
assigned to ordinary measurements, but are rarely used for anything and thus optional. For
all but the simplest calibrations custom properties are mandatory, since they are a vital part of
any calibration.
Tip:
It is possible to move images from another ensemble in the database to the calibration record
by simply dragging the ensemble onto the calibration record. If the project does not contain a
calibration record it can be created by right clicking the project and selecting the menu option
"Create Calibration Record". It is however not possible to move images and ensembles out of
the calibration folder and into a normal acquisition "Run".
12.3.10 AgentHost service
After having installed any part of the DynamicStudio installation, AgentHost service will be run-
ning. AgentHost is the key component in the distributed environments that are supported by
DynamicStudio.
Distributed Analysis, Acquisition and Database is based no using the local area network
between the PC's.
Note: DynamicStudio supports only IPv4.
When AgentHost is running it is possible to use the PC as a Database-, Analysis- or Acquisition-
agent. If AgentHost is not running the PC can not be used by other DynamicStudio install-
ations.
AgentHost service is ready as soon as Windows has started up. You do not need to log in on
the PC to have AgentHoset started.
A "DynamicStudio Icon" is shown in the notification area when AgentHost serive is running.
Stopping and Starting AgentHost service
If you want to start/stop or prevent AgentHost service from running when the PC has booted
do the following:
1.   Login as an administrator
2.   Click Start -> Control Panel -> Administrative Tools-> Computer Management -> Service
and Applications -> Services
3.   RightClick on the service named "DynamicStudio Agent Host Service" and select Prop-
erties
4.   Select the Tap named "General
5.   Change "Startup type:" to
"Automatic" to have AgnetHost started when the PC is booted.
"Manual" : to prevent AgentHost service from starting when the PC is booted.
6.   At Service Status you can click
"Start" to start AgentHost Service
"Stop" to stop AgentHost Service
7.   Click OK to close the dialog.
PAGE | 104
PAGE | 105
Stopping and Starting AgentHost service remotely
It is possible to remotely start and stop the AgentHost service, you must be an administrator
on the remote computer in order to do so:
1.   Login as an administrator
2.   Click Start -> Control Panel -> Administrative Tools-> Computer Management
3.   Right click "Computer Management " and select Connect to another computer.
4.   When connected click Service and Applications -> Services.
5.   Follow step 3 trough 7 in order to control AgentHost service on the remote computer.
Manually  Starting and Stopping AgentHost
When AgentHost serivce is not running you can manually start AgentHost:
Click Start->All programs->Dantec Dynamics -> Remote Agent for DynamicStudio
To stop AgentHost right click the Icon in the notification area and select "exit".
12.3.11 Known  issues
When using Distributed acquisition it is not possible to modify the amount of memory to use
as reserved memory on the Agent PC's.
When using Distributed acquisition it is not possible to use the already reserved memory for
acquisition on the Agent PC's
Reason:
New restriction has been added to the OS that makes it impossible to access level 0 device
driver such as DXMapMem.
Two ways exists to solve this issue:
Solution 1:
Starting up Agent host service with a specific login name and password that has the necessary
rights to access Level 0 device drivers such as DXMapMem:
On each agent host do the following:
1. From Control Panel\Administrative Tools select and open Services
2. Find the service named "DynamicStudio Agent Host Service" and double click the service:
3. Go to Tap "Log On":
PAGE | 106
PAGE | 107
4.Instead of "Local System account" select "This account:" and enter the login name and pass-
word.
5.Go to Tap "General" and click first the button "Stop" and then the button "Start".
Now when the service is started the login that you described in step 4 will be used and this
login has the necessary rights to access hardware the issues are solved.
Solution 2:
Instead of letting AgentHostService start up Agent.AgentHost.exe do this manually.
On each agent host do the following:
1. From Control Panel\Administrative Tools select and open Services
2. Find the service named "DynamicStudio Agent Hoset Service" and double click the service
3.Set Startup type to "Disabled".
4.Click the button "Stop"
5.Clock OK. Now the service is stopped, and will not start up again on the next boot.
6. Click Start->All programs->Dantec Dynamics->Remote Agent for DynamicStudio
Now the Agent.Host Icon will appear in the notification tray, and the Agent PC is now ready
for use.
You will have to manually start up Agent host each time you have rebooted.
12.3.12 Database with  Demo Data
On the DVD?that you received when your imaging system was delivered and installed, you can
find several Zip-Files that contains a packed DynamicStudio databases with data from many dif-
ferent experiments. These data reflect a wide range of applications and introduce the analysis
methods in DynamicStudio that are most widely used to process the raw data with the goal to
extract the physical information.
The Databases can be opened by going to "File-> Open Compressed Database".
Than select one of the Demo Database files with the ending ".dZip"
To fit the database onto the DVD some intermediate data, which is not strictly necessary to
understand the processing and work with the examples, was deleted from the database. This
is indicated by icons that are grayed out, like in the Voxel Reconstruction example shown
below
If you try to open these you will only open an empty window, and if you try to apply an analysis
on them, you will get the following waring indicating that the parent data is missiing:
In order to recompute the data simply right mouse click on it and select "Perfrom Analysis
again"
PAGE | 108
PAGE | 109
Databases
with DS 7.0 four new demo data bases were introduced, or the old ones updated. along with
the analysis you can find images and read-me files in the database that explain the processing
l  Demo_Data_base_01_EDU PIV
l  Demo_Data_base_02_Microfluidics
l  Demo_Data_base_03_Shadow
l  Demo_Data_base_04_VV
EDU-PIV Demo Data
this data base gives you an introduction to the basic application of PIV?data and how it is ana-
lyzed. The data is based on a jet-flow from our educational PIV system (EDU PIV). Two dif-
ferent analysis approaches can be found. In a first basic one the vectors are computed,
filtered, averaged, and some profiles and Power-spetrcums are extracted. In a more
advanced 2nd section, the flow pre-filtered by a POD before the pressure fields are com-
puted and vector uncertainties are computed and propagated into some vortex criteria.
Microfluidics Demo Data
this data base gives you an introduction to the concept of image pre-processing and masking.
Before a velocity profile is extracted. Note that in this case no proper scaling is given.
Shadow Demo Data
this data base uses the old and the new Shadow processing routines to perform droplet size
and velocity analysis. Different images and plots are extracted from it.
VV Demo Data
this data base discusses the concept of Volumetric Calibration Refinement, as well as all the dif-
ferent Volumetric analysis methods for 3D LSM and 3D PTV. To show the impact of the cal-
ibration refinement different images are added and can be re-created by dragging the
different data-sets above each other.
12.3.13 Combined PIV-DIC measurements
Additional Cabling
Sync ?Out of Trigger Box (BNC backside) to -> IN 1 PIV System
Trigger Out (BNC front side) to -> IN 2 PIV System PIV System
Calibration
The systems can be calibrated separately from each other. But also the PIV system needs at
least a Pin-hole calibration even if only one camera is used. The target plane must be the light
sheet plane.
Measurement settings
PIV-System
Connect the synchronization as described in Figure 1. Select the image mode you need and
set the system for external synchronization, external start. Therefor got to the timer box
device settings and set the
l  Start: “Start on external trig”
l  Mode: “External”
l  Use Trigger enable input: “Yes”
PAGE | 110
PAGE | 111
Measurement Procedure
l  (DIC) Acquire at least one image of the object without load/flow. This shall be the ref-
erence state of the surface. Store these in on measurement series (evaluate it and check
the quality).
l  (DIC) Use Pre/Post trigger mode for the measurement under load. E.g. 5 frames Pre-trig-
ger. Set the number of images at least 5 higher than the number of frames you need (to
cover the pre trigger images).
l  (PIV) Prepare the Acquisition of PIV system by enetering the frame-mode and the the
amount of images you want to acquire and activate thy system clicking on “Acquire” Now
the system waits for a start trigger
l  (DIC) Start the synchronized measurement e.g. by mouse click in Istra4D or TTL at Trigger
In. This soft trigger creates a TTL pulse in the timer box that starts the synchronized PIV
measurement. Note that starting the Pro/Post trigger mode also generates a sync pulse.
Hence the DIC system needs to be enabled before the PIV system is set to “Acquire”
l  (DIC) Check the images. The first laser should be visible at the images defined by the Pre
trigger. This is the first image recorded by PIV. You will see more images with laser illu-
mination than defined in the PIV software, which does not has an influence of the PIV
measurement
l  (DIC) Save the images starting from the Pre trigger image.
l  (PIV) Save the images
Evaluation and Export of the DIC system
l  Add the reference image (first series) at pos 0 in the evaluation series.
l  Perform a standard evaluation (be aware, that you may not be able to evaluate the inter-
section of the laser beam with the surface of the object).
l  Define an object coordinate system in a way that the XY plane is in the surface of the
object.
l  Select the other visualization parameter accordantly (e.g. Step 0 = reference step).
l  Export the Data as HDF snapshot.
Evaluation of PIV  data and Import of the DIC Data
l  Preform the 3D calibration
l  Incase 2D 3C Data chose a stereo method you like using the stereo processing based on
dewarped images is recommended
l  In case of 2D 2C measurements dewarp the images, based on the 3D calibration
l  Analyze the PIV Data
l  Import the DIC Data by having a run selected and then select ->“File” -> ”Import” -> “import
HDF5 data to selected Run”
l  Opening the data enables the 3D view.
l  Drag-Drop the desired PIV analysis into the 3D view:
You can also drag the dewarped raw images of your PIV measurement into the 3D view, if you
are interested. Or even other PIV results, as long your object has a metric calibration.
Different coordinate Systems
Please note, that if your calibration planes are different between the PIV and the DIC meas-
urements it is necessary to transform the DIC coordinate system so that it fits the PIV coordin-
ates. For this purpose you best measure the distances between your calibration target center
markers.
The transformation is achieved by right clicking in the visualization of the DIC results and
chose “Display options”. In this Surface Setup UI one can select X-Y-and Z- translations of the
PAGE | 112
PAGE | 113
coordinate system defined in mm. As well as the rotations around the X-Y- or Z- Axis in
degree.
In the given example one can clearly see the light sheet positions of the PIV experiment. And
for fine adjustment, it is also possible to drag the dewarped image into the 3D ? view, this can
be used for fine-alignments.
l  For this purpose switch off the perspective view.
l  Right click in the visualization, > “View” > unselect “Perspective Projection”
l  Select a view that aligns with your coordinate system. In the example from below it is the
X-Y View
l  Right click in the visualization, > “View” > select “xyz view” according to your initial coordin-
ate system.
l  In case your coordinate systems does not align with the standard views one can use the
mouse to rotate the view
How to algine the 2 coordinate systems with
help of the raw images
Final visualisation of the combined PIV-DIC
measurement
PAGE | 114
PAGE | 115
13 Acquisition
This section describes the following operation:
13.1 Acquisition  Manager
This control makes it possible to create a grid definition managing a series of acquisitions
which can be performed automatically.
In the database the grid definition is stored in the current project. For every acquisition a new
run will be created under the project to store acquisition settings and images.
For troubleshooting click here.
13.1.1 Opening the Acquisition  Manger
To open the Acquisition manger either click on an acquisition grid saved in the database, or
click Run->Acquisition Manager.
In order to make the Acquisition Manger work with the Acquisition system, DynamicStudio
must be in Acquisition Mode, and all devices must be ready for Acquisition.
Note:
When entering Acquisition Manger a dialog will appear stating a warning that the Laser warn-
ing dialog will be disabled. This dialog normally appears just before starting an acquisition and
pulsing the laser
When start running the steps in Acquisition Manager no Laser warning will take place!
You can not enter Acquisition Manager without agreeing to disable the Laser warning.
When open the Acquisition Manger Dialog look like this:
13.1.2 Description  of possible settings
X, Y, Z
This is the position of the traverse in millimeters.
Time between pulses
This is the time between pulses of the laser in micro seconds.
Trigger rate
This is the rate at which the event are happening.
For single frame mode the rate of each laser pulse.
For double frame mode the rate of each double laser pulse.
this is in Hz.
Trigger delay
This is used when Trigger mode is set to External, in device properties of the synchronizer.
This is the delay introduced after every trigger signal in micro seconds.
Number of images
This is the number of images to acquire at the specific traverse position.
Analysis  sequence
It is possible to have the data that is acquired analyzed. The way this is performed is that a
selected Analysis sequence is applied on the acquired data. To select an analysis sequence
please "Selecting Analysis seqeunce" on page?119
Start delay?
This is the delay introduced when an acquisition is started,
It will disable the cameras during the delay but everything else will run.
Prompt
If checked, a dialog will appear in which you will have to click "OK" in order to continue to the
next row.
13.1.3 Buttons in  Acquisition  Manager dialog
Save
Clicking Save will save the work done in Acquisition Manager to a Grid Definition record in the
database in the current selected Project.
NOTE: You can open the Grid Definition record in Acquisition Manger by double clicking the
Grid definition record in the database.
Run all
Clicking Run All will start the acquisition specified in Acquisition Manager.
Stop
Clicking Stop while Acquisition Manager has started an Acquisition will stop the acquisition.
PAGE | 116
PAGE | 117
Postpone Analysis  until all acquisition done
If "Postpone Analysis until all acquisition done" is checked, no analysis will take place before all
acquisitions specified in Acquisition Manger has been performed. Otherwise analysis will be
done right after one acquisition has been performed.
13.1.4 Working with  rows in  the Acquisition  Manager
Most actions are done via the context menu for a Row (right clicking a row index number). The
context menu looks like this:
Generate Grid
"Generate Grid" on the next page.
Insert new
When selecting Insert new a new row will be entered.
Delete
Will delete the currently selected row
Select All
Will select all rows
Move to position
Will move the Traverse to the position given by x,y and z.
Run on position
Make an acquisition based on the settings in the selected row.
Run From position
Start acquisition at the selected row, and when done, continue with the remaining rows.
Run naming convention
"Naming convention" below
13.1.5 Naming convention
Right clicking the grid and selecting naming convention enables the user to select between
three different naming conventions on how each run should be named:
l  Creation time (time and date of creation of the run)
l  Position (the x, y, z position entered in the grid will be used for naming the run)
l  Index (the index will be used for naming the run)
13.1.6 Generate Grid
It is possible to generate rows automatically. This is done using the Grid Generator.
Right click a row in the Acquisition Manger dialog and select "Generate Grid". This will bring up
the following dialog:
Start by choosing the right Grid type. Here it is possible to select between 1D, 2D, or 3D sys-
tems meaning 1, 2, or 3 axis traverse positions.
In the dialog above, a 1D system has been selected.
Now select the start position, number of steps, and the size of each step (all in mm).
When clicking OK, five new rows will be created.
Settings for the new rows created (apart from settings for x, y, and x)?will be based on existing
rows in the Acquisition Manager.
PAGE | 118
PAGE | 119
13.1.7 Selecting Analysis seqeunce
To select an analysis sequence to be performed on the acquired data, just click the drop down
button in the cell. This will make a list of available analysis sequences drop down:
Just select the analysis sequence, or if more information is needed select the "<<Browse>>"
entry in the list. This will bring up the Analysis Sequence library dialog as shown below:
Here you can select the analysis sequence that you want to have performed on the acquired
data.
As can be seen in the example above, an analysis sequence might have external depend-
encies. Here it is calibration for each of the camera, to be used when analyzing the Stereo PIV
Processing.
NOTE:External dependencies must be marked as Input for Analysis before opening the Acquis-
ition Manager.
Removing a selected Analysis sequence from a row
To remove a selected analysis sequence from a row in the Acquisition Manager, just select the
empty entry in the drop down list.
Analysis  Sequence execution
1.   First it will be checked if the necessary “External dependencies” defined in the sequence
(if any) has been marked in the database. If not, no analysis will be performed and the sys-
tem will stop with an error.
2.   If the Analysis Sequence only holds one parent, each and every acquired image ensemble
will be treated with the specified Analysis sequence.
3.   If the Analysis Sequence holds more parents, then it will be checked if the necessary par-
ents just have been saved as part of the acquisition just done. If check is ok, only one exe-
cution of the sequence will be performed. If the check fails, the analysis will be skipped
and the system stopped with an error.
13.1.8 Troubleshooting?
Problem                                                                                  Action
The manager hangs after first pos-
ition
Make sure that the camera is connected both physically
and in the synchronization diagram. The Acquisition
manager does not run preview or free run so
everything needs to be connected.
First try to make a single acquisition from the System
Control window. This will indicate if the system is cor-
rectly set up.
The traverse does not move                See troubleshooting under Traverse Control
Not all steps in the selected analysis
sequence are performed
Check if the analysis sequence selected have external
dependencies, if so, that these are not selected in the
database before entering the acquisition manager, ana-
lysis steps requiing external input will not be per-
formed.
13.2 Device  registration
PAGE | 120
PAGE | 121
13.2.1 Registering 3rd party devices to unlock and enable full control of
them  via  DynamicStudio
Non-original Dantec Dynamics devices - such as a camera bought from a 3rd party supplier ?
cannot by default be used under the DynamicStudio software platform. To unlock and enable
full control of the device via DynamicStudio, you can register your 3rd party device with Dan-
tec Dynamics. When registration is completed, you can benefit from all advanced capabilities
of DynamicStudio like e.g. synchronization of camera with lasers or other cameras. Registered
devices will work under DynamicStudio with the same functionalities as original Dantec Dyna-
mics devices of the same type.
13.2.2 Requirements
In order for registration to be possible it is required that your device is compatible with the
DynamicStudio software. For precise information, please consult your Dantec Dynamics rep-
resentative.
13.2.3 How  to register
If you have a 3rd party device that you wish to use together with your Dantec Dynamics ima-
ging system, simply connect it to the acquisition computer, start DynamicStudio and enter
Acquisition mode.
If the device is compatible, DynamicStudio will automatically recognize it and add it to the
Device Tree as an unregistered device. The red text “Unregistered device ? Please register”
will be shown following the device name.
To start the registration process, right click on the device and select the entry “Register
devices”. Doing so will bring up the “Register devices” dialog.
“Register devices” Dialog
This dialog will list all the unregistered devices that are currently connected to the system. At
each device there is a checkbox, and by default all devices are checked to indicate that the
registration will be valid for all devices. If there are some devices that you do not wish to
register, please uncheck these devices.
The dialog will also list the dongle serial number that is currently used. You have the possibility
to add more dongle serial numbers to this list, if you have more than one dongle for Dynam-
icStudio. When you register you must make sure that all your dongle serial numbers are
listed. Only dongles listed will enable use of the devices you are registering.
When you have checked the devices you want to register and added all the relevant dongle
numbers, click “OK”. A “DeviceKeys.zip” file is created holding information on each of the
devices checked in the dialog, as well as the dongle serial numbers listed. The file is by default
placed on your desktop.
Please send the file to Dantec Dynamics support: support@dantecdynamics.com
In a few days after Dantec Dynamics support has received you application you be will con-
tacted.
NOTE:
PAGE | 122
PAGE | 123
If you instead ignore the warning that you have unregistered devices connected to your sys-
tem, and try to run the system by either Free run, Preview or Acquire, the system will not
start acquisition, but a message box will appear asking to register the device. If you wish to
continue with image acquisition without registering any device, please disable or disconnect all
unregistered devices to proceed.
Dantec Dynamics  provides  quotation for the registration
If not all devices are purchased at Dantec Dynamics you will be contacted by our sales team
with a quotation for the device registration. The price for the registration will vary depending
on the number of the devices you wish to register as well as the brand/model of the devices.
If you accept the quotation, please contact the sales person who sent it to you, to officially
order the registration.
13.2.4 Receiving the “RegisteredDeviceKeys.zip” file
When your order has been processed, you will receive an email including a
“RegisteredDeviceKeys.zip” file attached. You need to import the file into DynamicStudio on
your acquisition computer. This is done from the File menu and then select Import->Ap-
proved Device keys…
Selecting this menu entry will bring up a file open dialog where you can locate the
“RegisteredDeviceKeys.zip” file to import. Having imported the file successfully, Dynam-
icStudio shows the following message:
The listed devices have now been registered to be used on the current installation of Dynam-
icStudio and with the listed dongles.
When you now enter Acquisition mode, the device will be available for use just like any other
Dantec Dynamics device.
13.3 Reporting  acquisition  settings
From an existing data base, it is possible to generate a file including all the acquisition para-
meters, and hardware description. This html file can be used to document the data acquis-
ition.
l  From a run icon in the data base tree, do a mouse right click and select "generate acquis-
ition setting reports". Type in the file name and storing location.
PAGE | 124
PAGE | 125
13.4 Storing  and  loading  acquisition  settings
When a new data base is created, DynamicStudio loads setting which are stored in the acquis-
ition device (last used ones).
To use the default configuration hardware configuration, you can reset the acquisition sys-
tem
l  from the tool bar, select run
l  under acquisition system, select "reset acquisition system"
To re-use an old acquisition configuration, you need to copy it from the existing data base to
the acquisition devices:
l  open the old data base which includes the settings you want to re-use.
l  from the system control window, press the store icon.
l  The settings are now stored in the acquisition system
l  If now you create a new database, it loads the settings you have just stored from the
acquisition system
l  If you open an existing database, you can load the settings by right clicking from the run
icon (select load acquisition settings)
13.5 Imaging  Setup  Assistant (ISA)
The Imaging Setup Assistant helps the user to configure, plan for and setup an imaging exper-
iment. For e.g. a classic Stereo PIV setup several parameters are of great importance: The
Image overlap, the depth of field, the Scheimpflug angle, the camera angle, the time
between laser pulses, as well as the particle image diameter in pixels. Using ISA these para-
meters and their influence on the planned measurements can be investigated.
For simplicity the user can select one of 15 predefined template configurations. These include
both air and water as medium and covers typical Planar, Stereo and Volumetric setups (1, 2 &
4 cameras respectively). The templates can be used as is or modified to match a planned
experiment. By selecting a suitable window orientation relative to the cameras line-of-sight dif-
ferent water tank geometries and camera arrangement can be simulated. ISA is integrated
with the Cameras and Lenses supported by DynamicStudio. For laser based systems light
sheet properties are inserted manually.
This manual contains:
l  ISA Step-by-Step instructions (general Imaging setup)
l  Example 1. Planar PIV, single camera setup (2D2C PIV)
l  Example 2. Stereo-PIV, two camera setup (2D3C PIV)
l  Example 3. Volumetric, four camera setup (3D3C PIV)
13.5.1 ISA Step-by-Step instructions
From the start page you can load a previously saved setup or create a new one by clicking
"Add New" and picking the template that most resemble the intended experiment (see image
below)
PAGE | 126
PAGE | 127
You can select from 15 predefined configurations to be used as a starting point. Cameras can
be removed or additional cameras added to an existing design. Once a configuration is selec-
ted the user can give the setup a unique Setup Name and Save the configuration for future
reference.
The selectable configurations includes both air and water as medium as well as different water
tank geometries. Starting with a predefined water configuration the user can change the
refractive index of the medium to match other liquids. A global coordinate system with
Bounds in X, Y, Z at (-300; 300; -200; 200; -300; 300) in mm is created by default, but this can
be changed by the user. In the water examples the distance from the camera to the tank win-
dow can be changed to further modify the size of the tank to match the users’ experimental
setup. The window orientation relative to the camera line-of-sight can also be changed.
The complexity of the imaging setup templates increases left to right and top to bottom:
The topmost setups describe measurements in air, where we need not worry about the
effects of refraction. The remaining rows show measurements from air into water through a
window, where the topmost of these setups have the camera normal to the window. This is
the recommended solution when measuring through a window, since looking through a win-
dow at an angle makes it much harder to focus and align camera views. If the cameras cannot
be normal to the window, use a window parallel to the lightsheet as shown in the last row. In
most cases this will allow the camera(s) to be "almost" normal to the window, i.e. close to the
recommended solution.
ISA?also offers the option to use a window that is normal to the lightsheet, but in this con-
figuration the camera(s) will have a significant angle to the window and/or the lightsheet mak-
ing both focusing, alignment and subsequent analysis very challenging. This kind of setup
should be attempted only as a last resort and can only be created manually by modifying one
of the setups above.
The 3 leftmost setups are 'Planar' and have just a single camera. Here we need not worry
about aligning the field of view of multiple cameras.
The next two columns are 'Stereo' setups with two cameras, where it is important to align the
fields of view so both cameras look at the same part of the experiment. The first column has
both cameras on the same side of the lightsheet while in the second they are on opposite
sides. Both are feasible, but the latter is a bit more challenging in terms of calibration and
subsequent analysis; Combining data from two (or more) cameras is easier when they are all
on the same side of the lightsheet. It is not particularly hard when they are on opposite sides,
but the odds of making simple mistakes increase significantly.
The six rightmost setups are all 'Volumetric' with four cameras in 'Typical' arrangements; 'Fan',
'Plus' or 'Cross'. The last of these require two-axis Scheimpflug mounts.
Aligning, focusing and calibrating cameras in a Volumetric setup is often challenging so for
measurements in water the templates all use windows normal to the camera/lens axis. Any
other window orientation will only make a difficult task even harder.
By manually modifying the setup you can use other window orientations, but if possible con-
sider using wedges or prisms in front of each camera to make the "window" normal to the
optical axis.
13.5.2 The Medium  Bounds and Medium  Refractive Index
Start with defining the Medium Bounds, (as default the Bounds in X, Y, Z are at (-300, 300; -
200, 200; -300, 300) in mm.?
In an imaging setup in a liquid fluid the refractive index is important as it determines how
much light is bent, or refracted, when entering a material. By selecting one of the predefined
water configurations the user can change the medium Refractive Index to match other
liquids. At atmospheric pressure gases have a low density and a refractive index close to 1.
(Refractive Index is defined as n = c/v; where c is the speed of light in vacuum and v is the
phase velocity of light in the medium.)
13.5.3 The Measurement Volume - Center and Size
The Measurement Volume (MV) is used to define the desired size of the region/volume to be
“imaged” inside the MV. The Volume Center is fixed at (0,0,0). A light sheet is always parallel to
the X/Y plane with the sheet depth centered on Z=0.?
Example: In the figure below the defined Measurement Volume (MV) is represented by the
white box within the light sheet (green). The FoV and DoF, achieved with the current camera
and lens, is visualized in the center. By changing the lens and/or camera position, the user can
find a configuration to make the FoV best match the desired Measurement Volume. You can
PAGE | 128
PAGE | 129
of course also switch to a different camera in case the realization of the experiment is difficult
with the current camera.
13.5.4   Hardware configuration
Camera /  Sensor
For a specific camera the Sensor Resolution (Width x Height in pixels), the Pixel Pitch (distance
between neighbor pixels) in microns (μm) and the resulting physical Sensor Size in mm are
fixed parameters.
Option 1 Camera in the DynamicStudio Device Library:
l  Select a Camera from the drop-down list
The Sensor Resolution (pixels) and Pixel Pitch (μm) is displayed together with the physical
Sensor Size in mm.
Option 2 Custom Camera:
l  Create your own camera by selecting the 'Custom Camera' or by modifying a camera type
in the drop-down list
l  Specify Sensor Resolution (pixels) and the Pixel Pitch in microns (μm). The physical Sensor
Size in mm is calculated from these two parameters and displayed.
Lens  parameters
For lenses in the DynamicStudio Device Library specifications are provided by the lens man-
ufacturer . Key parameters, such as Focal length and minimum f-number are always included,
but others, such as minimum focusing distance, may not be. The Min-focus distance specifies
the shortest Camera-Object distance possible at which the object can still be focused. This will
also correspond to the maximum magnification possible of the given camera / Lens con-
figuration. Note that for small field of views a macro lens is the best option.
The Focal length of the lens and the Camera-Object distance (Zo) determine the Magnification
of the system as M=f/(Zo-f). The user can test different settings in ISA to investigate this fur-
ther before the measurements.
The f-number will need to be optimized for the lighting conditions in a real measurement situ-
ation and is also important e.g. for the resulting Depth-of-Field (DoF) as well as for the particle
image size in pixel.
Option 1  Lens in the DynamicStudio Device Library:
l  Select a Lens from the drop-down list
The Lens properties belonging to the selected Lens are displayed.
Option 2 Custom Lens:
l  Create your own Lens by selecting the 'Custom Lens' or by modifying a lens type in the
drop-down list
l  Insert the correct parameters for the Lens being used:? Focal length (f) and f-number).
Scheimpflug Correction
Note: When looking at a light sheet at an angle instead of head on, the lens and image plane
need to be tilted as illustrated below. This is known as the Scheimpflug condition and ensures
proper focusing across the light sheet and over the entire FoV. To achieve this so called
Scheimpflug adaptors are mounted between the camera and the lens. Scheimpflug adaptors
can be of two types 1) adjustment possible in one direction (Pan angle, see image below) or 2)
adjustment possible in two directions (Pan and Tilt angle).
For measurements in air or in water where the camera is normal to the window the Scheim-
pflug angles are calculated by ISA:
PAGE | 130
PAGE | 131
Scheimpflug 'Pan' & 'Tilt' angles are used in the same way as Camera Pan & Tilt describe cam-
era location/orientation (See below).
Scheimpflug angles are also shown graphically (examples below does not match the numerical
values above):
The left figure shows a 'Top Down' view of a camera and a lens, showing clearly how the cam-
era housing is tilted relative to the lens. The middle figure is the exact same camera seen
from above and behind. In both figures the gray line shows the optical axis and remain
aligned with the lens regardless of the Scheimpflug correction applied. The green line is
aligned with the camera housing and moves with it when Scheimpflug correction is applied.
The optical axis will normally be gray while the camera axis follows the color of the camera.
Note the 'Crosshair' that is drawn on the back end of the camera ; If the optical axis exits the
camera on the horizontal line of the crosshair only Scheimpflug Pan is nonzero. In a 'Tilt only'
scenario the optical axis would exit the camera on the vertical crosshair line and in this setup,
where Scheimpflug Pan & Tilt are both nonzero, it exits in one of the quadrants of the
crosshair. The outer circle of the crosshair corresponds to a total Scheimpflug angle of 15°,
which is the mechanical limit allowed by most Scheimpflug adaptors. If the total Scheimpflug
angle exceeds 15° the line representing the optical axis will turn red instead of the normal
gray to give a visual clue about a potential problem. This is illustrated by the rightmost figure
above.
In the measurement region ISA will also show the Near and Far focal planes of each camera in
the setup. If the camera is focused on the lightsheet the near and far focal planes illustrate
how near/far to/from the camera you will still have decent focus. Depth of field is the distance
between these two. By default the focal planes are on and shown with Scheimpflug correction
applied. If you press 'T' you can turn off the display of focal planes and see only the field of
view. Pressing 'T' once more will turn the focal planes on again, but shown as they will be for a
camera without Scheimpflug correction applied. Press 'T' once more to switch the Scheimpflug
correction back on. Examples below.
Scheimpflug correction On.
Focal planes Off.
Scheimpflug correction Off.
PAGE | 132
PAGE | 133
Camera placement/position:
The inputs required by the user are seen below along with an example of a two camera (Ste-
reo-PIV) setup.
Insert coordinates for the Camera(s) pose
Camera Pose describes the position and orientation of the camera (or the lens to be exact);
The Distance [mm] is always positive and measured from the target point (0,0,0) to the lens
(/pinhole).
The Pan angle [°] is from -180 to +180 and describes camera position in the x/z-plane. Positive
Pan angles will place the camera at positive x-positions and similarly Pan<0 leads to x<0. Pan
angles in the range from -90° to +90° will place the camera at positive z-positions providing a
normal 'Front View' of the light sheet (x/y-plane). Pan angles outside this range will place the
camera at negative z-positions providing a 'Back View', looking at the light sheet 'from behind'.
The Tilt angle [°] is from -90 to +90 and describes camera position above or below the x/z
plane. Positive Tilt angles will place the camera at positive y-positions and similarly Tilt<0 leads
to y<0.
The Roll angle [°] is from -180 to +180 and describes rotation of the camera around it's own
axis. For cameras with a square sensor it makes little difference, but for cameras with a rect-
angular sensor a Roll of +/-90° is equivalent to switching between 'Landscape' and 'Portrait' ori-
entation. With the default Roll=0° 'Up' in the camera images align with the y-axis ('Up') in the
physical world. With multiple cameras at skew angles their fields of view can sometimes be
made to overlap better by adjusting camera Roll angles, but this is rarely done as it turns out
quite difficult to do in practice.
Distance, Pan & Tilt define the camera position. The camera is always looking towards the tar-
get point (0,0,0), which together with the Roll angle defines the camera orientation.
See the also the four images at the end of this section illustrating the camera move-
ment in space.
Window ? only relevant for water/liquid applications:
l  Select “No Window”? for applications in air
Or
l  If a predefined water tank setup is selected a “Window” is by default present in the light-
path / tank wall. The default information about the window orientation and the distance
between the Camera and the Window can be modified.
l  Window Orientation:? (Select from the drop-down list)
1) Window orthogonal to camera axis
2) Window parallel to X/Y plane
3) Window parallel to X/Z plane
4) Window parallel to Y/Z plane
A window that is orthogonal to the camera axis is the best and recommended solution. If this
is not possible or feasible use a window that is parallel to the lightsheet (i.e. the X/Y plane).
Remaining window orientations are possible, but very difficult to use in practice and should
thus be avoided.
ISA does not account for refraction in the window itself, since it is normally just a small fraction
of the total distance from camera to measuring position and thus has very little influence on
the measurement.
PAGE | 134
PAGE | 135
Short cuts  /  hot keys  in the 3D display  view
l  A - toggle axis display On/Off
l  S - toggle Scheimpflug Crosshair etc. On/Off at back end of camera housings
l  T - toggles Depth-of-Field Visualization
l  V - toggle camera view cones On/Off (they are nice for one or two cameras, but may be
confusing when there are four)
l  Space- return to default view
l  Comma - toggle viewpoint: X/Y-plane, X/Z-plane, Y/Z-plane, Isometric (but with per-
spective)
l  X, Y or Z - rotate around X, Y or Z axis in steps of 15° (Upper case one way, Lower case the
other)
In a multi camera setup:
0, 1, 2 etc ? dim all but camera ‘n’, thereby highlighting the n’th cameras FoV
Esc ? to switch all cameras FoV display back on.
Illumination Settings
To visualize the light sheet an estimated Height and Depth is inserted. The height should typ-
ically match the measurement volume height.
Note: The light sheet is always parallel to X/Y plane with a sheet depth centered on Z=0.??
The Light sheet Depth (or thickness) is an important parameter as it, together with inform-
ation about the estimated Out-of-plane velocity, is used to calculate upper limit of the time
between laser pulses (Δt) to be used in the PIV acquisition. Δt_limited by out-of-plane velocity = [?
x Light sheet Depth / Out-of-plane velocity]
The Pulse Energy (mJ) is related to the specific laser that will be used. The wavelength (nm) of
the laser light is inserted and used for the calculation of Depth-of-Field and the Diffraction lim-
ited spot size.
In later PIV acquisitions with the same setup as simulated in ISA, the Time Between Pulses
should match the Recommended Δt given in the ISA Setup Information.
Depending on Laser Pulse Energy (see Laser specifications), Seeding particle properties (size
in μm, reflective index) the light budget might not allow for a desired Light sheet dimensions
(Height and Depth). Note also that depending on the propagation direction of the light sheet
the light scattered from the particles will be stronger in the forward direction (i.e. in the dir-
ection of the laser propagation) than in the backward direction.
This means that if the laser propagates in the positive X-direction, in the images below, both
cameras in the right image (cameras on opposite side of the light sheet) collects light
scattered in the favorable forward direction ? this is an ideal setup for PIV. However, this can-
not always be achieved in a real application. To compensate for this the user can adjust the f /
number settings on the cameras.
Flow information, Seeding particle size, PIV  analysis  settings
In this section the user specifies (or estimates) details of the flow to be investigated/simulated
and the seeding particle size. Also the Interrogation Area (IA) size is added to the Analysis Set-
tings, this is related to the PIV spatial resolution that can be achieved (see below).
Input information provided by user:
l  Flow details:
l  Expected maximum In-plane velocity (m/s)
l  Out-of-plane velocity (m/s)
l  Seeding particle Size (μm)?
l  PIV analysis information:
l  Interrogation Area (IA) size, i.e. the side of the IA in pixels
l  Only square shaped IAs are used in ISA, e.g. 32 x 32 pixels (used as default)
Output information generated by ISA:
l  Geometrical IA size (an average over the FoV) in mm (e.g. 2 x 2 mm)
l  The IA size in mm is related to the spatial resolution that can be achieved
l  Δt_limited by in-plane velocity = [? x Geometrical IA size / In-plane velocity]
l  Δt_limited by out-of-plane velocity = [? x Light sheet Depth / Out-of-plane velocity]
l  The shortest? Δt of these two is the Recommended Δt given by ISA
Information concerning the size and quality of Particle Images that can be achieved with the
defined imaging setup and seeding particle size:
l  Final particle Image diameter*
l  The Final particle Image diameter is recommended to be at least 2 pixels to avoid peak (or
pixel) locking effects. A warning is given in ISA if this criteria is not fulfilled. The user
should consider to change the magnification, seeding particle size or the f/number when
this warning flag is raised. To bypass of this effect, it is also possible to set the focus slightly
“wrong” this will work best in case of 1-camera set-up with a perpendicular view onto the
Light Sheet. However it becomes tricky as soon as Scheimpflug mounts are involved. For
thicker volumes as used for Volumetric Velocimetry this is no longer feasible at all!
*Final particle Image diameter= SQRT(DiffrationLimit2 + GeometricParticleImageSize2). The
Diffraction Limited Spot Size and the Geometrical Particle Image Size is also found in the Cam-
era Results in ISA.
PAGE | 136
PAGE | 137
13.5.5 2 Examples how  to use ISA to configure,  plan  for and setup an  PIV
experiment.
Example 1.?  PIV, single camera setup (2D2C PIV)
The PIV technique in short:
The PIV processing methods computes 2 component (2C) velocity vectors in a 2D plane (a thin
light sheet). The light sheet, generated by a pulsed laser and sheet forming optics, is typically
0.5 - 1 mm in depth in 2D2C PIV.? A camera with lens, provides double-frame images of the
seeding particles, following the flow, in the light sheet. The laser pulse defines precisely the
exposure time of the two frames.
The time between laser pulses (Δt) should be optimized to maximize the PIV signal i.e. the cor-
relation strength when correlating the intensity information between small regions (Inter-
rogation Areas, IA) in the two frames.? As rule of thumb the particles should not move more
than ? of the IA size and also not more than ? of the light sheet thickness.
In a real measurement situation a camera calibration is performed prior to data acquisition: It
describes how points in object space map to points in the image plane of the camera.? The cam-
era calibration is performed in acquisition mode, see [Link to calibration in the online help].
However, ISA correctly calculates/simulates and displays the magnification and the size of the
FoV at the light sheet based on the selected camera, lens and the camera position. Based on
this information the smallest IA size in mm in the FoV can be obtained and used to calculate a
suitable time between laser pulses (Δt).
Setting up a PIV  system in ISA and test its  performance
By selecting one of the predefined configurations above for a single camera system the fol-
lowing information (input) is displayed in the Properties in ISA and can be modified by the
user:
l  Coordinate System & Bounds:
l  Medium Bounds - the dimensions of e.g. a tank or wind tunnel or section thereof
l  Measurement Volume (MV) size ( region/volume to be “imaged”)
l  Medium type:
l  air, (or other Gases)
l  water (or other liquid with different refractive index)
l  Hardware:
l  Camera selected from the device library? (Sensor details, camera position)
l  Lens? selected from the device library?
l  Light sheet height and depth
l  Custom cameras and lens properties can be inserted manually
l  Flow details:
l  Expected maximum In-plane and Out-of-plane velocity
l  Seeding particle Size (μm)
l  PIV analysis:
l  Interrogation Area (IA) size (e.g. 32 x 32 pixels)
Based on the input from the user the Setup Information in ISA displays the calculated “Ima-
ging Parameters” such as: Magnification (M), Field of View (FoV), Depth of Field (DoF).
From the expected maximum In-plane and out-of-plane velocity [m/s] and the geometrical IA
size [mm] (calculated from the IA size in pixels and the magnification), ISA then calculates a suit-
able Δt (time between laser pulses) for a PIV (Double Frame) acquisition. If a high-speed
(“time-resolved”) system is used the recommended frame rate to be use is = 1/dt.
In the 3D view the user can get an overview of the entire setup, including the camera pos-
ition, resulting FoV, DoF and light refraction.
For a defined 2D PIV setup ISA displays the:
l  Size of the field-of-view (FoV),
l  The Interrogation Area (IA) size in mm ? (related to the spatial resolution).
l  A recommended Δt (time between laser pulses) to be used in the PIV acquisition.
l  The final particle Image diameter*
* Final particle Image diameter= SQRT( DiffrationLimit^2 + GeometricSize^2). Recommended
to be larger than 2 pixels to avoid peak (or pixel) locking effects.
Example 2. Stereo PIV  setup (2D3C PIV)
The Stereo PIV technique in short:
The "Stereo-PIV" (on page?838) processing method computes 3 component (3C) velocity vec-
tors in a 2D plane (a light sheet with a few mm depth) by combining data from two cameras,
each providing double-frame images of the seeding particles in the light sheet as seen from
different view-points. The 2D plane is defined by the light sheet but it should be noted that in
Stereo-PIV measurements the sheet depth (or thickness) needs to be a few mm to allow for a
registration of the out-of-plane motion when combining the 2D information from both
cameras.?
From each of the two cameras the method requires a 2D vector map and a camera calibration
describing how points in object space map to points in the image plane of the camera in ques-
tion. The camera calibration is performed in acquisition mode, see "Stereo-PIV" on page?838].
However, ISA correctly calculates/simulates and displays the magnification and the size of the
overlapping FoVs at the light sheet based on the selected cameras, lenses and the camera pos-
PAGE | 138
PAGE | 139
itions. Based on this information the smallest IA size in mm in the overlapping FoVs can be
obtained.
In an optimal stereo PIV setup the cameras are looking at the light sheet at 30o-45o angle. To
ensure proper focusing across the light sheet and over the entire FoV the lens and image
plane need to be tilted as illustrated below. This is movement is achieved by the use of Scheim-
pflug adaptors positioned between the camera and the lens. (See"1-Axis Scheimpflug mounts"
on page?176)
Setting up a Stereo-PIV  system in ISA and testing its  performance
Of the eight predefined configurations above the leftmost setups are the easiest and thus
recommended to be preferred over the ones further right:
l  To get started with Stereo-PIV - measure in air if you can.
Water applications:
l  If possible let each camera look straight through a window.
l  If that’s not possible let the window be parallel to the light sheet.
l  The rightmost setup types are possible, but difficult in practice since the cameras will be
looking through a window at a steep angle and/or looking at the light sheet at a steep
angle. Use them as last resort only, avoid them if you can.
`
After selecting one of the predefined configurations above for a Stereo camera arrangement
the following information (input) is displayed in the Properties in ISA and can be modified by
the user:
l  Coordinate System & Bounds:
l  Medium Bounds & Measurement Volume (MV) size
l  Medium type:
l  air, or water (or fluid with different refractive index)
l  Hardware:
l  Cameras and Lenses selected from the device library or customized items
l  Light sheet height and depth. (The sheet needs to be a few mm thick)
l  Flow details:
l  Expected maximum In-plane and Out-of-plane velocity
l  Seeding particle Size (μm)
l  PIV analysis:
l  Interrogation Area (IA) size (e.g. 32 x 32 pixels)
Based on the input from the user the Setup Information in ISA displays the calculated “Ima-
ging Parameters” for both cameras such as: Magnification (M), Field of View (FoV), Depth of
Field (DoF).
From the expected maximum In-plane and out-of-plane velocity [m/s] and the smallest geo-
metrical IA size [mm] in the overlapping part of the two cameras FoVs ISA then calculates a
suitable Δt (time between laser pulses) for a PIV (Double Frame) acquisition.
In the 3D view the user can get an overview of the entire setup, including the two cameras
position, the overlapping region of the two FoVs, DoFs and light refraction.
For a defined Stereo-PIV setup ISA presents the:
PAGE | 140
PAGE | 141
l  Size of the overlapping field-of-view (FoV)
l  The average projected Interrogation Area (IA) size in mm ? (related to the spatial res-
olution).
l  A recommended Δt (time between laser pulses) to be used in the PIV acquisition.
l  The final particle Image diameter*
* Final particle Image diameter=SQRT(DiffractionLimit^2 + GeometricSize^2). Recommended
to be at least 2 pixels to avoid peak (or pixel) locking effects, where measured displacements
are biased towards integer pixel values.
3  Illustration of the camera movement in space
Distance (>0mm)
Pan angle (Span: -180° to +180°)
Note that positive Pan angles will position the camera at positive x-coordinates, while negative
Pan angles will correspond to negative x-coordinates.
Similarly an absolute value of the Pan angle below 90° will position the camera at positive
z-coordinates (i.e. normal "Front View" with the camera in front of the light sheet), while abso-
lute values of the Pan angle above 90° will give negative z-coordinates (i.e. "Back View" with
camera behind the light sheet).
Tilt angle (span: -90° to +90°)
Positive Tilt angles will position the camera at positive y-coordinates looking down at the exper-
iment, while negative Tilt angles will position the camera at negative y-coordinates, looking up
at the experiment.
PAGE | 142
PAGE | 143
Roll angle (span: -180° to +180°)
Roll will rotate the camera around its own axis.
If the camera sensor is rectangular the Field of View will be rectangular also. When looking at
a light sheet with an off-axis camera the Field of View will be stretched further due to per-
spective. By rotating the camera you can make the perspective "stretch" the smallest of the
sensor dimensions to get a less oblong Field of View than you would otherwise get.
13.6 Remote  Controlled  Acquisition
Remote Control is a special option that requires implementing a driver based on the inform-
ation below.
Using the DynamicStudio Remote Control interface it is possible to
l  Set Traverse position before starting Acquisition,
l  Set properties that will be saved together with the data saved during an Acquisition
l  Starting Acquisition that will end up with data saved in the database
l  Stopping Acquisition
l  Perform analysis on the acquired data
l  Deleting the just acquired data from the database
The need for Remote Control of Acquisition is often a part of an automation process, where
more equipment must be synchronized to work together, such as experiments in larger facil-
ities. The remote control can also be used to repeat a certain measurement routine over and
over again, or to ensure control over a fixed set up between experiments.
There is no driver installed with the installation of dynamic studio. In order to receive a demon-
stration driver (including source code) please contact support@dantecdynamics.com.
13.6.1 How  to start Remote Controlled Acquisition
Remote control can only be started when in Acquisition mode by selecting Remote Con-
trolled Acquisition from the Run menu. Before entering remote control it is recommended
to run a preview to ensure proper setup of devices and connections. The dialog will look like
this:
The Close button ends the session and closes the Remote connection, the More button
toggles between more and less information.. When expanded a log windows is displayed
providing status information on the communication between the "Applic-
ation.Remote.AKProtokol.dll" and DynamicStudio.
PAGE | 144
PAGE | 145
The entry Session Command is a string that is passed to the "Applic-
ation.Remote.AKProtokol.dll" when a session is started. The default string is "COM1" but the
last string entered will be remembered by DynamicStudio. The string could be ex.
"10.10.100.101:7009" instaed of a comport, it is up tp the implentatio nof the "Applic-
ation.Remote.AKProtokol.dll" to handel and interpriate the string.
If an error occurs during remote control interaction, then in order to get out of the error
state the user have to either click the Restart button that stats a new session or send the AK-
Protocol command End or Abort. This ends or aboars the acquisition and returns to a ready
state.
13.6.2 Infomation  on  the Remote Control Driver
Since the remote control feature is designed to be integrated into various systems, it consists
of a dedicated driver. The driver is a DLL that must be installed on the PC running Dynam-
icStudio. You can implement this DLL yourselves and integrate this into DynamicStudio soft-
ware when the interface described below is satisfied.
Dll Interface Methods
The interface to the driver DLL consists of three interface calls, that must be implemented by
the driver:
RCStart
int __declspec(dllexport) __RCStart( HWND hWnd,const char* psInfo)
Starts the remote driver. Here you can connect to or start any sub routines that your driver
must run.
This method is called when the Remote Control dialog is opened in DynamicStudio software,
with the last used information string. This method is also called during a request of a restart of
the remote driver, with optional a new information string provided.
Parameters
[in] hWnd is a Windows Handle to the Remote Control dialog. You must use this Windows
Handle to call the remote commands. This is also provided for use with the __RCMessage call-
back routine, see description.
[in] psInfo is a pointer to an information string, passed from the Remote Control dialog in
DynamicStudio to the driver. The string can contain any text for your driver. The information
string can be max. 64 characters long.
Return value
If your remote driver starts successfully you should return 0, otherwise you can return an
integer error code that will be displayed in the Remote Controller dialog for easy error hand-
ling.
Example
The information string can contain the serial COM port, or other necessary information for
your driver to work properly. The string is saved along with BSA Flow Software.
__RCMessage
This is an optional interface for providing a message based callback function for the remote
driver. If the remote driver needs a window for receiving Windows Messages, the handle
passed in the __RCStart method can be used and the message will be passed back to this func-
tion. This simplifies the design of the remote driver.
LRESULT __declspec(dllexport) __RCMessage(WPARAM wp,LPARAM lp)
Parameters
[in] wp is the first data holder.
[in] lp is the second data holder.
Return value
Not used.
Example
If you receive asynchronous messages from e.g. a serial communication driver you can pass
the hWnd received in the __RCStart method to this sub method. Inside this sub method you
PAGE | 146
PAGE | 147
can then call the WIndows messages SendMessage or PostMessage with data:
#define WM_REMOTEDRIVERDATA (WM_APP + 2134)
::SendMessage(hWnd,WM_REMOTEDRIVERDATA,(WPARAM)data1,(LPARAM)data2);
The same data will then be received in the __RCMessage method:
LRESULT __declspec(dllexport) __RCMessage(WPARAM data1,LPARAM data2)
{
:
:
__RCStop
int __declspec(dllexport) __RCStop()
Stops the remote driver. In the method you can disconnect or end running sub routines in
your driver.
This function will be called by the Remote Control dialog when a Restart is requested and
when the dialog is closed.
Return value
Not used.
13.6.3 Commands
Commands are send to DynamicStudio via a simple Windows Messages using WM_COPYDATA.
This message provides a mean to copy data from one process to another in Windows. Each
command is given a unique command ID recognized in DynamicStudio, and some of the com-
mands requires a command text line in a special syntax, all can be found in the list below.
The remote driver must call the DynamicStudio with the following command set:
#define RC_USERDATA 1
#define RC_RUN 2
#define RC_ABORT 3
#define RC_STATUS 4
#define RC_ADDANDRUN 5
#define RC_END 6
#define RC_SET_ANALYSIS_SEQUENCE_ID 7
?define RC_DELETE_RUN 8
RC_USERDATA
Adds user data to the current acquisition. User data is displayed in the log window of Dynam-
icStudio and saved in the database later when saving the acquired data to the database.
User data must be a ';' separated string with the following format:
[Category name1;name2];[property name;property value];[property name;-
property value]...
Ex 1:
"Temperature;Port1;Temp1;20.5;Temp2;Temp2;19.5"
This will create a category named "Temperature.Port1" in properties for the Run saved later.
In this category the following properties will be seen:
Temp1 20.5
Temp2 19.5
Ex 2:
"WindSpeed;#1;Point1;10.5;Point2;11.3;Point3;10.4;Point4;10.9"
This will create a category named "WindSpeed.#1" in properties for the Run saved later.
In this category the following properties will be seen:
Point1 10.5
Point2 11.3
Point3 10.4
Point4 10.9
RC_STATUS
Asks for status from DynamicStudio.
The status is returned by the message call.
0 ready
1 busy (acquiring)
2 error
RC_ADDANDRUN
Moves the Traverse to a new position, sets a new trigger delay on the synchronizer (if used)
and starts an acquisition.
Add and run info must be a ';' separated string with the following format:
ignored;ignored;X;Y;Z;Trigger delay
Ex:
"Ignored;Ignord;0.00;0.00;10.00;100.0"
PAGE | 148
PAGE | 149
RC_END
Stops the acquisition.
No command string.
RC_SET_ANALYSIS_SEQUENCE_ID
Specifies the analysis sequence to be performed after data has been saved to the database
during acquisition.
Ex: "3s2vj1v0"
NOTE: The right sequence ID can be found in the "Analysis Sequence Library"
RC_DELETE_RUN
Can be called to delete the just acquired Run and all data below.
No command string..
13.7 PIV  Simulator
The PIV simulator is a tool that is well suited for both evaluation of PIV algorithms (accuracy,
speed, etc.) or as an educational tool that illustrates the basic principle of PIV and let new
users get accustomed to the work-flow of Dynamic Studio, without the complexity of real phys-
ical setup.
Within Dynamics Studio it is possible to define a virtual PIV experiment. This virtual exper-
iment, that consist of a particle simulator/engine, a light sheet and any number of observing
cameras, will generate synthetic images, which subsequently can be used for 2D-, Stereo- or
Volumetric/Tomographic PIV analysis, as if the images where acquired from a physical setup.
The PIV simulator can generate calibration images as well as particle images.
13.7.1 Running the experiment.
Running the synthetic experiment is exactly as any other acquisition within Dynamic Studio,
except that no hardware is involved.
The System Control handle the setup of the general parameters and execution of the acquis-
ition.
The number of images to acquire, whether it is double frame or single frame and the time
between pulses and trigger rate can be found in this section, as well as the controls for ini-
tiating and ending the acquisition.
The acquisition system is described in more detail in the section "Normal Use" (on page?76)
13.7.2 Configuring
To create a virtual PIV experiment, a particle engine needs to be added to the acquisition sys-
tem. This is done by right clicking the Acquisition Agent inside the device view and selecting
'Add New Device ...'.
This will bring up a dialog, from which the particle engine can be selected and added to the
acquisition system.
PAGE | 150
PAGE | 151
Once the particle engine is present in the acquisition system, any number of cameras can be
added in a similar way. By right clicking the newly created particle engine and selecting 'Add
New Device...', cameras can be added to the particle engine one by one.
13.7.3 Adjusting the particle engine properties
The particle engine exposes a number of properties that controls the behavior of the virtual
experiment.
Property                                                                            Description
Calibration grid
Generate calibration
images
True / False: If this property is set to true the simulation will gen-
erate calibration images instead of particle images. See the sec-
tion about "Calibration images" (on page?153) for more
information on how to specify the calibration grid.
Light source
Light sheet thickness          Defines the thickness of the lightsheet (mm). Only particles, that
lie in the light sheet are visible to the cameras.
Light sheet normal              Indicates a vector that is orthogonal to the lightsheet plane. Used
to specify the orientation of the lightsheet.
Light sheet position             A point in object coordinates that lie in the center of the light
sheet. (mm)
Particle field
Particle movement type     This property can be set to either vortex or gradient. It specifies
the simulated flow type that the particles should abide by.
Velocity scaling factor          A Scaling factor by which the velocity of the simulated flow is
scaled. To achieve a reasonably particle displacement, one will
have to adjust either the time between pulses (double frame
mode), trigger rate (single frame mode) or the velocity scaling
factor in order to decrease/increase the frame to frame particle
movement.
Particle size                           The range of particles sizes (mm).
Particle density                    Defines the simulated seeding concentration, given as particles
per cubic millimeter.
Simulated volume                The dimensions of the simulated volume that is seeded with
particles. The volume origin is defined as the center of the
volume. The simulation might bring particles outside of this
volume, but initially they are contained within the given volume.
Simulated volume offset    This property will offset the entire volume and the simulated
flow, by the amount specified.
13.7.4 Adjusting the camera
PAGE | 152
PAGE | 153
Property                                             Description
Lens  model
Lens model                                     The projection model can be
specified as either pinhole or
telecentric (orthogonal pro-
jection)
Focal Length                                   Only applicable in case of a pin-
hole projection model. Spe-
cifies the focal length of the
lens.
Magnification                                  Only applicable in case of a tele-
centric projection model. Spe-
cifies the lens magnification.
Position and orientation
Position                                            The position property defines
where the camera and lens is
located in the scene.
Target position                               The target position specifies a
point the camera is looking at.
The viewing direction (optical
center line) is determined by
the camera position and the
target point.
Roll angle                                         Defines the camera roll angle
around the optical center line.
13.7.5 Calibration  images
When the property 'Generate calibration images' is set to true. The particle engine exposes a
different set of properties which are related to the rendering of the calibration target. The cal-
ibration target is a plane, where markers are placed in a grid and with a well defined spacing.
Property                                                                            Description
Calibration Grid
Marker sizes                         Specifies the size of the three kind of markers (axis marker,
standard marker and center marker)
Grid size                                 This property indicates the number of vertical and horizontal
markers on the calibration target.
Grid spacing                          Defines the spacing between adjacent markers in the calibration
grid.
Target movement                Rotation or traverse. Specifies which movement pattern the cal-
ibration target should perform while images are acquired.
Angle sweep                         Only applicable when target movement is specified as 'Rotation'.
Defines the angular sweep (per rotation axis) that the calibration
target will undergo during acquisition.
Traverse movement            Only applicable when target movement is specified as 'Traverse'.
Defines the traversal along the z-axis that the calibration target
will undergo during acquisition.
The first image of any calibration acquisition will always be a calibration target that is aligned
with the X and Y plane and placed at Z=0, as the first image in a calibration set will be the basis
for the coordinate system of the calibration.
13.8 Online  Vectors
Online Vectors is an analysis which is performed during acquisition. The analysis result is dis-
played as a vector map.
PAGE | 154
PAGE | 155
For purposes of performance the interrogation areas are spaced equally apart according to
the number of areas specified by the user, as shown above.
The online analysis is performed only during Free run and Preview . After an acquisition has
take place the analysis method will try to calculate a result for each acquired image pair, in this
way it is possible to examine the acquired images and have vectors map displayed before
determining which images to save.
13.8.1 Adding Online Vectors to be performed during Acquisition
To add Online vectors to be performed during acquisition select "Add Online Analysis... " from
the Context Menu of a"Image format" node in the device tree in Devices. Now in the "Add
Online Analysis" dialog select "Online vectors".
13.8.2 Parameters for Online Vectors
There are three parameters for Online Vectors:
Number of interrogation areas:
This parameter specifies the number of interrogation areas to be analyzed. The default size is
8 x 8. The minimum is 4 x 4 and the maximum is 40 x 40.
Interrogation area size:
This parameter specifies the interrogation area size. The default size is 32 x 32 pixels. The min-
imum size is 16 x 16 and the maximum depends on image size.
Peak Ratio
This is a validation criteria. A vector is only valid if the first to second correlation peak height is
higher that the number specified.
13.9 Adding  the  Focus  Assist online  analysis  function  to
the  setup
The Online Focus assist is added to the setup as a sub device to “Image Format” device in the
device tree.
1.To add the device right click the “Image Format” device and in the context menu that pups
up select “Add Online Analysis”. The following dialog will appear:
Select “Online Focus Assist” and click OK.
2.The Online Focus assist device will be added just below the Image Format as shown below:
3.When clicking “Free run” or “Preview” Online analysis will run. When doing a real acquisition
the Online analysis will not run, only when all data has been collected the Online analysis will be
PAGE | 156
PAGE | 157
enabled again so that when browsing through data the result form the Online analysis
method will be shown.
13.10 Understanding  the  resulting  image  from  Online
Focus  Assist
The result from Online Focus assist is an image with the same size and bit depth as the original
image.
The image will display gradients found in the image. A bright pixel indicates a high gradient.
Below is an example of an image captured by a FlowSense 2M camera. The camera was aimed
at a calibration target:
The resulting image from Online Focus Assist may look like this:
In the example above the image is focused. If the user moves the focus away from the sur-
face of the target the pattern will fade away. The more focused the original image is the
higher intensities will be seen in the resulting image.
In the example below the camera is aimed at white particles on a black surface:
PAGE | 158
PAGE | 159
The resulting image from Online Focus assist is shown below.
In this example the image is focused at its best. If the user moves the focus away the image
will dim out and be totally black. The more focused the original image is the higher intensities
will be seen in the resulting image.
The highest possible value shown in the resulting image will be if a saturated pixel is next to a
totally dark pixel. The resulting pixel value will be 255 for 8bit image and 4095 for 12 bit
images.
13.11 Using  Online  Focus  Assist to set up  a  stereo PIV  sys-
tem
A stereo PIV set-up requires the optical arrangement to fulfill the Scheimpflug condition: the
object, image and lens planes have to cross each other along the same line.
In order to fulfill this condition, the camera has to be tilted with respect to the lens as illus-
trated on the following figure :
PAGE | 160
PAGE | 161
The use of the Online Focus Assist makes the alignment of a stereo system easier and faster.
The following describes how to use the Online Focus Assist for such a set-up.
o  Open the lens  aperture at maximum
This reduces the depth of field for more precise alignment
o  Ensure that the illumination of the target is  homogenous
o  Adjust the LUT of both the camera image and the Focus  Assist displays
Double click on both displays
o  Use grid display  (Right click on the display, Options/Grid)
Set a grid size of half the width the Camera chip.
o  Set the camera in the direction to match the desired Field of View
o  Set the Scheimpflug angle to the maximum opposite position
Setting the Scheimpflug in the opposite direction reduces even more the part of the
image that will be in focus:
PAGE | 162
PAGE | 163
o  Align the focused area with the middle of the image
Use the grid as a reference
o  Set the correct Scheimpflug angle
The histogram of the focus assist, displayed in Log scale, can be used to track the correct
position of the Scheimpflug mount.
The histogram is usually mainly composed of two values that represent ares with no gradi-
ent information (the left part of the histogram) and areas with high gradients (the right
hand side peak in the histogram). Therefore, the correct scheimpflug angle is set when
the second peak reaches the rightmost position, see below.
The set up is now aligned and ready for calibration:
13.12 Parameters  for  the  device  Online  Focus  Assist
13.12.1 Pixel dilation  Diameter
Only one parameter for the Focus Assist device exists and this is Pixel dilation Diameter.
The functionality of this parameter is to expand high intensity pixels values to neighbor pixels.
The number that can be selected describes the diameter in pixels of this expansion.
13.13 Online  Light-Field
13.13.1 Online data  analysis
One of the main advantages of light-field velocimetry over tomographic is the ability to view
depth information in near real-time processing of depth. Once calibration is completed an
online process can be started. In the acquisition setup under the VolumeSense camera a dis-
play option can be added called "Light-field online. This display can only function when linked
to a light-field calibration dataset. Under properties for the light-field online a browser is
provided for quick selection of a calibration record.
13.13.2 Acquiring data  to disk
Capture and storage of images for light-field data is identical to other cameras. Please consult
the Dynamic Studio manual for more info.
PAGE | 164
PAGE | 165
13.14 Synthetic  image  generator
13.14.1 Introduction:
To check setting and learn to use Dynamics Studio it is possible to simulate PIV images, even
for multiple cameras. This allows simulation from 2D-2C PIV images up to full volumetric meas-
urements.
To simulate the images the “Acquisition Mode” needs to be enabled. Start with resetting the
acquisition agent by clicking “> Run > Acquisition System > Reset Acquisition system”. The
device list will know indicate, that no devices are added. By right clicking on icon of the “Acquis-
ition Agent” in the device list, one can add new devises (see image below).
To be able to simulate particles first a “Particle Engine for PIV simulations” needs to be added
at first. Otherwise the simulated cameras will only show a test image.
Once the particle engine is added to the device list, one or more cameras can be added to the
device list as children of the particle engine (see next image).
In order to simulate a measurement the System control is used as normally. Here the user
can select whether double or single images are recorded. The DT as well as the trigger-rate
and the number of images can be freely defined. With using the button “Acquire” the images
are simulated and can later on be saved in the “Acquired Data Tab” ("Acquiring Images" (on
page?64)
The camera tool is based on the system of normal cameras used in real applications in the
“Device Properties“ window. It is possible to select all kind of camera properties in the menus
numbers can be typed in, or drop down menus are used for different pre-selections.
The camera device Properties are described in the following section:
l  Sensor Size
Defines the width and the height of each image in pixel. As default it is set to 1024x1024
pixel
l  Pixel Pitch
Defines the physical size of each pixel in the given example 12μm
l  Lens Model
The optical model determines what kind of image model will be used. A pinhole model or a
telecentric model can be chosen in a drop down menu
l  Principal Point
Defines the optical axis of the camera for the pinhole model. This should be the middle
point of the sensor in the given example 512:512
l  Lens distortion
Defines the distortion coefficient values for the pinhole camera model
l  Position
Defines the X-Y and Z- Position of the camera
l  Target Position
Defines the X-Y and Z- Position of the cameras Principal point is aiming at
PAGE | 166
PAGE | 167
l  Roll angle
Describes the angle around optical axis of the camera
l  Rotation Vector
Gives the Rodrigues rotation vector that specifies the camera orientation. (informal can-
not be changed)
l  Translation Vector
Gives the cameras translation vector. (informal cannot be changed)
l  Background Noise Level
Describes image noise that can be added over the simulated particles. This specifies the
standard deviation of the Gaussian distributed background Noise that is added to the cam-
eras image. The mean for the distribution is twice the specified value.
l  Full Name
Gives the cameras Name. (informal cannot be changed)
The Image Format Properties are described in the following section:
l  Frame Mode
Defines in a drop down menu if the setting from the system control should be used
(recommend), single frames, double frames or double exposure on a single frame can
also be selected
l  Mirroring or Rotate
Defines an image mirroring or rotation
l  Pixel Depth
Defined the bit-depth of the images 8-16 bit can be selected via a dropdown menu
l  Image area (ROI)
Here the region of interest can be selected, (not recommended to use this since the
image format can be change in the camera stings menu.)
In the Image buffer tab only a single selection can be made:
l  Delay defines a delay in μs here 1000 should be used as standard
Important for the set-up and the simulation is also the Particle Engine, because it defines
weather calibration images are generated or not, where the measurement volume is, what
the seeding density will be and what kind of flow should be simulated. This is also done by edit-
ing number or making choices form drop down menus.
l  Particle movement type
Defines if a vortex or a gradient flow will be simulated by a drop down menu: Please add
formula for the flow
l  Max velocity (m/s)
Defines the maximum velocity of the flow
l  Velocities have Z-Component
Defines by a dropdown menu weather this is TRUE or NOT.
l  Particle Size
Defines a particle diameter range of a particle distribution in mm
l  Particle density
Specifies a particle density in particles / mm3 in the measurement domain. Note here is a
limiting number of possible particles set. Hence if large volumes are simulated, this limit
can be reached. Please add the max. NR of particles possible to be created
l  Simulated Volume
Defines the size of the simulated volume in mm. This volume will be seeded with particles.
Note that the particles in the volume are initialized only in the 1st time step. This means,
the particles drift out of the measurement domain over time.
l  Simulated Volume offset
Defines an offset of the volume in mm
l  Particle random seed
Defines the seed used for generating the random particles. If -1 is selected, a time based
seeding is forced
In the tab Calibration grid, the user specifies if he wants to simulate target images that can
later be used for camera calibrations.
l  Generate calibration images
Drop down menu to select if target images should be simulated or not. If TRUE is selected
the device property tab changes to the following and in case of recordings are generated
Dantec dotted target will be simulated:
l  Marker Sizes
Selects the size in mm of the target markers for the “Axis-” “Standard-” and “Centre-” mark-
ers
l  Grid Size
Defines the number of marker points in X-and Y- direction
PAGE | 168
PAGE | 169
l  Grid Spacing
Defines the distance in mm between the markers
l  Target Movement
Defines the calibration method, if “Rotate” is selected, images for a freehand calibration
are simulated that make the Pinhole model for calibration necessary. In this case the next
point is:
l  Angle Sweep defines the angels of rotation around the X-Y- and Z-Axis.
, if “Traverse” is selected, images for a 3rd order polynomial calibration are simulated, by
traversing the target along the x-axis. In this case the next point is:
l  Traverse Movement specifies the starting position of the target and the spacing it
moves. -10;2 means it will move in mm steps with -10mm being the 2nd image because
the software will always generate a 0-Plane image at first. If now 12 recordings are
made. The target will be positioned at the following Z-Positions: 0mm; -10mm -8mm; -
6mm; …; 8mm; 10mm. After sorting the images, the proper Z-positions need to be
added manually to the record properties. Seed LINK to this sections:
In case of generating particle simulations, the last part that needs to be dined is the light
sheet.
l  Illumination Type
Selects whether a single “light sheet” is used, or the “light sheet intersection” mode. In
case of the intersection mode, three light sheets are generated, and only the part where
all of them intersects, particles will be illuminated. This enables the user to simulate a lar-
ger volume than what is illuminated later on and thereby ensure that new particles can
enter the measurement domain.
l  Light sheet thickness
Is a standard light sheet has endless size in 2 axis. Here the thickness in mm is defined
l            Light sheet normal
Ddefines the normal in which axis this thickness is.
l            Light sheet position
Defines where the centre point of the light sheet is.
l            Light sheet Intersection
In this case the user needs to specify a light sheet thickness, a light sheet normal
and a light sheet position for the 3 light sheets that are intersected.
14 Devices
Devices are the terms for all external hardware connected to DynamicStudio. This includes
cameras, lasers, synchronizers, AD boards etc.
14.1 Lens  Mounts
Special lens mounts exists for lenses that can be controlled via software. The following are a
description of these devices..
14.1.1 Remote lens control
The Remote lens control is based on a lens mount from Birger Enginering. This special lens
mount is capable of controlling focus and aperture for Canon lenses supporting remote con-
trol.
Installing the Remote lens  control communication driver
The Birger Enginering lens mount is delivered in two different versions. One with a USB inter-
face and another with a COM port interface.
The lens mount with COM port interface does not require any special driver installation.
For using the USB version a special driver from Birger Enginering has to be installed. The
driver can be found on the installation DVD or on the download section of Dantec Dynamics
A/S home page.
The USB version of the Birger mount is simply a COM port version equipped with a USB to
serial Port converter.
This means that when the USB version of the Birger mount is connected to the PC, it will
appear as a serial COM port in the operative system.
Detection and appearance of the Remote lens  control
When the DynamicStudio acquisition system is started all serial ports on the PC are displayed
in the device tree. After system initialization detection of devices will start.
Once detected the Remote lens control will appear in the device tree under one of the COM
ports:
PAGE | 170
PAGE | 171
The Remote lens controller will also be displayed in the Synchronization Cables Diagram:
Associating Camera and Remote lens  controller
To associate a Remote Lens controller drag the Remote lens controller on top of the camera
device in the Synchronization cables diagram. Doing this will virtually attach the Remote lens
controller to the camera. When the Remote lens controller is attached to a camera the two
devices will follow each other when moved around in the Synchronization Cables diagram.
To detached the Remote lens controller from a camera, right click the Remote lens controller
and select "Detach device".
This can also be done via properties for the Remote lens controller.
Properties  for the Remote lens  controller
The Remote lens controller has the following properties:
Control Properties
l  Show control dialog
By Selecting this property and clicking the “...” button the dialog for controlling the Lens
will be displayed.
Device information and version
l  Full name
This property displays the device name and version
l  Serial number
Displays the serial no. of the Birger Lens Mount
l  Firmware version
Displays the Firmware version
Setup
l  Connected Lens type
Displays the lens type connected.
l  Camera attached
Here it is possible to identify the camera to which the Lens mount is attached (see Attach-
ing Lens Mount to camera).
Controlling the lens
To control the lens right-click the Remote lens controller in the Synchronization cables dia-
gram and select "Select lens control dialog", or go to properties for the Remote lens
PAGE | 172
PAGE | 173
controller and select the property "Show control dialog" and click the "..." that appears. These
actions will bring up the LensControl dialog as shown below:
Dialog functionality:
Main dialog
l  Lens type:
Displays the lens type connected.
l  Lens status:
1. Manual Focus: Shows if the Lens is set to Manual Focus.
2. Sticky lens: Enables or disables special functionality in the Remote lens controller for con-
trolling “sticky” lenses.
Focus
l  Allow Control:
If unchecked lens focus cannot be changed. This is useful to avoid accidentally changing
focus.
If checked lens focus can be changed.
l  Keyboard step size:
Controls the step size when the keyboard is used to change the focus.
l  Re-Learn focus range:
When clicked the lens focus range is learned by moving the focus from minimum to max-
imum. It will not be possible to change the focus on the lens before the Birger mount
knows the focus range.
l  Focus Preset
When the user has found a good focus he can click this button. This will save the current
focus position.
l  Recall Preset:
When clicked the focus will return to the saved focus position (saved by clicking Focus Pre-
set)
Aperture
l  Allow Control:
If unchecked lens aperture cannot be changed. This is useful to avoid accidentally chan-
ging the aperture.
If checked lens aperture can be changed.
l  Initialize:
By Clicking this button the aperture range is read from the lens. After that it will be pos-
sible to change aperture.
l  Aperture Preset:
When the user has found a good aperture setting he can click this button. This will save
the current aperture position.
l  Recall Preset:
When clicked the aperture will return to the saved aperture position (saved by clicking
Aperture Preset).
14.1.2 SpeedSense Ethernet camera  Canon  lens mount
SpeedSense Ethernet ameras can be delivered with a Canon Lens mount. This lens mount
offeres the ability to control focus and aperture from the software.
Detection of the special Lens  mount
Once the camera has been detected the system will check if the camera is equipped with a
Canon lens mount and if so if a lens is attached to the camera.
If a lens is detected a Remote Lens controller device will be created as a child device for the
camera just like the Image buffer and Image format device:
In Synchronization cables connection diagram the Remote lens controller will also appear as a
child device and attached to the camera as shown below:
PAGE | 174
PAGE | 175
Right clicking the Remote lens controller will bring up a context menu from where the Lens
control dialog can be accessed.
If the lens is detached from the camera the Remote lens controller device will disappear from
the device tree and from the Synchronization cables connection diagram
Parameters  for the Remote Lens  controller
The device properties or parameters for the SpeedSense Ethernet camera remote lens con-
troller are limited. Essentially only 2 parameters give any meaning:
Show control dialog
-This property allow the user to bring up the lens control dialog
Connected Lens  type
-Displays information on what type of lens is connected.
Controlling the attached lens
At the top the detected lens type is displayed.
The controls for handling focus are very limited. It is possible to click either the plus or minus
button to move the focus closer or further away. The step size in the selection box refer to a
stepper motor inside the lens and cannot be easily interpreted as a change (in mm) of the
focal distance.
The aperture is controlled via the slider in the bottom of the dialog. Here also the total range
is displayed as well as the current selected aperture.
14.2 1-Axis  Scheimpflug  mounts
14.2.1  Instructions  for mounting the FlowSense EO  cameras                                                              177
14.2.2  Scheimpflug 1-Axis                                                                                                                                               179
14.2.3  Instructions  for mounting the SpeedSense 1040  cameras                                                      181
14.2.4  Scheimpflug 1-Axis                                                                                                                                               184
PAGE | 176
PAGE | 177
This section describes assembly and alignment of the 1-axis Scheimpflug mount series
1.   Mount the camera as described in the sections above
2.   Focus the camera on a surface. This can be a target plate or similar.
3.   Move the Sheimpflug angle from one side to the other. Observe the center of the image.
If an image movement is observed the camera position has to be changed to remove this
uncertainty.
A few pixels (up to 4 is ok) will not disturb finding the right Scheimpflug angle later on.
4.   Move the camera in one direction until image only moves within the uncertainty. If the
movement of the image becomes larger, change the direction you move the camera
Forward movement of the camera:
To move the camera forth, mount the fine adjustment toll on the slider. Tighten the bolts that
holds the fine adjustment and slightly un-tighten the bolds that hold the camera, so that it is
possible for the adjustment toll to move the camera forth. Turn the screw on the fine adjust-
ment toll right to move the camera forth towards the lens.
14.2.1 Instructions for mounting the FlowSense EO  cameras
Remove the original C-mount front from the camera and replace it with the new front, using
the same screws.
Mount the bellows using the retaining ring and four M4x6 bolts.
Place the assembly on the rail, and slide the camera forward to attach the lens mount to the
bellows using four M3x12?mm bolts.
Make sure the bellow is placed on the inside of the retaining ring, and on the outside of the
front ring.
To make sure the image plane is correctly positioned, measure from the back of the rail to the
rear of the carrier, and from the pod plate to the lens mount.
Camera                            Lens mount height                       Distance from rear
FlowSense EO                 16.6 mm                                         93.2 mm
PAGE | 178
PAGE | 179
14.2.2 Scheimpflug 1-Axis
Instructions  for mounting the HiSense Zyla camera
Mount the carriers using 1/4" UNC screws. Make sure to use the outer holes in the carrier.
Let the screws be a little loose and slide the assembly onto the rail.
Carefully turn the Scheimpflug assembly upside down. Be careful not to drop the camera!
Align the screws (green arrows) with the holes in the rail and tighten the M4's on the carriers.
Then tighten the 1/4" UNC screws.You may need to slide the camera forward to access the
screw on the rear carrier.
Remove the camera from the Scheimpflug again. Unscrew the original C-mount front from
the camera, and place the bellow and retaining ring on the camera and secure it with four M4
screws.
Slide the assembly back on to the Scheimpflug mount and slide the camera forward to attach
the lens mount to the bellow using four M3x12mm screws. Make sure the bellow is placed on
the inside of the retaining ring, and on the outside of the front ring.
To make sure the image plane is correctly positioned, measure from the back of the rail to the
rear carrier, and from the pod plate to the lens mount.
PAGE | 180
PAGE | 181
Camera                            Lens mount height                       Distance from rear
HiSense Zyla                   26,8 mm                                         15,6 mm
14.2.3 Instructions for mounting the SpeedSense 1040 cameras
Remove the original F-mount front from the camera, and place the backup front, bellows, and
retaining ring with four M5 bolts.
Mount the bracket under the camera using two M5 UH bolts, and mount the carrier using
1/4" UNC bolts.
Place the assembly on the rail, and slide the camera forward to attach the lens mount to the
bellows using four M3x12?mm bolts.
Make sure the bellow is placed on the inside of the retaining ring, and on the outside of the
front ring.
PAGE | 182
PAGE | 183
To make sure the image plane is correctly positioned, measure from the back of the rail to the
rear of the carrier, and from the pod plate to the lens mount.
Camera                            Lens mount height                       Distance from rear
SpeedSense 1040          21.6 mm                                         101.4 mm
14.2.4 Scheimpflug 1-Axis
Instructions  for mounting the SpeedSense Miro camera ABE
Remove the original F-mount front from the camera and store it in a safe place. Mount the bel-
low using the retaining ring and the original screws.
Mount the carriers using 1/4" UNC screws. Make sure to use the inner holes in the carriers.
Let the screws be a little loose and slide the assembly onto the rail.
Carefully turn the Scheimpflug assembly upside down. Be careful not to drop the camera!
Align the screws (green arrows) with the holes in the rail and tighten the M4's on the carriers.
Then tighten the 1/4" UNC screws.
Loosen the M4's and slide the camera forward to attach the lens mount to the bellow using
four M3x12mm screws. Make sure the bellow is placed on the inside of the retaining ring, and
the outside of the front ring.
PAGE | 184
PAGE | 185
To make sure the image plane is correctly positioned, measure from the back of the rail to the
rear carrier, and from the pod plate to the lens mount.
Camera                            Lens mount height                       Distance from rear
SpeedSense M               36,7 mm                                         72,3 mm
14.3 Image  Intensifiers
The image intensifier is the most crucial component of an intensified camera system, beside
the sensor itself. It allows for an optimum adoption of the camera to any specific application.
The main function of the image intensifier is the amplification of the incoming light signal. This
enables the camera to take images at very low light conditions and/or at very short exposure
times.
l  "Custom Image Intensifier" below
l  "Hamamatsu Image Intensifier" on page?189
14.3.1 Custom  Image Intensifier
A custom image intensifier is a device where the user has full control over all signal para-
meters. The custom image intensifier will work for most intensifiers on the market.
It is possible to manually add a custom intensifier to the device tree by right-clicking the
Acquisition agent and select 'Add New Device...'.
It is possible to add as many intensifiers as needed to reflect the image acquisition system
used.
Properties of the intensifier are shown below and are arranged in two groups. The Control
group is for controlling general properties of the intensifier and the Timing group is for con-
trolling signal properties.
PAGE | 186
PAGE | 187
Gain
This parameter does not actively control the gain, but can be set to reflect the chosen gain of
the image intensifier. More info See?Camera Attached
Phosphor type or Decay  Time
The intensifier uses a phosphor screen to convert the electrons from the MCP (Micro Channel
Plate) back to photons. Depending on the phosphor type used in the image intensifier there is
a certain decay time associated. This decay time will influence when the intensifier can be
gated again without seeing a ghost image from the previous exposure. It is possible to select
between two predefined phosphor types (P43 or P46) or manually enter a decay time.
Camera Attached
This is a drop-down list that contains all cameras detected by the system. From here it is pos-
sible to attach an intensifier to a camera. The gain used for the image intensifier will auto-
matically be saved with the images from the selected camera. To detach the intensifier from a
camera select ‘None’.
An attached image intensifier will look like this in the ‘Synchronization Cables’ diagram:
It is also possible to attach an intensifier to a camera from the ‘Synchronization Cables’ dia-
gram. This is done by dragging and dropping the intensifier on top of the camera. To detach
the intensifier drag it away from the camera.
Gate Pulse Width
This is the width of the gate pulse.
Second Gate Pulse Width
When running in double frame mode this is the width of the second gate pulse. It is possible
to select ‘Same as first’ or enter a specific width.
Delay  to open
An internal delay from the image intensifier gets a gate signal until it actually opens (hardware
specific value to be found in the image intensifier manual).
Gate Pulse Delay
The time delay of the gate pulse relative to the light pulse of the system (T0). This value must
be negative if the image intensifier is to be open when the first light pulse is fired.
Gate Min. Off time
The minimum time the intensifier must be off (hardware specific value to be found in the
intensifier manual).
Gate Min. On time
The minimum time the intensifier must be on (hardware specific value to be found in the
intensifier manual).
Timing diagram
below is shown a complete system with a laser, camera and an image intensifier:
The zoomed in timing diagram for an intensifier is shown below. The dark blue part of the
'Phosphorescence time' shows the time that the intensifier is open and the light blue part
shows the time for the phosphorescence intensity to decay to 1%. The red dotted line is T0
(position of first laser pulse).
The timing diagram above is a result of the properties shown below:
Multiple image intensifiers  connected to the same output
It is possible to attach two or more identical image intensifiers to the same output on the tim-
ing device. In that case the intensifier parameters will be synchronized.
PAGE | 188
PAGE | 189
14.3.2 Hamamatsu  Image Intensifier
The following Hamamatsu image intensifiers can be fully controlled and auto detected by
DynamicStudio:
l  C9546 Series
l  C9547 Series
The properties of a Hamamatsu image intensifier are shown below.
Operation Mode
Operation mode can be set to Normal, Gated or Auto.
l  Normal means that the image intensifier will be held open while acquiring.
l  Gated, means that an external gate signal must be applied to the intensifier typically from
the synchronizer.
l  Auto means that in free run it will operate as Normal and in Preview or Acquire mode it
will be Gated.
Gain
This property controls the physical gain of the image intensifier. It’s a relative value from min-
imum gain (0) to maximum gain (999). An important note is that the gain adjustment on the
intensifier hand control does not operate in the same range as the gain from DynamicStudio.
The gain from the hand control operated from 600 to 999 meaning if it is set to 0 then the
real gain is 600. More info see "Gain" on page?187
Phosphor type or Decay  Time
See "Phosphor type or Decay Time" on page?187
Reset Warning/Error State
The Hamamatsu image intensifier needs to be reset if a warning has been issued or an error
has occurred. This drop-down reveals a button that has to be pressed to perform a reset.
Camera attached
See "Camera Attached" on page?187
Serial number
Is an internal number identifying the image intensifier.
Timing parameters
For the timing parameters see "Gate Pulse Width" on page?187
Known issues
If the intensifier is shot down while connected to DynamicStudio, there is a high risk of getting
a total system crash resulting in a blue screen of death (BSoD). To prevent this, disable the
device first or close down DynamicStudio before turning off the intensifier.
14.4 Analog  Input
14.4.1 Analog Input
DynamicStudio supports a number of analog input options for simultaneous sampling of ana-
log signals and recording of images.
The following analog boards are supported:
Device                                   Sample
rate              Resolution
Voltage
range             Channels
Analog Input Option                   10?kS/s              12 bit           -10V to +10V     8 channels
Fast Analog Input Option, 4 ch.        2.8?MS/s             14 bit           -10V to +10V     4 channels
Fast Analog Input Option, 8 ch.        2.8?MS/s             14 bit           -10V to +10V     8 channels
Ultra Fast Analog Input Option           2?GS/s                8 bit              -5V to +5V       2 channels
14.4.2 Installation
All analog input devices are auto detected by DynamicStudio.
Analog Input Option
This device is connected to the PC via USB and has direct BNC access.
PAGE | 190
PAGE | 191
Fast Analog Input Option
These devices are placed in a PCI slot in the PC. A special cable is connected from the board to
a terminal box to get BNC access.
Ultra Fast Analog Input Option
This device is placed in a PCI slot in the PC and has direct BNC access.
14.4.3 Connecting hardware for acquisition
Analog Input Option
Connect the "Trigger in" connector to the device generating the start measurement pulse
train.
Fast Analog Input Option
Connect PFI0 in the "Trigger/Counter" group to the device generating the start meas-
urement pulse train. If for example this is the Dantec Timer Box, connect PFI0 to any available
output on the Timer Box. Depending on the analog input device you may use AI0-AI3 (4 chan-
nel model) or AI0-AI7 (8 channel model) for the analog signal inputs.
Ultra Fast Analog Input Option
Connect TRIG to any available output on the Timer Box and use CH0 and CH1 for the analog
signal input.
14.4.4 Preparing software for acquisition
All analog input devices have the same fundamental user interface. In the Synchronization
Cables view the devices appear with only one input pin. This should be connected to the
device that generates the start waveform measure pulse. In the figure below the analog
input device is connected to a Dantec timer box and the timer box will deliver the pulse train.
With the System Control settings shown below the analog input device will be triggered 100
times at a rate of 40 Hz:
PAGE | 192
PAGE | 193
If the analog input device properties are set as shown below then every pulse from the timer
box will trigger the analog input device to acquire 5000 samples at a rate of 2?MHz.
The Sample delay property is used to add a delay relative to the first laser light pulse. In this
example the delay is 2?μs meaning that the analog input device will not start sampling until
2?μs after the laser have fired the first light pulse. The delay can be negative in which case
sampling of the analog input will start before the first laser pulse is fired.
Each channel has its own properties. These properties include the following:
l  Enabled. This enables or disables the channel.
l  Minimum input voltage. Allows the user to specify the minimum input voltage that should
be measured. If a better resolution is needed, a higher voltage can be specified.
l  Maximum input voltage. Allows the user to specify the maximum input voltage that should
be measured. If a better resolution is needed a lower voltage can be specified.
l  Channel name. Allows the user to give a meaningful name to the analog channel. This
name will be used in the resulting plot diagram during and after measurement.
Ultra Fast Analog Input Option
This analog input device has some additional parameters as shown below:
The additional parameters are:
l  Fast Sampling Option. This enables the device to sample channel #1 at a speed of 2 GS/s.
When fast sampling is enabled channel #2 will automatically be disabled.
l  Input impedance. This can be either 50 Ω or 1 MΩ. The 1?MΩ path is required in applic-
ations that require minimal loading. The 50?Ω inputs are protected by a thermal dis-
connect circuit. If however a voltage change is large and sudden enough, the protection
circuits might not have enough time to react before permanent damage occurs. It is
therefore important that you observe the specified maximum input voltages, especially
when the input impedance is set to 50?Ω.
l  Input coupling. This can be either DC or AC. Select AC-coupling if the input signal has a DC
component that you want to reject.
PAGE | 194
PAGE | 195
l  Minimum input voltage. This will show the minimum voltage that can be measured with
the current voltage range and voltage offset.
l  Maximum input voltage. This will show the maximum voltage that can be measured with
the current range and offset.
l  Voltage range. This specifies the voltage range of the input signal. Depending on what the
voltage offset has been set to, the minimum and maximum voltage that can be measured
will change.
l  Voltage offset. This is an offset that can be applied so the minimum and maximum voltage
that can be measured increased or decreased.
l  Probe attenuation. If your probe attenuation is 10:1 and your voltage range is 10?V, the
analog board is set to measure a 1?Vpk-pk signal. The data returned is 10?Vpk-pk.
14.4.5 Acquiring data
During free run and preview it is not guaranteed that the number of sample waveforms will
be the same as the number of images recorded. To get the same number of analog data as
there are images, perform a real acquisition.
14.5 Cameras
14.5.1 HiSense MkI
The HiSense MKI camera (originally name HiSensePIV/PLIF camera) was one of the first cam-
eras introduced by Dantec Dynamics.
Main specification:
l  1280x1024 pixels.
l  12 bits/pixel.
l  maximum frame rate: 9 Hz single frame / 4.5 Hz double frame.
l  Pixel binning: No binning, 2x2 and 4x4.
The HiSense camera is connected as shown below:
The frame grabber must be a National Instruments PCI-1424 (for 32-bit RS422 and TTL image
Acquisition).
Known issue?
l  In double frame mode the very first image pair in an acquisition can sometimes be out of
sync. The rest of the acquired image pairs is OK.
There is presently no solution for this issue.
14.5.2 HiSense MkII Camera
The HiSense MkII camera uses a highly performant progressive scan interline CCD chip, with
typically 72% quantum efficiency at 532 nm. This chip includes 1.344 by 1.024 light sensitive
cells and an equal number of storage cells.
In cross-correlation mode the first laser pulse exposes the CCD, and the resulting charge is
transferred as the first frame to the storage cells immediately after the laser pulse. The
second laser pulse is then fired to expose the second frame. The storage cells now contain the
first frame and the light sensitive cells the second. These two frames are then transferred
sequentially to the digital outputs for acquisition and processing. The charges in the vertical
storage cells are transferred up into a horizontal shift register, which is clocked out sequen-
tially line-by-line through the CCD output port.
In relation to PIV and planar-LIF experiments, the Dantec HiSense MkII camera has a number
of benefits compared to other cameras with:
PAGE | 196
PAGE | 197
l  Very high light sensitivity (typically 72% quantum efficiency at 532 nm)
l  Extremely low background noise
The high dynamic range is a valuable flexibility in the practical performance of the PIV and LIF
experiments (although most PIV experiments do not require 12-bit resolution, LIF does in
terms of scalar resolution, precision and accuracy in some cases). Also, there is less need to
adjust the laser power and the seeding, simply because a wider range of input intensity levels
provide successful results. Likewise, if problematic windows, dirt or other produces uneven
illumination over the area, there is less loss of local information, because the signal is still
received at the CCD due to the higher dynamic range.
14.5.3 HiSense NEO  and Zyla  Camera
The HiSense NEO and Zyla camera are sCMOS cameras with a resolution of 2260x2160 pixels,
16 or 12 bits per pixel. The Neo camera has 4GByte and the Zyla has 1GByte of memory used
for image-handling and image-buffering. The maximum frame rate is 50 frames per second in
single frame mode.
The HiSense NEO camera is delivered with a BitFlow Neon Frame grabber, and the Zyla cam-
era, a BitFlow Karbon frame grabbe or with a USB3 interface. The frame grabbers are the
only frame grabbers supporting the cameras.
The Neo camera is equipped with pipes for water cooling. This system cooling is not used.
The HiSense Neo and Zyla Camera transport case includes all that is necessary for installing
and running the camera under DynamicStudio. For the Camera Link verision there is also a
standard USB cable included in the transportcaase. This cable is only used for updating firm-
ware in the camera. Leave the USB cable in the case, and store the transport case for later
use in case the camera is not used for longer periods of time or in the case the camera has to
be shipped.
Frame grabber and camera
There is one piano-type switch block on the Neon and Karbon frame grabber with two
switches. These are used to identify individual boards when there is more than one board in a
system. The switch settings are read for each board from DynamicStudio and used to identify
the board.
Set the switch for each frame grabber in the PC:
Frame grabber 1:?1:down 2: down (As also illustrated in the image above)
Frame grabber 2: 1:up 2: down
Frame grabber 3: 1:down 2: up
Frame grabber 4: 1:up 2:?up
Connecting the camera
The camera and frame grabber is connected with a Camera Link cables. When connecting
cables, always make sure to disconnect the power cables from both the PC and the camera.
Synchronization cables
The camera is delivered with a special synchronization cable consisting of one TTL/DAC 26 D
type connector in one end and 4 BNC plugs in the other end.
Label and function description:
l  EXTERNAL?TRIGGER (Input: Used for triggering the camera. Connect this cable to the Syn-
chronizer in the system)
l  FIRE (Output: High while exposing)
l  SHUTTER (not used)
l  ARM (not use)
Detecting the camera
It is important that the camera is connected and turned on before starting up DynamicStudio,
otherwise the camera will not be detected. While DynamicStudio is running, the camera must
not be turned off. If the camera is turned off, it enters an unknown state making Dynam-
PAGE | 198
PAGE | 199
icStudio unstable. The problem will remain even if the camera is turned back on while Dynam-
icStudio is still running.
After entering the DynamicStudio Acquisition mode, the system will first detect the BitFlow
frame grabber, After this the frame grabber device will try to detect the camera. When the
camera is detected the Device tree will contain a 'BitFlow Neon' branch like this:
The camera is sensitive to grounding issues. If the system is not properly grounded the cam-
era might not be detected. Make sure that all equipment in the system is grounded.
Acquiring images
The HiSense NEO camera acquires images in a special way compared to other cameras. To
acquire a single image two sets of images are transferred from the sensor to the camera
memory.
Immediately after reset of the entire sensor a reference frame is acquired. For this frame
the exposure time is extremely short so it measures the noise level of the sensor and is later
used for noise reduction. After the reference frame is acquired the actual signal frame is
acquired.
The readout of the reference puts a lower limit on the exposure time of the actual image.
When the signal frame has been transferred from the sensor to the camera memory the two
images are combined to produce the resulting images.
Single frame mode
In single frame mode the camera acquires one frame for each trigger as shown below:
As shown the reference frame is read out of the sensor during the exposure of the signal
frame. This means that the minimum exposure time will be the read out time of one frame
corresponding to ~10ms.
Double frame mode
In double frame mode the camera will acquire one signal frame for each trigger pulse. The
time between each pulse defines the exposure time. The camera is triggered twice in double
frame mode giving the double exposure.
This means that the camera is exposing all the time from the time the camera is triggered the
first time until end of acquisition as shown below:
As seen above the minimum exposure time becomes ~20ms, since it depends on the readout
of both the reference frame and the signal frame.
(The Exposure time Signal Frame 1 and Exposure time Signal Frame 2 is combined in to one
double image. Exposure time Signal Frame 3 will be the first frame of the next double frame)
PAGE | 200
PAGE | 201
Parameters  for the camera
Fan speed
Configures the speed of the fan in the camera. There are three possibilities:
l  Off
Although the fan can be turned of this is not recommended. The camera can over heat,
but before this happens an acoustic alarm will sound. Turn of the camera and let it cool
down.
l  Low
The camera will control the fan speed.
l  High (default)
The fan speed is set to the maximum.
Camera pixel clock
The pixel clock is set to maximum for the camera as default. It is possible to lower the pixel
clock. This will reduce the maximum frame rate, but also lower the image noise .
Well capacity
This parameter is only visible in 12bit per pixel. Well capacity can be set to High or Low.
High well capacity means that each pixel cell can hold a larger amount of photons/electrons,
where as Low well capacity can hold fewer as can be seen below:
Bitdepth and Well
capacity
Sensitivity e-/ ADU(typ-
ical)
Data Range                   Effective pixel sat-
uration limit / e-
12-bit (high well capa-
city)
7.5                                    12-bit                             30000
12-bit (low well capa-
city)
0.42                                  12-bit                             1700
16-bit                               0.45                                  16-bit                             30000
Camera image buffer
The BitFlow frame grabber is capable of transferring 15 frames/sec (full frame) from the cam-
era to the PC memory. If the acquisition frame rate is higher than 15 frames/sec images will
buffer up in the cameras internal memory.
If an acquisition is done at a rate where images are buffered in the HiSense NEO camera, and
the user wants to stop the acquisition by click'ing 'Stop', the system will stop, but the camera
will continue transferring images to the PC for a while after the stop has been pressed. THis
image transfere can take some time, and if you do not want to use the images buffered in the
camera you can stop this process by clicking 'Abort/Reset'.
Frame rate
By changing the ROI a higher frame rate can be archived.
l  Changing the height will have an effect. Below are some theoretical frame rates. The
assumption that all ROIs are centered on the sensor (this provides the fastest frame rates
due to the design of the sensor).
l  Changing the width of the ROI will not affect the frame rate when storing the data to the
on board camera memory however there will be a slight increase in the throughput to the
card simply because there is less data to transfer.
Neo camera:
ROI Height (ROI Centered on Sensor)          Current frame rate
(Hz)
Expected frame rate
(Hz)
2160                                                    28.2                                   30.8
1040                                                    74.6                                   75.0
512                                                     152                                    152
256                                                     300                                    301
128                                                     358                                    592
48                                                       566                                   1396
Zyla camera (frame grabber)
ROI Height (ROI Centered on Sensor)             Max frame rate (Hz)
2160
49
2048                                                         52
1080                                                         97
512                                                         201
128                                                         716
Zyla camera (USB3)
ROI Height (ROI Centered on Sensor)             Max frame rate (Hz)
2160
40
PAGE | 202
PAGE | 203
2048                                                         52
1080                                                         98
512                                                         201
128                                                         716
Troubleshooting with USB3  version
Sometimes the USB3 version of the Andor cameras will create a new device in the acquisition
system instead of being detected as the same camera. This can occur if the camera is turned
off and back on while the computer is running. The issue can be corrected by disconnecting
and reconnecting the USB cable to the camera.
14.5.4 FlowSense 2M Camera
The FlowSense 2M camera uses a high performance progressive scan interline CCD chip but
with lower performance than the HiSense MkII camera (approximately 75% at 532 nm and
approximately 60% in the yellow-orange region of the light spectrum). Though, the chip
includes a much larger number of storage cells with 1.600 by 1.200 light sensitive cells, which
greatly limits the performance of LIF results in terms of precision of the scalar property meas-
ured. In relation to PIV experiments, the FlowSense 2M camera has the benefits to record in
8- or 10-bit data resolution, high light sensitivity at 532 nm (about 75% of the HiSense MkII)
and low background noise. The 8- or 10-bit dynamic range is a valuable flexibility in the prac-
tical performance of the experiment.
Tips  and Tricks
Black and White Level Adjustment
The FlowSense 2M camera is a two TAP camera, this means, that there are two AD converters
in the camera, each one responsible for one half of the total image.
Due to small differences in the analogue circuits surrounding the two ADC's and the CCD, the
image produced can have different intensities for each half image. This note describes how to
adjust the analogue setting to bring the black and white level intensity as close as possible to
one another.
Procedure:
1.   Turn the camera on.
2.   Wait until the camera has reach work-temperature.
3.   Put on a cap in front of the camera so that no light reaches the CCD sensor.
4.   In DynamicStudio enter Acquisition mode, and wait until the camera has been detected.
5.   Black Level Adjustment In the Device Properties for the "FlowSense 2M" camera, enter
the command "BLF=0;" in Arm command.
6.   This command adjusts black level of the right site of the image. The value entered after
BLF= can be in the range from -512 to 511. Ex: Arm String: BLF=-100; A higher value gives
a higher black level.
7.   In System control click "Free Run".
8.   Examine the image captured. If you can see any intensity difference near the center of
the image repeat from step 5 with a new value. (Use "Display options" to change the
lookup table for the image, to increase the intensity shown.)
9.   Gain Balancing Remove the cap in front of the camera.
10.   In the Device Properties for the "FlowSense 2M" leave the command and setting found
during black level adjustment and add the command "GAF=0;" ex: Arm command BLF-
F=34;GAF=0;
PAGE | 204
PAGE | 205
This command adjusts the gain level of the right site of the image. The value entered after
GAF= can be in the range from -2048 to 2047. A higher value gives a higher gain.
11.   Examine the image captured. If you can see any intensity difference near the center of
the image repeat from step 10 with a new value.
The camera is now adjusted.
14.5.5 FlowSense 4M Camera
The FlowSense 4M camera uses a high performance progressive scan interline CCD chip but
with a sensitivity similar to the FlowSense 2M camera (approximately 55% quantum efficiency
at 532 nm)
The camera resolution is 2048 by 2048 light sensitive cells and an equal number of storage
cells. In relation to PIV experiments, the FlowSense 4M camera has the benefits to record in
8- or 10-bit data resolution. The 8- or 10-bit dynamic range is a valuable flexibility in the prac-
tical performance of the experiment.
FlowSense 4M MKII is similar to FlowSense 4M but has can do 12-bit per pixel. Apart form this
the camera is slightly more sensitive compared to FlowSense 4M.
Scanning format
It is possible reduce the size of the image by selecting Scanning format. By reducing the
image size the trigger rate can be increased. The table below shows the scanning area size
and start position relative to the full image.
Scanning area                             Start Line(from top of image)      Effective Area
Full Frame                                    1                                                       2048 x 2048
1/2 Frame                                    525                                                   2048 x 1000
1/4 Frame                                    775                                                   2048 x 500
1/8 Frame                                    901                                                   2048 x 250
Tips  and Tricks
Power-up Issue
There exist an issue powering up some versions of FlowSense 4M cameras. It is often not
enough just to remove the cord plug to the camera, instead to avoid this problem unplug the
entire power supply.
Black and White Level Adjustment?FlowSense 4M MKII
The FlowSense 4M MKII camera is a two TAP camera, this means, that there are two AD con-
verters in the camera, each one responsible for one half of the total image.
Due to small differences in the analogue circuits surrounding the two ADC's and the CCD, the
image produced can have different intensities for each half image. This note describes how to
adjust the analogue setting to bring the black and white level intensity as close as possible to
one another.
Two procedures exists, the first one described just initiated the balancing, and then the cam-
era will perform the adjustments needed.
Auto Balancing procedure:
1.   Turn the camera on.
2.   Wait until the camera has reached work-temperature.
3.   Cover the lens of the camera and click "Free run".
4.   In Device Properties for the camera enable "Enable black balance". This feature will try to
adjust the two images areas so that the black level is optical equal. The parameter will
reset to "Disable" state when done.
The best way to examine the black level adjustment is to use the LUT control from the con-
text menu of the display -> Display option->Colors.
(The adjustment may take some time, and if you are satisfied with the black level adjust-
ment, and the parameter has not yet been set to Disable by the system, then you can just
stop the camera from going on by disabling "Enable black balance". )
PAGE | 206
PAGE | 207
5.   Remove the cap in front of the lens and place a white surface in front of the camera. Make
sure that pixel values is not above 80% of full scale This feature will try to adjust the two
images areas so that the gain level is optical equal.
Enable "Enable Gain balance". The parameter will reset to disable state when done.
Manual Balancing procedure :
1.   Start up DynamicStudio and enter Acquisition mode. Click "Free Run". Let the camera run
in free run until the temperature of the camera has stabilized.
2.   Put a cap on the camera.
3.   Disable Auto Gain Control (Done in Parameters for the camera)
4.   Open "Color map and Histogram" by double clicking the Image display. Right click the his-
togram window and select "Auto Fit to Lut"
5.   Expand the property entry "User Gain control" (It is an advanced property, do if you can
not see the property right click the property windows and select Advanced View)
6.   Set "Master voltage offset" and Slave voltage offset fine tune" to 0
7.   Adjust the value "Master voltage offset" so that "maximum value"(in Statistical info of
Color map and Histogram window) is around 30. (This is an iterative process, start out with
the value 1000.)
8.   Adjust the value "Slave voltage fine tune" so that the two image half's have the same
intensity. (This is an iterative process, start out with the value 2000.)
9.   Take off the cap, and either take the lens of or put a White Balance filter in front of the
lens.
10.   Adjust the exposure time or open the aperture so the the mean intensity in the image is
around 2/3 of the full range.
11.   As a start value set the Master Gain to 2000, then go back and do 10. (this is an iterative
process. You will have to find a value that makes sense. No pixels values must be sat-
urated)
12.   Adjust "Slave gain fine-tune"(start out with a value of ex 1250) so that the two image half's
have the same intensity. (This is again an iterative process)
13.   Put a cap on the lens and check and adjust the "Slave voltage offset fine tune" so that the
two image half's have the same intensity.
14.   Take off the cap, and either take the lens of or put a White Balance filter in front of the
lens.
15.   Adjust "Slave gain fine-tune" so that the two image half's have the same intensity.
14.5.6 FlowSense CM 12M-70
The FlowSense CM is a CMOS?camera.
The cameras includes 8/10/12 bits data transmission.
Connecting the camera
Frame grabber
The only frame grabber that supports this camera under DynamicStudio is the NI PCIe-1433.
The camera is connected to the frame grabber as shown below:
Synchronization
The synchronization cable is connected to the frameg grabber on the TRIG input.
PAGE | 208
PAGE | 209
Camera appearance in DynamicStudio
Camera Parameters
Device information and version
This section of the camera parameters describe the camera in some detail.
Full name
The camera name
Serial number
The camera serial number.
Firmware version
The firmware version installed in the camera
Image sensor
This section describes the camera sensor.
Sensor size
The width and height of the sensor in pixels
Effective sensor size
Some sensors have inactive pixels that are not used. This parameter describes the area of the
sensor that are active.
Pixel depth(s)
Supported pixel depth.
The desired Pixel depth is selected in parameters for Image Format.
("FlowSense CM 12M-70" on the previous page
Pixel pitch
Describes the distance between two adjacent pixels(center to center).
Settings
Camera file.
The camera file used.
Pixel binning
The camera an do pixel binning or more corect Avaraging. Enabling binning does not gain
higher FPS but lowers the signal to noise level.
Bining 2x2 is suipported.
Analog Gain control
Digital Gain control
Others Settings
Hot Pixel Correction
Defective Pixel Correction
Tap readout
The camera can only run in 10, 4, 2 and 1 Tap mode. In 10 tap mode the camera can only run
in 8bit per pixel. In 4, 2 and 1 Tap moe the camra can run in 12, 10 and 8 bit per pixel.
CCD Temperature
The camera has a built in temperature sensor which monitors the internal camera tem-
perature. The sensor is placed in the warmest location inside the camera.
Image Format Parameters
Frame mode
This parameter specifies if the camera is to run in Single frame, Double frame or Single frame
double exposure mode. The default is "Use default from System Control" which means that
settings in the System control dialog will determine the frame mode.
Mirror or Rotate
This parameter specifies if the captured image should be mirrored or rotated.
Pixel depth
The internal camera processing of the CCD data is performed in 14 bits. The camera can out-
put the data in (14,) 12, 10 or 8 bit format. During this standard bit reduction process, the
least significant bits are by default truncated. If you wish to discard the most significant bits
instead use bit shift as described above.
Image Area (ROI)
The frame rate is determined by the selected vertical height settings. Reducing the width will
not effect the achievable frame rate.
14.5.7 FlowSense CX  4M-563
The FlowSense CX 4M-563 camera is a CMOS camera with an effective resolution of 2336 x
1728 pixels, 8/10 bit per pixel. The camera can run at frame rates up to 563 Hz full frame
(8bit).
PAGE | 210
PAGE | 211
Connecting the camera
Frame grabber
The camera is a CoaxXPress x4 camera, meaning that it needs 4 coax cables from the camer to
the CoaXPress frame grabber.
Only the Active Silicon CoaXPress AS-FBD-4XCXPX grabber supports this camera in Dynam-
icStudio.
The camera gets power from the frame grabber. When installing the frame grabber in the PC
remember to connect power to frame grabber.
The camera is delivered with either four coax cables or the following 5w5 CoaXPress cable
Frame grabber connectors:
Connecting the camera to the frame grabbber using the 5w5 CoaXPress :
The 4 coax BNC connectors must be connected to the frame grabber in the following way:
Red -> J1
Green -> J2
Blue -> J3
White -> J4
(Yellow Is not connected.)
Connecting the camera to the frame grabber using the four coax cables:
The camera has the following connectors:
Connector 1 is the first connector from left.
The 4 coax BNC connectors must be connected to the frame grabber in the following way:
Connector 1 -> J1
Connector 2 -> J2
Connector 3 -> J3
Connector 4 -> J4
Note:
Synchronization
The camera is triggered via the Frame grabber. The frame grabber can control up to 4 cam-
eras that all can be triggered through the frame grabber. Connect the Sync output on the syn-
chronizer to Trig 1 on the back plate next to the frame grabber:
14.5.8 Parameters for the camera
Analog Control
Gamma Correction:
Controls the gamma correction for pixel intensity
Black Level:
Defines the analog black level in %
AFE GAIN:
Defines the analog Gain in %
Custom Features
Filter Mode
PAGE | 212
PAGE | 213
Define the image filter mode of the camera
RAW:?image?filter?is?deactivated? Mono:?compensation?of?monochrome?images
Color:?compensation?of?color?images (color cameras are not supported by DynamicStudio)
Fixed Pattern Noise Reduction
Digital?sensors?have?a?noise?signature,?the?so?called?Fixed?Pattern Noise?
(FPN).?This?feature?switches?the?fixed?pattern?noise?reduction ON?or?OFF.f
ON:?Pixel?FPN?reduction?is?activated?in?order?to? improve?the?quality?of?the?image
OFF:?FPN?is?deactivated
Custom Sensor Clock
Defines the sensor clock to use if Custom Sensor Clock is enabled
Customer Sensor Clock Enable
Enables/disables the customer defined sensor clock
Device Information Selector
This feature defines the selector value for accessing an element from the device information
list.
Device information
This feature returns an element of the device information list selected by feature Device
information selector
Pulse Drain enable
Enables/disables the Pulse Drain feature
Pixel Pulse Reset enable
Switches the Pixel Reset mode feature on or off
When using slow frame rates of < 50 fps the image will show white dots. This is caused by the
long storage times for the pixel charge. To prevent this effect the PixelResetMode is activated
by default but can be deactivated Note: If PixelResetMode is switched on, the maximum
frame rate for ROIs with a width <3200 pixel is limited. In practical use this does not cause any
problem as this mode is not used with higher frame rates
---
Calibration (in PC software)
This calibration handles static noise, pixel sensitivity, defective pixel readout. To use the cal-
ibration, two different images have to be created by running the camera in special modes,
where a black reference image, a sensitivity map, and an error pixel map is generated.
Semi Automatic Sensor calibration
for high image quality and noise removal a camera sensor calibration might be necessary
after changing the exposure time, the frequency, the operation mode form single- to double-
frame (or the other way round). When the user tries to start a preview or an acquisition after
a change of parameters and the software reminds him to perform a calibration and the fol-
lowing pop up will occur:
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration. Please note that for many
cameras without an internal mechanical shutter the lens has to be covered during the pro-
cess.
This "Semi automatic calibration" can also be started from the device settings under "Image
Sensor Calibration"
In some cases a manual calibraiton may still be needed. therefore please follow the following
routine:
Note: The Black reference and Flat field correction must be performed while images
are acquired by  the camera and shown in the user interface. Do a Free run or Pre-
view to perform the calibration.
Black reference
First an average black reference image has to be acquired and calculated. The Lens must be
covered so that no light reaches the sensor and the images must be acquired in Free run of
Preview to have a steady stream of data for the calibration routine to work on.
The acquired images are averaged and saved to an internal black reference image for later
use.
After 5 seconds of operation, the system prompts the user to stop the process when a steady
Black Reference image seen on the screen.
The user should look at the image to see if it is static, and no major changes are seen.
To be able to examine the image, which is very dark, the lookup table in the Color map dialog
has to be used.
PAGE | 214
PAGE | 215
Flat field correction
Second, a pixel sensitivity map is created. To do this a White Balance filter and a light source
with constant light has to be used.
Put the White Balance filter in front of the lens, and aim the camera at the light.
The camera must be acquiring images in either Free run or Preview.
The Image must not be saturated. The mean intensity of the image should be around 2/3 of
the saturation level (i.e. around 170 for an 8-bit camera).
After 5 seconds of operation, the system prompts the user to stop the process when a good
White balance image is seen.
The user should examine the images presented to see if the image is all the same pixel value,
and no major changes are seen. As the image gets more and more static, less and less dif-
ferent pixel values will be seen.
To examine the image, the lookup table in the Color map dialog should be used.
Defective pixels
After both the black reference, and the Pixel sensitivity calibration is performed, the cal-
ibration routine will try to find defective pixels.
This defective pixel map is created based on information from the black reference image and
(if calculated) the pixel sensitivity map.
(The outer row and columns and rows of the full image is not validated or corrected)
Finding hot sensor cells
Hot sensor cells are cells where:
Black Reference Image [x,y] > mean (Black Reference Image) + RMS(Black Reference
Image)*2)
Finding defective sensor cells
Defective (insensitive) sensor cells are cells where:
Abs(Pixel Sensitivity map (x,y) - mean(Pixel Sensitivity map)) > RMS(Pixel Sensitivity map) * 1.4
When Calibration is done
When calibration is done the parameter "Perform black reference calibration" and Perform
flat field correction calibration" should be locked. This to secure that the parameter not by acci-
dent is clicked and a new calibration is started. If a new calibration is started the old calibration
will be lost.
Locking and unlocking parameters is done in the following way:
l  To lock a parameter right click the parameter name and select "Lock Value".
l  To unlock the parameter right click the parameter name and select "Unlock value".
Enabling and using the calibration
The tree different calibrations can be enabled separately in parameters for the camera.
Background removal
Enabling this calibration, the calculated background reference will be subtracted from the
image acquired.
Flat field correction
Enabling this calibration, the acquired images are multiplied by the calculated pixel sensitivity
map.
Defective pixel correction
Enabling this calibration, will replace the pixels in the acquired image, indicated in the Defect-
ive pixel map, with a calculated value based on the surrounding pixel values.
The calculation is done in the following way:
The surrounding pixels are weighted by use of the following metrics, The center value is 0 rep-
resenting the Pixel value that is to be calculated:
(Pixels along the image edge are not validated or corrected)
Using Streaming to disk and Quick conversion warning
When streaming to disk only raw images are saved to the disk. This means, that if you do Quick
conversion the Background removal, Flat field correction and Defective pixel correction set-
tings will have no effect, the images will be saved raw to the database.
If you do not use Quick conversion and save the streamed images normally to the main data-
base, the Background removal, Flat field correction and defective pixel correction settings will
have effect and the images saved to the database will be without noise.
14.5.9 FlowSense CX  12M-160
The FlowSense CX 12M-160 camera is a CMOS camera with an effective resolution of 4096 x
3072 pixels, 8/10 bit per pixel. The camera can run at frame rates up to 160 Hz full frame
(8bit).
Connecting the camera
Frame grabber
The camera is a CoaxXPress x4 camera, meaning that it needs 4 coax cables from the camera
to the CoaXPress frame grabber.
Only the Active Silicon CoaXPress AS-FBD-4XCXPX grabber supports this camera in Dynam-
icStudio.
PAGE | 216
PAGE | 217
The camera gets power from the frame grabber. When installing the frame grabber in the PC
remember to connect power to frame grabber.
The camera is delivered with either four coax cables the following 5w5 CoaXPress cable
The CXP-6 plug is connecter to the camera connectors shown below. The CXP-6 plug is turned
so that the red core is connected to the left most connecter (CX1) on the camera.
(Connector on the camera are named CX1 to CX4 read from left)
The 4 coax BNC connectors must be connected to the frame grabber in the following way:
CX1 -> J1
CX2 -> J2
CX3 -> J3
CX4 -> J4
Note:
Synchronization
The camera is triggered via the Frame grabber. The frame grabber can control up to 4 cam-
eras that all can be triggered through the frame grabber. Connect the Sync output on the syn-
chronizer to Trig 1 on the back plate next to the frame grabber:
14.5.10 Parameters for the camera
Analog Control
Column Gain:
Controls the column gain of the image; 0-3; gain can be incremented by 1
Black Level:
PAGE | 218
PAGE | 219
Controls the analogue black level; 0-255; level can be incremented by 1
Gamma Correction:
Controls the gamma correction;?0.1-3;
Custom Features
Fixed Pattern Noise Reduction:
Switches the Fixed Pattern Nose reduction on or off
Calibration (in PC software)
This calibration handles static noise, pixel sensitivity, defective pixel readout. To use the cal-
ibration, two different images have to be created by running the camera in special modes,
where a black reference image, a sensitivity map, and an error pixel map is generated.
Semi Automatic Sensor calibration
for high image quality and noise removal a camera sensor calibration might be necessary
after changing the exposure time, the frequency, the operation mode form single- to double-
frame (or the other way round). When the user tries to start a preview or an acquisition after
a change of parameters and the software reminds him to perform a calibration and the fol-
lowing pop up will occur:
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration. Please note that for many
cameras without an internal mechanical shutter the lens has to be covered during the pro-
cess.
This "Semi automatic calibration" can also be started from the device settings under "Image
Sensor Calibration"
In some cases a manual calibraiton may still be needed. therefore please follow the following
routine:
Note: The Black reference and Flat field correction must be performed while images
are acquired by  the camera and shown in the user interface. Do a Free run or Pre-
view to perform the calibration.
Black reference
First an average black reference image has to be acquired and calculated. The Lens must be
covered so that no light reaches the sensor and the images must be acquired in Free run of
Preview to have a steady stream of data for the calibration routine to work on.
The acquired images are averaged and saved to an internal black reference image for later
use.
After 5 seconds of operation, the system prompts the user to stop the process when a steady
Black Reference image seen on the screen.
The user should look at the image to see if it is static, and no major changes are seen.
To be able to examine the image, which is very dark, the lookup table in the Color map dialog
has to be used
Flat field correction
Second, a pixel sensitivity map is created. To do this a White Balance filter and a light source
with constant light has to be used.
Put the White Balance filter in front of the lens, and aim the camera at the light.
The camera must be acquiring images in either Free run or Preview.
The Image must not be saturated. The mean intensity of the image should be around 2/3 of
the saturation level (i.e. around 170 for an 8-bit camera).
After 5 seconds of operation, the system prompts the user to stop the process when a good
White balance image is seen.
The user should examine the images presented to see if the image is all the same pixel value,
and no major changes are seen. As the image gets more and more static, less and less dif-
ferent pixel values will be seen.
To examine the image, the lookup table in the Color map dialog should be used.
Defective pixels
After both the black reference, and the Pixel sensitivity calibration is performed, the cal-
ibration routine will try to find defective pixels.
This defective pixel map is created based on information from the black reference image and
(if calculated) the pixel sensitivity map.
(The outer row and columns and rows of the full image is not validated or corrected)
Finding hot sensor cells
Hot sensor cells are cells where:
Black Reference Image [x,y] > mean (Black Reference Image) + RMS(Black Reference
Image)*2)
Finding defective sensor cells
Defective (insensitive) sensor cells are cells where:
Abs(Pixel Sensitivity map (x,y) - mean(Pixel Sensitivity map)) > RMS(Pixel Sensitivity map) * 1.4
When Calibration is done
When calibration is done the parameter "Perform black reference calibration" and Perform
flat field correction calibration" should be locked. This to secure that the parameter not by acci-
PAGE | 220
PAGE | 221
dent is clicked and a new calibration is started. If a new calibration is started the old calibration
will be lost.
Locking and unlocking parameters is done in the following way:
l  To lock a parameter right click the parameter name and select "Lock Value".
l  To unlock the parameter right click the parameter name and select "Unlock value".
Enabling and using the calibration
The tree different calibrations can be enabled separately in parameters for the camera.
Background removal
Enabling this calibration, the calculated background reference will be subtracted from the
image acquired.
Flat field correction
Enabling this calibration, the acquired images are multiplied by the calculated pixel sensitivity
map.
Defective pixel correction
Enabling this calibration, will replace the pixels in the acquired image, indicated in the Defect-
ive pixel map, with a calculated value based on the surrounding pixel values.
The calculation is done in the following way:
The surrounding pixels are weighted by use of the following metrics, The center value is 0 rep-
resenting the Pixel value that is to be calculated:
(Pixels along the image edge are not validated or corrected)
Minimum interframe time
The camera is configured to have a minimum interframe time of 37 μs, which will work in most
cases. For very specific configurations with reduced ROI, the camera may not deliver images
with this setting. If this occurs in your setup, switch the UI?level to Expert in the properties
and under Settings for?Double-Frame, increase the Minimum Interframe time by few micro-
seconds until the camera works again.
14.5.11 FlowSense CX  25M-80
The FlowSense CX 25M-80 camera is a CMOS camera with an effective resolution of 5120 x
5120 pixels, 8/10 bit per pixel. The camera can run at frame rates up to 80 Hz full frame (8bit).
Connecting the camera
Frame grabber
The camera is a CoaxXPress x4 camera, meaning that it needs 4 coax cables from the camera
to the CoaXPress frame grabber.
Only the Active Silicon CoaXPress AS-FBD-4XCXPX grabber supports this camera in Dynam-
icStudio.
The camera gets power from the frame grabber. When installing the frame grabber in the PC
remember to connect power to frame grabber.
The camera is delivered with either 4 single coax cables the following 5w5 CoaXPress cable
The CXP-6 plug is connecter to the camera connectors shown below. The CXP-6 plug is turned
so that the red core is connected to the left most connecter (CX1) on the camera.
(Connector on the camera are named CX1 to CX4 read from left)
The 4 coax BNC connectors must be connected to the frame grabber in the following way:
CX1 -> J1
CX2 -> J2
CX3 -> J3
CX4 -> J4
PAGE | 222
PAGE | 223
Note:
Synchronization
The camera is triggered via the Frame grabber. The frame grabber can control up to 4 cam-
eras that all can be triggered through the frame grabber. Connect the Sync output on the syn-
chronizer to Trig 1 on the back plate next to the frame grabber:
14.5.12 Parameters for the camera
Analog Control
Column Gain:
Controls the column gain of the image; 0-3; gain can be incremented by 1
AFE GAIN:
Controls the AFE gain of the image; 1-4; gain can be incremented by 1
Black Level:
Controls the analogue black level; 0-255; level can be incremented by 1
Custom Features
Pixel Reset Mode:
Switches the Pixel Reset mode feature on or off
When using slow frame rates of < 50 fps the image will show white dots. This is caused by the
long storage times for the pixel charge. To prevent this effect the PixelResetMode is activated
by default but can be deactivated Note: If PixelResetMode is switched on, the maximum
frame rate for ROIs with a width <3200 pixel is limited. In practical use this does not cause any
problem as this mode is not used with higher frame rates
Fixed Pattern Noise Reduction:
Switches the Fixed Pattern Nose reduction on or off
Calibration (in PC software)
This calibration handles static noise, pixel sensitivity, defective pixel readout. To use the cal-
ibration, two different images have to be created by running the camera in special modes,
where a black reference image, a sensitivity map, and an error pixel map is generated.
Note: The Black reference and Flat field correction must be performed while images
are acquired by  the camera and shown in the user interface. Do a Free run or Pre-
view to perform the calibration.
Semi Automatic Sensor calibration
for high image quality and noise removal a camera sensor calibration might be necessary
after changing the exposure time, the frequency, the operation mode form single- to double-
frame (or the other way round). When the user tries to start a preview or an acquisition after
a change of parameters and the software reminds him to perform a calibration and the fol-
lowing pop up will occur:
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration. Please note that for many
cameras without an internal mechanical shutter the lens has to be covered during the pro-
cess.
This "Semi automatic calibration" can also be started from the device settings under "Image
Sensor Calibration"
PAGE | 224
PAGE | 225
In some cases a manual calibration may still be needed. therefore please follow the following
routine:
Black reference
First an average black reference image has to be acquired and calculated. The Lens must be
covered so that no light reaches the sensor and the images must be acquired in Free run of
Preview to have a steady stream of data for the calibration routine to work on.
The acquired images are averaged and saved to an internal black reference image for later
use.
After 5 seconds of operation, the system prompts the user to stop the process when a steady
Black Reference image seen on the screen.
The user should look at the image to see if it is static, and no major changes are seen.
To be able to examine the image, which is very dark, the lookup table in the Color map dialog
has to be used.
Flat field correction
Second, a pixel sensitivity map is created. To do this a White Balance filter and a light source
with constant light has to be used.
Put the White Balance filter in front of the lens, and aim the camera at the light.
The camera must be acquiring images in either Free run or Preview.
The Image must not be saturated. The mean intensity of the image should be around 2/3 of
the saturation level (i.e. around 170 for an 8-bit camera).
After 5 seconds of operation, the system prompts the user to stop the process when a good
White balance image is seen.
The user should examine the images presented to see if the image is all the same pixel value,
and no major changes are seen. As the image gets more and more static, less and less dif-
ferent pixel values will be seen.
To examine the image, the lookup table in the Color map dialog should be used.
Defective pixels
After both the black reference, and the Pixel sensitivity calibration is performed, the cal-
ibration routine will try to find defective pixels.
This defective pixel map is created based on information from the black reference image and
(if calculated) the pixel sensitivity map.
(The outer row and columns and rows of the full image is not validated or corrected)
Finding hot sensor cells
Hot sensor cells are cells where:
Black Reference Image [x,y] > mean (Black Reference Image) + RMS(Black Reference
Image)*2)
Finding defective sensor cells
Defective (insensitive) sensor cells are cells where:
Abs(Pixel Sensitivity map (x,y) - mean(Pixel Sensitivity map)) > RMS(Pixel Sensitivity map) * 1.4
When Calibration is done
When calibration is done the parameter "Perform black reference calibration" and Perform
flat field correction calibration" should be locked. This to secure that the parameter not by acci-
dent is clicked and a new calibration is started. If a new calibration is started the old calibration
will be lost.
Locking and unlocking parameters is done in the following way:
l  To lock a parameter right click the parameter name and select "Lock Value".
l  To unlock the parameter right click the parameter name and select "Unlock value".
Enabling and using the calibration
The tree different calibrations can be enabled separately in parameters for the camera.
Background removal
Enabling this calibration, the calculated background reference will be subtracted from the
image acquired.
Flat field correction
Enabling this calibration, the acquired images are multiplied by the calculated pixel sensitivity
map.
Defective pixel correction
Enabling this calibration, will replace the pixels in the acquired image, indicated in the Defect-
ive pixel map, with a calculated value based on the surrounding pixel values.
The calculation is done in the following way:
The surrounding pixels are weighted by use of the following metrics, The center value is 0 rep-
resenting the Pixel value that is to be calculated:
(Pixels along the image edge are not validated or corrected)
14.5.13 FlowSense EO  Camera  series
The FlowSense EO cameras are advanced, high-resolution, progressive scan CCD cameras.
They are built around SONY’s and KODAK’s line of interline transfer CCD imagers.
PAGE | 226
PAGE | 227
FlowSense EO cameras are feature rich, have small size, very low power consumption, low
noise, and efficient and optimized internal thermal distribution.
The FlowSense EO cameras feature programmable image resolution, frame rates, gain, off-
set, programmable exposure, transfer function correction and temperature monitoring. A
square pixel provides for a superior image in any orientation.
The cameras includes 8/10/12(/14) bits data transmission. The adaptability and flexibility of the
cameras allows them to be used in a wide and diverse range of applications.
FlowSense EO  Camera specifications
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
FPS
(full
frame1)
Pixel Size
(microns)
Tap
Read
out
FlowSense EO VGA           640x480     8, 10, 12,
(14 sing.)
10         207/259       7.40         1/2
FlowSense EO 2M           1600x1200  8, 10, 12,
(14 sing.)
10            35/44          7.40         1/2
FlowSense EO 4M           2048X2048  8, 10, 12,
(14 sing.)
10            16/20          7.40         1/2
FlowSense EO 5M           2448X2050   8, 10, 12        12.5          11/16          3.45         1/2
FlowSense EO 11M        4008X2672   8, 10, 12          15          4.8/6.5        9.00          1/2
FlowSense EO 16M         4872x3248  8, 10, 12,
(14 sing.)
15           3.2/4.3        7.40          1/2
FlowSense EO 29M         6600x4400  8, 10, 12,
(14 sing.)
15           1.8/2.5         7.40         1/2
FlowSense EO 4M-32     2072x2072   8, 10, 12           8             32/26          7.40           4
FlowSense EO 4M-41     2352x1768   8, 10, 12          10            41/33          5.50           4
FlowSense EO 16M-9     4920x3280   8, 10, 12          10          8,3/6,6         5.50           4
FlowSense EO 16M-8     4880x3256   8, 10, 12          10              8/6             7.0            4
FlowSense EO 29M-5     6600x4400   8, 10 ,12          10          4.7/3.5          5.5            4
FlowSense EO 8M-12     3296x2472   8, 10, 12          10            21/17           5.5            4
FlowSense EO 8M           3312x2488  8, 10, 12,
(14 sing.)
8                11
5.5          1/2
FlowSense EO 1M-140   1032x1032   8, 10, 12           8               148             5.5            4
FlowSense EO 6M-25     2756x2208    8,10,12           10            25/20          4.54           4
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
FPS
(full
frame1)
Pixel Size
(microns)
Tap
Read
out
FlowSense EO 9M-17     3388x2712    8,10,12           10            14/17          3.69           4
1: The cameras can run in two different modes: Normal clocked and Over clocked. First figure
is normal clocked, second one over clocked.
The frame rates listed are valid when the cameras are used in Free running mode. When run-
ning in Single frame mode the exposure time has influence on how fast the cameras can run.
The maximum frame rate in Single frame mode can be calculated in the following way: max fps
=?1/(1/Full fame rate + exposure time).
Connecting the frame grabber and 4  Tap Read out Cameras
Connect Port 0 on the frame grabber to BASE on the camera.
Connect Port 1 on the frame grabber to MEDIUM on the camera.
Camera appearance in DynamicStudio
When the camera is detected by Dynamic studio it will appear in the device tree as follows:
Each of these "device" has properties that will be described in the following.
Camera Parameters
Device information and version
This section of the camera parameters describe the camera in some detail.
Full name
The camera name
Serial number
The camera serial number.
Image sensor
This section describes the camera sensor.
Sensor size
The width and height of the sensor in pixels
PAGE | 228
PAGE | 229
Effective sensor size
Some sensors have inactive pixels that are not used. This parameter describes the area of the
sensor that are active.
Pixel depth(s)
Supported pixel depth.
The desired Pixel depth is selected in parameters for Image Format.
("FlowSense EO Camera series" on page?226
Pixel pitch
Describes the distance between two adjacent pixels(center to center).
Settings
Camera file.
The camera file used.
Analog gain control
Specifies the analog gain and offset.
In Dual Tap readout the FlowSense EO camera has dual analog signal processors (or Analog
Front End ? AFE), one per channel. It features one dual processor, each containing a dif-
ferential input sample-and-hold amplifier (SHA), digitally controlled variable gain amplifier
(VGA), black level clamp and a 14-bit ADC. The programmable internal AFE registers include
independent gain and black level adjustment.
The figure below shows the relationship between the video signal output level and gain/off-
set. Theoretically, the black level should reside at 0 volts and the gain changes should only
lead to increasing the amplitude of the video signal.
Since the camera has two separate video outputs coming out of the CCD, there is always some
offset misbalance between the video outputs. Thus, changing the AFE gain leads to a change
in the offset level and to a further misbalance between the two video signals. To correct the
balance between two signals for a particular gain, always adjust the offset for each output.
Analog gain and offset can be linked, meaning that if gain for one tab is changed the other will
follow.
Bit selection
The Bit selection feature allows the user to change the group of bits sent to the camera out-
put and therefore manipulate the camera brightness. Up to 7 bits left or right digital shift can
be selected. The internal camera processing of the data is 14 bits. If the camera is set to out-
put 10 bits of data then the four least significant bits are truncated. In some cases the user
may need to convert from 14 to 10 bit by preserving the 4 least significant bits and truncating
the 4 most significant ones. Please note that the camera signal-to-noise ratio will be reduced
using the data shift option.
Tab Balance
Three different modes can be selected : None, Dynamic, Dynamic Once and Static Automatic.
In Dual tab readout the camera has two separate video outputs coming out of the CCD. There
is always some offset misbalance between the video outputs. Thus, changing the Gain leads to
a change in the offset level and to a further misbalance between the two video signals. To cor-
rect the balance between two signals at any particular gain, the cameras have static and
dynamic balancing algorithms implemented in the firmware. The algorithms compares the
black and bright levels of the adjacent pixels around the tap line, and adjusts the gain and off-
set for each tap accordingly, until the balance has been reached. The selection to use static or
dynamic balancing depends on the application.
(If the “Dynamic Once” is to be used, the parameter has to be set from None to Dynamic
Once.)
Digital Gain control
The camera has a built in digital gain and offset control. There are 20 possible digital gain
levels from 1.0x to 3.0x with step of 0.1x, and 1024 offset levels from (?511, to + 511).
Pixel binning
Horizontal binning combines adjacent pixels in horizontal directions to artificially create larger
and more sensitive pixels but with less resolution. FlowSense EO supports binning modes 2x2,
4x4 and 8x8.
Horizontal Binning is done in the digital domain, where the data from the binned pixels are
added digitally.
Vertical binning is a readout mode of progressive scan CCD image sensors where several
image lines are clocked simultaneously into the horizontal CCD register before being read
out. This results in summing the charges of adjacent pixels (in the vertical direction) from two
lines.
Horizontal and Vertical binning is used simultaneously. Vertical Binning is done in the time
domain, where the data from the binned lines is added in the CCD. Binning will result in higher
possible frame rates.
Others Settings
Dynamic black level correction
As described in Analog Gain Control, the reference black level on each CCD Tap output fluc-
tuates around 0V. The AFE offset correction works on the entire image and if there are noise
PAGE | 230
PAGE | 231
fluctuations on a line level, the AFE is not capable of correcting them. The camera has a built in
dynamic signal-to-noise correction feature to compensate for this effect. In the beginning of
each line the CCD has several back (masked) columns. The dark level for each tap is sampled
over several of these masked pixels and the average per tap black level floor is calculated for
each frame. The average floor level for each tap is then subtracted from each incoming pixel
(from the corresponding tap) from the next frame.
Hot- and Defective- Pixel correction
A CCD sensor is composed of a two-dimensional array of light sensitive pixels. In general, the
majority of the pixels have similar sensitivity. Unfortunately there can be pixels where sens-
itivity deviates from the average pixel sensitivity. A defective pixel is defined as a pixel whose
response deviates by more than 15% from the average response. In extreme cases these
pixels can be stuck ‘black’ or stuck ‘white’ and are non-responsive to light. There are two major
types of pixel defects ? “Defective” and “Hot”. The cameras come with a preloaded Defective
Pixel Map file.
Hot and Defective Pixel correction can be enabled or disabled.
Sensor Over clocking
The FlowSense EO cameras provides a unique way to control and increase the cameras nom-
inal speed. When 'Sensor Over clocking' is not enabled, the pixel clock is set to the CCD man-
ufacture recommended pixel clock frequency. Since the FlowSense EO camera internal design
is optimized for higher clock rates, it is possible to over clock the camera sensor. What happen
is that the internal clock will run ~20% faster than the CCD manufacture recommended pixel
clock. Special measures have been taken in order to preserve the camera performance when
over clocking is enabled.
Tab readout
4 Tap cameras can only run in 4 Tab mode., where as other cameras can run in single or dual
tap mode.
When operating in a Dual Tab mode, the image is split in two equal parts, each side consisting
of half of the horizontal pixels and the full vertical lines. The left half of the pixels are shifted
out of the HCCD register towards the left video amplifier ? Video L, while the right half of the
pixels are shifted towards the right video amplifier ? Video R. In the horizontal direction the
first half of the image appears normal and the second half is left/right mirrored. The camera
reconstructs the image by flipping the mirrored portion and rearranging the pixels.
CCD Temperature
The camera has a built in temperature sensor which monitors the internal camera tem-
perature. The sensor is placed in the warmest location inside the camera.
Negative Image
When operating in the negative image mode, the value of each pixel is inverted. The resulting
image appears negative.
Flat Field Correction
A CCD imager is composed of a two dimensional array of light sensitive pixels. Each pixel within
the array, however, has its own unique light sensitivity characteristics. Most of the deviation is
due to the difference in the angle of incidence and to charge transport artifacts. This artifact is
called ‘Shading’ and in normal camera operation should be removed. The process by which a
CCD camera is calibrated for shading is known as ‘Flat Field Correction’. The cameras come with
a preloaded Flat Field Correction file.
Image Format Parameters
Frame mode
This parameter specifies if the camera is to run in Single frame, Double frame or Single frame
double exposure mode. The default is "Use default from System Control" which means that
settings in the System control dialog will determine the frame mode.
Mirror or Rotate
This parameter specifies if the captured image should be mirrored or rotated.
Pixel depth
The internal camera processing of the CCD data is performed in 14 bits. The camera can out-
put the data in (14,) 12, 10 or 8 bit format. During this standard bit reduction process, the
least significant bits are by default truncated. If you wish to discard the most significant bits
instead use bit shift as described above.
Image Area (ROI)
The frame rate is determined by the selected vertical height settings. Reducing the width will
not effect the achievable frame rate.
Known issue
The first time after DynamicStudio has been started or if you have changed Tap readout
mode, then during the first Preview or Acquisition frame 1 and frame 2 can be exchanged so
that frame 1 becomes frame 2 and vice versa. This is only seen the very first time first time a
Preview or Acquisition is done. Once you stop and restart Preview or Acquisition no such issue
is seen.
14.5.14 FlowSense USB  2M-165
The FlowSense USB 2M-165 camera is a CMOS camera with an effective resolution of 1920 x
1200 pixels, 8/10/12 bit per pixel. The camera can run at frame rates up to 165 Hz full frame
(8bit).
Connecting the camera
The camera needs to connected to a USB3 port on the PC.
PAGE | 232
PAGE | 233
Back side with connection                                                                                    Front
side
with
lens
mount
Synchronization
The synchronization cable that comes with the camera is connected to the round connecter on
the backside
Camera appearance in DynamicStudio
Parameters
The USB interface and the USB 3 port does not contain any parameters that needs attention
from a normal user.
Parameters for the FlowSense USB 2M-165 camera
Image timestamps
If a synchronizer is connected to the camera, then timestamps are acquired from the syn-
chronizer by default. If a synchronizer is not connected, then timestamps are acquired from
the camera by default. This is the recommended behaviour, however the user can override
this behaviour by changing the "Use camera Timestamp" parameter for the image buffer of
the camera, as shown on the following screenshot. Note that this parameter is only visible in
when UI?level is set to Expert. If this parameter is enabled, timestamps are always taken from
the camera. If it is disabled, then timestamps are taken from the synchronizer if available, oth-
erwise timestamps are taken from the PC when the image arrives in DynamicStudio. Note
that the parameter is automatically set to the previously mentioned recommended value
when a synchronizer is connected/disconnected, so the setting should be changed after syn-
chonization cables have been configured correctly.
PAGE | 234
PAGE | 235
USB 3  Connection and PC
The maximum achievable frame rate by the camera is limited by the USB bandwidth and?PCIe
bandwidth of the acquisition PC. The bandwidth and thereby transfer speed of the camera is
very hardware dependent. Inefficient controllers or additional USB devices connected to the
same USB 3 Controller might not provide the bandwidth needed to run the camera at max-
imum frame rate. Hence, it is recommended to use a dedicated USB interface board like the
"StarTech.com Dual-Port USB 3.1 Card - 10Gbps per port - 2x USB-A - PCIe - USB-adapter".
Please also note that for maximum image acquisition frequency it might be necessary to
switch off the image preview, by deselecting the box for the camera in the system control:
Semi Automatic Sensor calibration
for high image quality and noise removal a camera sensor calibration might be necessary
after changing the exposure time, the frequency, the operation mode form single- to double-
frame (or the other way round). When the user tries to start a preview or an acquisition after
a change of parameters and the software reminds him to perform a calibration and the fol-
lowing pop up will occur:
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration. Please note that for many
cameras without an internal mechanical shutter the lens has to be covered during the pro-
cess.
This "Semi automatic calibration" can also be started from the device settings under "Image
Sensor Calibration"
14.5.15 HiSense 4M Camera
The HiSense 4M / HiSense 4MC camera uses a high performance progressive scan interline
CCD with a higher resolution than the HiSense MkII and FlowSense 2M cameras, but lower
sensitivity to green light when operating in full-resolution mode (approximately 55-50% at
532 nm and approximately 45-30% in the yellow-orange region of the light spectrum). Pixel
binning (2x2) possibility is available as well to gain in sensitivity.
The camera resolution is 2.048 by 2.048 light sensitive cells and an equal number of storage
cells. It runs with 12-bit resolution and re-sampled (upper) 8-bit resolution to gain space on
the hard disk. (Note the latter settings do not increase the frame rate.)
The difference between HiSense 4M and Hisense 4MC is that the 4MC camera is cooled.
14.5.16 HiSense 11M Camera
The HiSense 11M camera uses a high performance progressive scan interline CCD with a
higher resolution than the HiSense 4M cameras. The sensitivity is approximately 50-45% at
532 nm. Pixel binning is not available.
The camera resolution is 4.000 by 2.672 light sensitive cells and an equal number of storage
cells. It runs with 12-bit resolution and re-sampled (upper) 8-bit resolution to gain space on
the hard disk. (Note the latter settings do not enhance the frame rate.)
14.5.17 HiSense 6XX  cameras
The HiSense 6XX cameras are based on high performance progressive scan interline CCDs.
PAGE | 236
PAGE | 237
The camera resolutions are:
l  HiSense 610: 1600 x 1200
l  HiSense 620: 2048 x 2048
l  HiSense 630: 4008 x 2672
The cameras support 14-bit resolution only.
Camera settings
The following settings can be adjusted for the camera.
Pixel binning
The cameras support pixel binning 2 x 2. Binning combines neighboring pixels to form super
pixels. It increases the sensitivity of the sensor while decreasing the spatial resolution.
Camera Pixel clock
The sensor pixel readout clock can be set to:
l  HiSense 610: 10MHz or 40MHz
l  HiSense 620: 10MHz or 40MHz
l  HiSense 630: 8MHz or 32MHz
The pixel clock has influence on the image quality. A low pixel clock results in low readout
noise, where as the high pixel clock results in faster image readout and thereby higher frame
rate.
ADC converters
Using two analog to digital converters (ADC) will decrease the readout time. The Region of
Interest (ROI) must to be symmetric in the horizontal direction. When selecting an ROI to use,
specify the x position, and the width will be adjusted accordingly.
When two ADCs are used the maximum gray level is 10000. The reason for this is that the out-
put amplifier of the CCD has not enough bandwidth.
When one ADC is used the maximum gray level is 16384.
Using only one ADC you can freely select the ROI, but the sensor readout time is increased
resulting in a lower maximum frame rate.
ADC Offset control
To minimize the offset between the to ADC's it is necessary to add a certain signal level to the
real signal, to enable the measurement of the total noise floor (if the offset would be zero, an
unknown amount of noise would be cut off, since a negative light signal is not possible). The
stability of this offset is usually guaranteed by a proper temperature control and a software
control, which uses the information of "dark pixels" information from the sensor limits.
Further, algorithms must be applied to match the sensor performance if 2 ADC's are used for
readout. All this can be done automatically (Offset control enabled) or can be switched off (Off-
set Control disabled).
Noise filter
By enabling the Noise Filter the image data will be post-processed in order to reduce the
noise. This is done by the camera.
Camera preset Temperature
This setting controls the temperature of the image sensor.
14.5.18 Image buffer recourses
Unless the camera has onboard RAM for storing images during acquisition, you will have to
allocate RAM or system memory in the PC.
Two ways exists to allocate memory.
1.   Reserved memory (default)
Reserved memory is memory a preallocated when booting the PC. This is the preferred
method, since you always know exactly how much memory you have preallocated and
there by how many images you can acquire. It is also possible to allocate very large buf-
feres compared to Allocate on demand. On a 64bit Vista PC you can choose to allocate
more then 4GByte of memory.
2.   Allocate on demand
Each time the acquisition starts memory is dynamically allocated to hold images during
acquisition. There is always other programs running on the PC, and you will never know
exactly how much memory you can allocate before the real acquisition starts.
The size of the image buffer that you can allocate using this mode is limited to how much
the DynamicStudio process can allocate, ussaly below 1024 MByte.
You can choose between the to ways of using memory in properties for the "Image buffer
recourses"
Image buffer recources also handles Streaming to disk.
Reserve memory  for image buffer
After you have installed DynamicStudio or just the acquisition agent on a PC there is no pre-
allocated memory. When the system has detected the cameras connected the following dia-
log will be shown:
PAGE | 238
PAGE | 239
You will now have to chose which type of memory you will use during acquisition:
1.   Right click the in the Devices view and set a check mark at "Advanced view"
2.   Select "Image buffer resources" in the Device tree view.
3.   Select the Memory mode to use.
4.   In the Device Properties view set the property "Buffer size" to ex. 1024 MByte
5.   If you have chosen "Reserved memory" you will have to reboot the PC, otherwise the sys-
tem is ready for acquisition.
Reserving memory  on 32bit systems
On 32bit systems it is recommended to only allocate up to 2GByte RAM. The operating system
and applications running on the PC must have access to memory in order to function cor-
rectly. Reserving more than 2GByte memory for image buffer can lead to very slow behavior
of the PC.
Reserving memory  on 64bit systems
On 64bit systens only memory above 4GByte physical address space can be allocated. It is
therefore recommended to have more than 4GByte installed on 64 bit PCs.
You will be able to allocate all memory above 4GByte physical memory. If you f. ex. have
24GByte memory installed on the PC you will be able to allocate ~20GByte memory for image
buffer.
If the PC only have exactly 4GByte memory installed
Devices such as Display Adapters, Network interface cards etc. will occupy physical memory
space below 4GByte, so if a PC has 4GByte memory installed, then part of the memory
installed on the PC will be placed above the 4GByte physical address space. The amount of
memory above 4GByte address space in such case will vary from a few MByte up to several
100MByte. This is why that even if you only have 4GByte memory installed on the PC you will
be able to reserve some memory for image buffer.
14.5.19 Streaming to disk
Using frame grabber based cameras it is possible to stream data to disk while acquiring. In this
way the number of images that can be acquired is not limited by the amount of physical
memory (RAM) in the PC, but limited only by the size of the disk used for streaming.
Enabling streaming to disk
Streaming to disk is enabled in Device Properties of the "Image buffer resources" device:
l  Find the parameter Setup->Disk buffer.
l  Change the setting to "Enable".
When the Disk buffer has been enabled it is possible to select which disk/drive to use for
streaming. This is done by changing the setting of Disk-buffer->Drive. ( "Selecting the disk to
use for streaming to disk" on page?243
How streaming to disk works  in DynamicStudio
Even if you choose to use streaming to disk DynamicStudio still need a RAM buffer. This
RAM?buffer is used as a FIFO or Ring buffer.
Acquired images are filled into the FIFO from the frame grabber and then moved from the
FIFO to the disk.
The recommended size of the RAM buffer when streaming to disk is at least 200 MBytes per
camera.
Acquisition rate limitations  when streaming to disk
When doing a normal acquisition (without streaming to disk) the only bandwidth limitation
(measured in MBytes/second) is the PCI or PCI Express bus (and in some rare cases also the
RAM). When streaming to disk there is however the disk bandwidth that might reduce the
maximum acquisition trigger rate.
Measuring Disk performance
The bandwidth limitation for a specific disk can be measured using a special tool named
'DiskPerformanceTest.exe' located in the installation folder of DynamicStudio (C:/Program
files/Dantec Dynamics/DynamicStudio). Using this tool it is possible to get an idea about the
disk bandwidth limitation during acquisition. The tool does not measure the exact bandwidth
that can be used to calculate the maximum Acquisition rate during acquisition, since the data
flow is only in one direction:
l  from the RAM to the disk.
and not:
l  from the frame grabber, to the RAM, and then to the disk.
but it gives a good indication of what can be achieved.
PAGE | 240
PAGE | 241
Disk performance test
The Disk Performance Test program is used to estimate bandwith when writing image data to
disk.
Executeing the program will open the following dialog:
A disk performance test is performed by simulateing the disk access pattern DynamicStudio
uses when streaming to disk.
To use Disk Performance Test do the following:
1.   Select the Disk to test at "Disk drive to test".
2.   Selected the desired number of images to write to disk. A typical number would be 1000.
3.   Select the image size. If simulating a camera with an 8M pixel sensor, select "8388609".
4.   Select the block size to save. The block size should normally be the the same size as the
image size.
5.   Click start test.
Disk performance test will test the selected disk and show the result in the bottom of the dia-
log.
The resuls and figures ae describe below.
Over all Mean bandwidth
This figure is somewhat misleading. The result is calculated based on the total amount of time,
including the time needed to prepare the data and write the prepared data to disk.
Mean bandwidth
This is the most relevant result of the test. The resulting mean bandwidth is a good indication
of what bandwidth can be exprected when streaming to disk from DynamicStudio. The actual
bandwidth in DynamicStudio might be lower than the estimated bandwidth as the frame grab-
bers share the PCI bus and RAM access with the disk controller which will lower the write band-
width.
Max bandwidth
This figure shows the maximum bandwidth measured during the test
Min bandwidth
This figure shows the minimum bandwidth measured during the test
Bandwidth graph
The graph at the bottom of the dialog shows the measured bandwith of each image saved to
disk during the test.
Monitoring disk performance during acquisition
During acquisition and streaming to disk it is possible to monitor the usage of the Ring buffer.
To do this move the mouse over the camera progress bar in the System Control window and a
tool tip will appear with information about the image acquisition from the camera.
As you can see in the example above the 'Ring buffer usage' is at 1.85% .
Understanding Ring buffer usage percentage:
PAGE | 242
PAGE | 243
l  If the 'Ring buffer usage' stays at a low percentage the disk buffer bandwidth is high
enough to handle the data from the camera.
l  If the 'Ring buffer usage' grows towards 100% the disk can not handle the amount of data
from the camera. At the time it reaches 100% the system will stop. In the log there will be
a message telling that the Disk buffer is lagging behind.
Selecting the disk to use for streaming to disk
Do not use the disk on which the Operating System (OS) is installed, but use a disk which is only
used for streaming to disk, and again this disk should not be a partition on the same disk as
the OS, but a totally independent disk.
Use a RAID 0 disk if possible, since these disks are optimized for speed.
Saving images  stored in a disk buffer
After the acquisition is done it is now possible to save the data to the database. When images
are streamed to disk the images saved in the image buffer can be quick converted. What is
done is that the main database is given a reference to where the data is saved. Of course the
images are still kept in the image buffer meaning that the space occupied for the saved
images cannot be used before it is released.
In the database the image ensembles representing the data saved in the database will have
an indication that the data is remote, and the data is also handled as if the data is remote rel-
ative to the main database. Please see Distributed Database.
To enable Quick Conversion of image data, click Tools-Options and select the Acquisition tab.
Disable "Save Acquired data in main Database" and Enable "Do quick conversion on data
streamed to disk".
14.5.20 NanoSense Camera  Series
Installing the NanoSense camera
Operating the camera (Single and double frame)
iNanoSense Cameras
"Using long time between pulses in double frame mode" on page?246
Note: Having used analysis MATLAB Link the integration of the camera might fail. To solve this
restart DynamicStudio before using the camera after having used analysis MATLAB Link
The NanoSense camera series use high-speed CMOS image sensors. Apart from most other
cameras they feature onboard RAM storing images in the camera for later transfer to the
host PC. This means also that no frame grabber is required for these cameras; instead they
connect via a USB2 port in the PC.
The NanoSense MkII has a resolution of 512 x 512 pixel and runs up to 5.000 fps, and the Nan-
oSense MkIII has a resolution of 1.280 x 1.024 running up to 1.000 fps. Both cameras can run
even faster with reduced image height, and both are 10-bit cameras, but for increased trans-
fer speed you will normally keep only the upper, middle or lower 8-bits. Later versions of the
camera can only read pout the upper 8 bits. Combined with an enhanced gain function, which
minimizes the read out noise, this ensures very nice and crisp PIV images.
Installing the NanoSense Camera
The NanoSense camera driver is embedded in the DynamicStudio installation package. Thus it
is not necessary to install a separate driver. However, the first time you plug in the camera,
you will need to make sure that windows selects the correct driver before you start Dynam-
icStudio. Please follow the steps in the figure below to guide the Windows Plug-And-Play install-
ation system to assign the driver correctly. Windows will at a stage (step 3) ask if you want to
continue the installation. You must select "continue anyway" at this stage. Please also note
that the driver is assigned to a specific USB port. If you install the camera on a different USB
port you must repeat this procedure.
Detecting the Camera
NanoSense cameras are either equipped with an USB interface only or with both an USB inter-
face and a Gigabit Ethernet interface:
PAGE | 244
PAGE | 245
l  When using the USB interface the camera is detected automatically when connected to
the acquisition agent PC.
l  When using the Ethernet connector, the camera can only be detected, when the Nan-
oSense camera detector is added to the acquisition agent PC. This is done by right clicking
the agent in the Devices list, and choose Add new device -> Detector for NanoSense cam-
era.
Use a separate/dedicated Gigabit Network Interface Card (NIC) for connecting Nan-
oSense cameras using Ethernet.
Set a static IP address on this NIC ex. 100.100.100.1 and the subnet mask 255.255.255.0.
Operating the Camera (Single and Double Frame)
In order to run the NanoSense camera it must be configured using the cable connections dia-
gram. Please see the section "Normal Use" on page?76 for more information. The NanoSense
camera can be run in both single or double frame mode. Single frame mode is typically used
for alignment purposes and for LIF applications. In single frame mode, the exposure time can
be set in the "Device properties" window. Click on the camera in the device tree and edit the
"exposure time" entry
Running the NanoSense camera in Single frame mode
Running the camera in double frame mode is typically used in connection with PIV applic-
ations. if the radio button in the system control window is set to "Double frame mode" the cor-
responding entry in the Device Properties window is changed to " Setting for Double Frame
Mode". Note that in double frame mode it is only possible to adjust the exposure time for the
first frame (Frame 1). Please also note that setting the exposure time for frame 1 to a high
value will limit to obtainable frame rate. The default setting us 20 Microseconds.
Running the NanoSense camera in double frame mode
Using long time between pulses  in double frame mode
In some cases, the sum of the duration of the first and the second frame exposure is smaller
than the desired time between pulses. DynamicStudio will not accept the user to type in a
time between pulses which execs the total exposure time. Instead the system will adjust the
entered time between pulses down to the maximum allowable.
To resolve this issue the user must, manually, specify a longer exposure time for the first
frame. This is done in Device Properties for the camera.
iNanoSense Cameras
The main difference between the an ordinary NanoSense camera and an iNanoseSense cam-
era is that the iNanoSense camera has a build in Intensifier in front of the image sensor.
The Intensifier can be operated in to different ways, selected on the back of the camera.
l  Continues on
Used for running camera in Free Run. (The Intensifier is always on.)
l  Gated mode
This is the preferred mode of operation when the camera is operated in a synchronized
mode, either Preview or Acquire.(The intensifier is only on when the Gate pulse is high)
There are two setting for the intensifier:
l  Gate Pulse Width
The gate pulse width is the time period that the intensifier is open.
l  Gate Pulse Offset
The gate pulse offset is the time period from the laser pulse to when the intensifier
opens.
l  Trigger Polarity
The trigger polarity is the polarity of the signal that the intensifier needs to open.
PAGE | 246
PAGE | 247
Using the iNanoSense camera
1. Always use the camera in gated mode. The gate switch is located on the back. If you want to
use the camera in Free run connect a BNC cable from the SyncOut to the Gate in on the back
of the camera.
2. Keep the gain knob around 3/4 of the way towards maximum. This ensures that sufficient
gain is applied to the unit.
3. For safety when the unit is turned on keep the exposure to a minimum and the camera
aperture almost closed. Then adjust the camera lens aperture to the desired setting. If the
image saturates decrease the exposure to prevent damage to the Intensifier tube. If there is
no image increase slowly the exposure.
4. The previous approach is indicated when you use the camera to visualize a normally illu-
minated scene.
5. In Preview and Acquire: Make this exposure as small as possible. To avoid over-exposure
that could damage the tube start with the lens aperture totally closed and then slowly
increase the aperture until an image is obtained.
14.5.21 Photron  Camera  Series
The Photron camera series use high-speed CMOS image sensors. Apart from most other cam-
eras they feature onboard RAM storing images in the camera for later transfer to the host
PC. This means also that no frame grabber is required for these cameras; instead they con-
nect via a FireWire port or a gigabit Ethernet to the PC.?
Camera                                                                   Specifications
APX                      frame rate: 2000 Hz at full resolution
full resolution: 1024 x 1024
pixel depth: 10 bit
APX RS                   frame rate: 3000 Hz at full resolution
full resolution: 1024 x 1024
pixel depth: 10 bit
SA3                      frame rate: 2000 Hz at full resolution
full resolution: 1024 x 1024
pixel depth: 12 bit
SA1                      frame rate: 5400 Hz at full resolution
full resolution: 1024 x 1024
pixel depth: 12 bit
Cameras supported
Camera                                                                   Specifications
SA-X2                     Frame rate 12.5KHz at full resolution
Full resolution: 1024 x 1024
Pixel depth 8/12 bit
*Supports showing images while acquiring
*Supports Full frame rate in mode TimerBox slave (Pre-
ferred connection!)
*Supports Retrieving number of images acquired if an
acquisition is stopped prematurely
Mini UX100               Frame rate 4000hz at full resolution
Full resolution: 1024 x 1024
pixel depth: 12 bit
*Supports showing images while acquiring
*Supports Full frame rate in mode TimerBox slave (Pre-
ferred connection!)
*Supports Retrieving number of images acquired if an
acquisition is stopped prematurely
Mini AX100                Frame rate 4000hz at full resolution
Full resolution: 1024 x 1024
pixel depth: 12 bit
*Supports showing images while acquiring
*Supports Full frame rate in mode TimerBox slave (Pre-
ferred connection!)
*Supports Retrieving number of images acquired if an
acquisition is stopped prematurely
Mini WX50                Frame rate 750Hz
Full resolution:?2048 x 2048
Pixel depth: 12 bit
*Support showing images while acquiring
*Support Full frame rate in mode TimerBox slave (Pre-
ferred connection!)
*Supports Retrieving number of images acquired if an
acquisition is stopped prematurely
Mini WX100               Frame rate 1080Hz
Full resolution:?2048 x 2048
Pixel depth: 12 bit
*Support showing images while acquiring
*Support Full frame rate in mode TimerBox slave (Pre-
ferred connection!)
*Supports Retrieving number of images acquired if an
acquisition is stopped prematurely
PAGE | 248
PAGE | 249
"Working with more than one Photron camera?" below
"Photron Camera Series" on page?247
Camera sensor calibration
"Camera sensor calibration" on the next page
"Trigger modes" on page?252
Controlling Intensifier
More images  acquired than specified/acquisition takes  forever
Slow working of the Photron cameras
Preview and multiple cams
Detecting the camera
Photron cameras are either equipped with a firewire interface or an Ethernet interface. A
firewire interface camera is detected automatically when connected to the acquisition agent
PC. An Ethernet camera can only be detected, when the Photron camera detector is loaded.
This is done by right clicking the agent in the device list, and choose - add new device -
Detector for Photron camera.
The camera must be connected and powered up before the detector is added.
Ethernet cameras?
For Ethernet cameras set the IP address of the Network Interface card in the PC to
192.168.0.1 and the subnet mask to 255.255.255.0.
In the case that it is needed to change the IP address of the camera then the detector IP
search range should include the cameras address.
For Ethernet cameras it is an advantage to setup the Ethernet adaptor in the PC to use Jumbo
frames if the adaptor supports it. This can dramatically reduce the time it takes to save
acquired data.
To use Jumbo frames (MTU) open the Windows Control Panel->Network Connections. right-
click the adaptor that is connected to the camera and select properties. Under the General
tab click the Configure... button. Under the Advanced tab enable Jumbo frames. It is only pos-
sible to use Jumbo frames if the adaptor supports it.
Working with more than one Photron camera?
Before working with the cameras it is important to make sure that the cameras have different
IDs.
This is necessary for DynamicStudio to save individual settings to the right camera.
To check this start DynamicStudio and detect the cameras as described in "Photron Camera
Series" on page?247
The ID can be seen from the properties:
If there are more cameras with the same ID they should be changed.
For Ethernet cameras the ID is the same as the last number in the IP address. The ID for
these cameras can be changed from Device Properties of the camera by changing the IP
address. After changing the IP address the camera must be restarted to update it.
For firewire and PCI cameras the ID is by default 1.
The ID for these cameras must be changed via the keypad delivered with the camera. For
more information on how to do this see the camera manual.
Camera sensor calibration
As with all cameras the senors need to be black referenced regularly. Ideally with every
change of the exposure time, the frame rate or the inter frame time a new calibration is
done. Normally the camera performs this step automatically. But sometimes a manual cal-
ibration is necessary. There go the "Calibration" line in the device properties, switch the cal-
ibration off and turn it on again.
For Double frame mode the following procedure is used:
1.   Set “Calibration” in camera properties to "Off"
2.   Select Single frame mode
3.   Choose twice the trigger rate as the one to be used in Double frame mode.
4.   Click Preview . While in preview mode set "Calibration" to "On"
5.   A message box will show "Calibration done!" (The Acquisition system will stop)
6.   Switch back to Double frame and change the trigger rate to half the trigger rate.
Single/double frame mode
The Photron camera can be run in both single or double frame mode. The double frame
mode the exposure time will follow the cameras internal frame rate. In single frame mode,
the exposure time can be set in the "Device properties" window. Click on the camera in the
device tree and edit the "exposure time" entry. In this mode, the camera is running with a
fixed frequency, and the laser is fired in the center of each exposure.?Therefore it is possible
PAGE | 250
PAGE | 251
to adjust the exposure time of the frames. However, the camera clock does only support a
number of different exposure time , which can be chosen from a pull down list.
Operating a Photron camera in single frame mode
Running the camera in double frame mode is typically used in connection with PIV applic-
ations. If double frame mode is selected the laser pulses will be in the end of one frame and
the beginning of the next frame respectively.
Operating a Photron camera in double frame mode
Running the Photron camera in circular trigger mode
Running the camera in circular trigger mode makes it possible to listen to an external
event/trigger and when this event happens save images before and after the event.
Running this mode is done by selecting circular as the trigger mode in camera properties. This
will reveal the Trigger frames property making it possible to enter the number of frames to
save before and after a trigger event.
In the cable connection diagram the general in input on the camera must be connected to an
external device ex: pulse generator.
The actual camera must also have some kind of pulse generator connected that only gen-
erates one pulse pr. acquisition.
No other trigger generating device can be connected to the camera. The camera must oper-
ate as a master (see below for description).
Acquired images
As can be seen from the picture below the images acquired starts from two. This is because
the first two images acquired are poor and should not be used.
(For the SA-X2 camera the first image will be image 1. Usually this camera run in TimerBox
Slave mode. Here the first image will be good quality, where as in Master mode the first
image can be distorted, in this case, just sjio saving this image to the database)
Trigger modes
In order to run the Photron camera it must be configured using the cable connections dia-
gram. Please see the section "Normal Use" on page?76 for more information.
The camera can act differently depending on how it is connected to other devices.
There are three ways to connect the camera:
l  As master?(Not valid for the SA-X2, AX100 and Mini UX100)
l  As slave (Not valid for the SA-X2, AX100 and Mini UX100)
l  As timer box slave
Master
Running the camera as master is the preferred way of operating the camera. The camera's
performance is best when it runs in this mode, since the clock of the CMOS sensor is set to the
preferred value. When running the system in this mode, the camera clock will control other
devices in the system.
In order to run this mode, connect the sync-out connector to the device that needs to be syn-
chronized and connect the general-out or TTL-out if the device needs a camera start pulse (for
a definition of general-out and TTL-out refer to the camera manual).
The width of the sync signal on sync-out is equal to the exposure time of the camera.
The lowest possible frame rate when running in Master mode is 50 Hz.
Note for the image below. If the SA1 or SA3 are used then the 'General out' is called 'General
out 2' and the 'Sync out' is called 'General out 1'
PAGE | 252
PAGE | 253
The above image shows the use of the Timer Box and the below image shows the use of the
TimingHub. The TimingHub do not have the ability to control one input trigger with another so
there are no way of synchronizing the start of measurement with this approach.
Note
For the image below. If the SA1 or SA3 are used then the 'Sync out' is called 'General out 1'
An example of how to run the Photron camera as a master clock
The camera sends out sync pulses even it is not running, this means that the system needs a
start measurement pulse to synchronize the start of other devices connected to the timer
box.
The start/enable signal from the camera can go high both in between and at the same time as
the sync signal goes high.
This and the fact that the timer box needs a start/enable pulse to arm the trigger in input
could result in an image shift in the camera by one. The same effect is described in the Master-
/Slave configuration.
A visual validation of the images must be done to see if the images are shifted or not. If they
are, then the frame shifter should be set to 1 (Found under "info" in the image buffer devices
properties).
When a shift has been made browse through the images to update.?This is something that
must be done before saving the images. The validation must be performed on every acquis-
ition. After a shift the first or last double frame image will not be usable depending on what
shift direction.
Master / Slave
It is important to note that when more than one camera is connected it is not possible to run
in preview mode without the risk of getting bad images. See Preview and multiple cams
If two or more Photron cameras are used in synchronization (as in a stereo setup) they must
be of the same model and one camera must be the master and the rest must be slaves of the
master. Any other device in the synchronization chain is likewise a slave. The slave cameras
expects a sync signal on its sync-in connector, and this sync signal MUST be provided before
and while the measurement is ongoing.
To run this mode, connect a sync signal from the sync-out of the master camera to the sync-in
connector on the slave camera and connect a start measurement signal to the general-in or
the TTL-in connector (for a definition of general-in and TTL-in refer to the camera manual).
The slave camera is synchronized with the master so when changing properties these should
be change for the master and the master will automatically update the slave with the changes.
This is true for frame mode, ROI, frame rate and exposure time.
The slave camera is synchronized with the master so when changing properties these should
be change for the master and the master will automatically update the slave with the changes.
The lowest possible frame rate when running in slave mode is 50 Hz.
Note for the image below. If the SA1 or SA3 are used then the 'General out' is called 'General
out 2' and the 'Sync out' is called 'General out 1'
The above image shows the use of the Timer Box and the below image shows the use of the
TimingHub. The TimingHub do not have the ability to control one input trigger with another so
there are no way of synchronizing the start of measurement with this approach.
PAGE | 254
PAGE | 255
Note for the image below. If the SA1 or SA3 are used then the 'General out' is called 'General
out 2' and the 'Sync out' is called 'General out 1'
An example of how to run Photron cameras in a master/slave configuration
An artifact of the master slave configuration, is the fact that the start measurement signal
emitted from the master is not completely synchronized with the sync signal. Due to delay in
the electronic circuits this signal may be slightly delayed when received by the slave, which
could therefore be starting one frame to late. When running in double frame mode, this
would result in a double frame image, where frame B of the first double frame image
becomes frame A of the second and so on. This is clearly visible when toggling the double
frame images. The problem is solved by setting the frame shifter to 1 (Found under "info" in
the image buffer devices ).
The result of the delay in the start measurement signal. The problem is corrected by using the
frame shifter (below)
The frame shifter
Timer box slave
This is the preferred way of synchronizing the SA-X2, AX100 and Mini UX100 camera.
Mini UX100:
The Mini UX100 camera is running in frame sync mode. Here the TimerBox is driving the
frame sync signal of the camera. The Acquisition started either by DynamicStudio sending a
Software trigger signal to the camera, or when in Circular mode, an external trigger signal
send (preferred start mode when multiple cameras is in use) .
In double frame mode it can happen that the start trigger signal falls in the middle of a double
frame. This will result in images saved in the camera memory will be shifted one image. The
result of this is that the first image in a double frame can be image 2 from previous double
image and frame 2 is frame one i current double frame. In this case go to parameters for the
Input buffer and set an image shift.
Input 1 is connected to the Synchronizer, Input is used for external trigger signal to start the
measurement.
SA-X2 and AX100:
these cameras are running in Random Reset, where the internal frame synchronization clock
is reset each time the camera receives a trigger signal.
The camera can be run at full frame rate.
General In is used for triggering the SA-X2 and Input 1is used for triggering the AX100
Other cameras:
For other cameras than the SA-X2, AX100 and the Mini UX100 camera the following is valid:
If necessary it is possible to synchronize the camera from an external timing device (Such as
an 80N77 timer box). In this case the camera is a timer box slave. Since the camera always is
running on an internal clock, the only way to achieve external synchronization is to reset this
clock repeatedly. This is called random reset, and can only be done at a maximum rate of half
the camera frame rate including a reset time in single frame and 2/3 the camera frame rate
including a reset time in double frame . Timer box slave mode is the only way to make the cam-
era acquire data at a frame rate less than 25 Hz double frame mode)
To run this mode connect the timer box camera-out to the general-in on the camera.
PAGE | 256
PAGE | 257
The above image shows the use of the Timer Box and the below image shows the use of the Tim-
ingHub.
Running the Photron camera as a slave of an external timer
The difference between internal clock mode and external triggered mode. The drift is highly exag-
gerated
More images  acquired than requested/Acquisition takes  forever
The camera needs to fill up a memory partition before it will stop and partition sizes cannot be
set freely. This means that even if just 2 images has been requested the camera will continue
measuring until the partition is full and thus provide more images than requested. If a low trig-
ger rate and/or a small ROI has been selected it may take a while to fill up the memory par-
tition and the acquisition may thus take much longer than expected.
Slow working of the Photron cameras
Due to verification of settings and the general working of the camera, acquisition and general
handling of the camera may appear slow compared to other cameras. This is a well known
issue.
Controlling Intensifier
The intensifier is setup through the keypad or the Fastcam viewer utility that came with the
camera. Set the intensifier mode to external and disable the focus mode time out. When that
has been done the intensifier can be gated through DynamicStudio. In the Devices window
add an intensifier to the camera and establish the necessary connections in the Syn-
chronization Cables window. There are three setting for the intensifier:
l  Gate Pulse Width
l  Gate Pulse Offset
l  Trigger Polarity
PAGE | 258
PAGE | 259
The gate pulse width is the time period that the intensifier is open. The gate pulse offset is the
time period from the laser pulse to when the intensifier opens (this can be a negative value).
The trigger polarity is the polarity of the signal that the intensifier needs to open.
Preview and running with multiple cameras
It is important to note that when more than one camera is connected it is not possible to run
in preview mode without the risk of getting bad images. The reason for this is that in preview
the cameras are started and stopped again and again to be able to get images out of the cam-
eras to show to the user. During this starting and stopping the cameras must not get
triggered on 'General in'. Because of the nature of the timer box and of the master camera in
a master/slave configuration it is not possible to prevent this pulse in 'General in', thereby risk-
ing poor image quality.
The Photron APX, and APX-RS cameras uses a standard FireWire (IEEE 1394) interface.
FireWireR is the registered trademark of Apple Computer, Inc., for the IEEE 1394 interface
developed by the 1394 Working Group within the Institute of Electrical and Electronics Engin-
eers (IEEE).
Photron APX, APX-RS, and SA1 are registered trademarks of PHOTRON LIMITED, PHOTRON
USA INC, and PHOTRON EUROPE LIMITED. For more information visit http://www.-
photron.com.
14.5.22 SpeedSense 10XX  series
SpeedSense 10XX cameras are camera link cameras. The data from the camera are trans-
ferred to system memory or RAID disk using a frame grabber.
Main specification SpeedSense 1010:
l  1280x1024 pixels.
l  10 bits/pixel (only 8 bits can be readout from the camera (lower, middle upper 8 bit select-
able)).
l  maximum frame rate(full frame): 520 Hz single frame / 260 Hz double frame.
Main specification SpeedSense 1020:?
l  2320x1728 pixels.
l  10 bits/pixel (only 8 bits can be readout from the camera (lower, middle upper 8 bit select-
able)).
l  maximum frame rate(full frame): 170 Hz single frame / 85 Hz double frame.
The SpeedSense 10XX camera is connected as shown below:
The frame grabber must be a National Instruments PCIe-1429 or PCIe-1433 (Full camera link
frame grabber).
Calibration file
The camera is delivered together with a CD. The content of this cd includes a calibration file
for the camera. The first time Noise reduction is enabled, and the calibration file is not found
on the PC, DynamicStudio will ask for this CD. Follow the instructions given by DynamicStudio
to have the calibration file copied from the CD to the PC.
Known issues:
When acquiring image data from a SpeedSense 10XX camera the acquisition can suddenly
stop. The reason for this is FIFO-buffer overflow on the NI PCIe-1429 frame grabber.
To minimize the risk of this issue happening, reduce the width of the image acquired.
This issue can have two reasons. The first is due to the system RAM being to slow or scarce,
the second is due to the northbridge on the motherboard.
RAM Cause
The frame grabber is trying to push data through the bus, and the RAM just can't fill fast
enough to keep up. If this is the case, the buffer error is being caused by the system RAM,
and can typically be solved by upgrading the RAM speed or adding more RAM to the system.
PAGE | 260
PAGE | 261
Northbridge Cause
Theoretically, PCI express is supposed to have dedicated bandwidth. This means that if you
have two PCI express cards in your computer, the bus traffic on one will not affect the band-
width on the other. However, some motherboards (and more specifically the northbridge on
the motherboard) do not handle PCI express protocol as well as others. The result is that dur-
ing times of intense loading of the northbridge, the available bandwidth of the PCI express
bus will decrease. If it decreases below the transfer rate of the frame grabber (the data
inflow from the camera is greater than the outflow through the PCI express bus), the buffer
memory on the frame grabber will start to fill up. Because the onboard memory on the PCIe-
1429 is very small, it quickly fills up and the error occurs. If this is the cause, a new mother-
board will have to be procured or a different computer used. Alternatively, decreasing the
frame rate or image size also reduces the likelihood of the northbridge reducing the band-
width, thus avoiding the error.
Another possibility is that the PCI express slot is not a full slot. For example, some video card
slots have 16 channels going to the device (from the northbridge to the PCIe card) but 1 chan-
nel coming from the device. The result is a x16 slot that the PCIe card fits into, but can only
capture images at a x1 speed.
For more detailed information see: http://di-
gital.ni.com/public.nsf/allkb/32F3F5E8B65E9710862573D70075EED1
Note: These causes could exist in any PCI or PCI express card. The likelihood of it happening is
low however because most other frame grabbers don't transmit enough data to overload the
northbridge.
Specification on a PC that is known to work well
Below table shows the vital components of a PC that is known to work well with the
SpeedSense 10XX cameras and the PCIe-1429
PC CPU                                                                   IntelR Core? i7-940
Motherboard                                                        ASUS P6T/DELUXE INTEL X58
RAM                                                                        OCZ 3x2048 MB DDR3 1600 MHz
Bright horizontal line in the image
Images grabbed from the SpeedSense 10XX cameras may have a horizontal bright line and a
difference in the average intensity between the areas above and below this line:
The reason for this phenomena is that the camera is exposing an image while reading out the
previous image. The "Shutter line" appears only if the exposure time and the readout time sat-
isfy the following formula:
Exposure time + Sensor readout time < 1/Trigger frequency
Horizontal line seen in image:
In order to remove the line reduce or enlarge the exposure time or reduce or enlarge the
trigger frequency for the camera.
Horizontal line not seen in image (Trigger frequency reduced) :
Horizontal line not seen in image (Exposure time reduced and Trigger frequency enlarged) :
PAGE | 262
PAGE | 263
14.5.23 SpeedSense 1040 camera
The SpeedSense 1040 camera is a CMOS camera with an effective resolution of 2320 x 1723
pixels,10 bit per pixel (8 bit per pixel read out). The camera can run at frame rates up to 193
Hz. The frame rate can be doubled, by using two frame grabbers(not yet supported)
Connecting the camera
Frame grabber
The camera is a Full Camera Link camera, meaning 2 Camera Link cables are required to con-
nect the camera.
Only the National Instruments PCIe-1433 frame grabber supports this camera.
Connect the frame grabber and the camera as follows:
Camera                        Frame grabber
O2                                 Port 0
O1                                 Port 1
Note:
-The L2 indicates Power on and L3 indicates Active exposure.
-Power is connected to the camera via the SUB D 15-pol connector named Control
Synchronization
The camera is triggered via the Frame grabber. On the frame grabber the SMB trigger con-
nector labeled 'TRIG' must be connected to the trigger output of the Synchronizer.
(The camera is delivered with a small SMB to BNC converter cable)
14.5.24 Parameters for the camera
Gain
The digital gain setting controls the shifting of the pixel bits. It selects, which eight of the ten
digitizer bits are output to Camera Link.(Overflow is avoided by saturation to maximum)
Dark Value offset
This value is a number between 0 and 255 which is added after the "in camera" calibration is
performed. The offset is used to adjust the dark level and avoid clipping of pixels to black in
low light situations.
PAGE | 264
PAGE | 265
It is only applied when "in camera" calibration has been performed. The eight bits are aligned
with the least significant bits of the 10-bit pixel data from the sensor. Thus if Gain is 0, offset
must increment in steps of 4 to increase the output gray level in steps of 1.
Calibration (in camera firmware)
The in camera noise reduction is an implementation of simple Background removal. A more
advanced software implementation exist that handles the background much more effectively.
("Calibration (in PC software)" below
The In Camera calibration has to be done in order to be able to use Dark Value offset.
To do an In Camera calibration Click the button and follow the instruction.
(The calibration takes up to 10 seconds to perform),
Calibration (in PC software)
This calibration handles static noise, pixel sensitivity, defective pixel readout. To use the cal-
ibration, two different images have to be created by running the camera in special modes,
where a black reference image, a sensitivity map, and an error pixel map is generated.
Note: The Black reference and Flat field correction must be performed while images
are acquired by  the camera and shown in the user interface. Do a Free run or Pre-
view to perform the calibration.
Black reference
First an average black reference image has to be acquired and calculated. The Lens must be
covered so that no light reaches the sensor and the images must be acquired in Free run of
Preview to have a steady stream of data for the calibration routine to work on.
The acquired images are averaged and saved to an internal black reference image for later
use.
After 5 seconds of operation, the system prompts the user to stop the process when a steady
Black Reference image seen on the screen.
The user should look at the image to see if it is static, and no major changes are seen.
To be able to examine the image, which is very dark, the lookup table in the Color map dialog
has to be used.
Flat field correction
Second, a pixel sensitivity map is created. To do this a White Balance filter and a light source
with constant light has to be used.
Put the White Balance filter in front of the lens, and aim the camera at the light.
The camera must be acquiring images in either Free run or Preview.
The Image must not be saturated. The mean intensity of the image should be around 2/3 of
the saturation level (i.e. around 170 for an 8-bit camera).
After 5 seconds of operation, the system prompts the user to stop the process when a good
White balance image is seen.
The user should examine the images presented to see if the image is all the same pixel value,
and no major changes are seen. As the image gets more and more static, less and less dif-
ferent pixel values will be seen.
To examine the image, the lookup table in the Color map dialog should be used.
Defective pixels
After both the black reference, and the Pixel sensitivity calibration is performed, the cal-
ibration routine will try to find defective pixels.
This defective pixel map is created based on information from the black reference image and
(if calculated) the pixel sensitivity map.
(The outer row and columns and rows of the full image is not validated or corrected)
Finding hot sensor cells
Hot sensor cells are cells where:
Black Reference Image [x,y] > mean (Black Reference Image) + RMS(Black Reference
Image)*2)
Finding defective sensor cells
Defective (insensitive) sensor cells are cells where:
Abs(Pixel Sensitivity map (x,y) - mean(Pixel Sensitivity map)) > RMS(Pixel Sensitivity map) * 1.4
When Calibration is done
When calibration is done the parameter "Perform black reference calibration" and Perform
flat field correction calibration" should be locked. This to secure that the parameter not by acci-
dent is clicked and a new calibration is started. If a new calibration is started the old calibration
will be lost.
Locking and unlocking parameters is done in the following way:
l  To lock a parameter right click the parameter name and select "Lock Value".
l  To unlock the parameter right click the parameter name and select "Unlock value".
Enabling and using the calibration
The tree different calibrations can be enabled separately in parameters for the camera.
Background removal
Enabling this calibration, the calculated background reference will be subtracted from the
image acquired.
Flat field correction
Enabling this calibration, the acquired images are multiplied by the calculated pixel sensitivity
map.
Defective pixel correction
Enabling this calibration, will replace the pixels in the acquired image, indicated in the Defect-
ive pixel map, with a calculated value based on the surrounding pixel values.
The calculation is done in the following way:
PAGE | 266
PAGE | 267
The surrounding pixels are weighted by use of the following metrics, The center value is 0 rep-
resenting the Pixel value that is to be calculated:
(Pixels along the image edge are not validated or corrected)
Using Streaming to disk and Quick conversion warning
When streaming to disk only raw images are saved to the disk. This means, that if you do Quick
conversion the Background removal, Flat field correction and Defective pixel correction set-
tings will have no effect, the images will be saved raw to the database.
If you do not use Quick conversion and save the streamed images normally to the main data-
base, the Background removal, Flat field correction and defective pixel correction settings will
have effect and the images saved to the database will be without noise.
14.5.25 SpeedSense Ethernet Cameras
The SpeedSense Ethernet camera series cameras are state of the art high speed cameras
from Vision Research. It is not a traditional PIV camera but it is possible to acquire both single
and double frames at a very high rate with full frame resolution. The older generation cam-
eras (SpeedSense 9020, 9030, 9040, 9050) need two synchronization pulses to acquire a
double frame. For these older cameras, there is no way to tell which pulse is the first and
which the second one, therefore the user needs to decide which images to pair into a double-
frame before saving the images to the database. This can be done by entering the Acquired
Data window and toggling between the two frames. If the two frames do not show correlated
particle movement, then an offset needs to be entered in the device properties of the image
buffer before saving the images from RAM?to the database ("Wrong light pulse in double
frame" on page?283. The newer generation cameras (SpeedSense 9060, 9070, 9072, 9080
and 9090 marked with a yes in Double frame trigger mode in the table below) need only one
synchronization pulses to acquire a double frame like any other CCD?camera. Therefore the
user does not need to care for the correct pairing.
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
Minimum
Interframe
time (ns)
FPS
(full frame)
Pixel Size
(microns)
Double
frame
trigger
mode
9020      1152 x 896    8, 10,
12
n/a                n/a                  1000                  11.5
9030      800 x 600       8, 10,
12, 14
n/a                n/a                  6688                  22
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
Minimum
Interframe
time (ns)
FPS
(full frame)
Pixel Size
(microns)
Double
frame
trigger
mode
9040      1632 x 1200  8, 12,
14
n/a                1500               1016/508          11.5
9041      1600 x 1200  8, 12,
14
n/a                n/a                  1000/500          11.5
9050      2400 x 1800  8, 12,
14
n/a                1500               480/240            11.5
9060      1280 x 800    8, 12       n/a                n/a                  6242/3121       20                yes
9070      1280 x 800    8, 12       n/a                n/a                  3140/1570       20                yes
9072      1280 x 800    8, 12       n/a                n/a                  2190/1095       20                yes
9080      2560 x 1600  8, 12       n/a                n/a                  1514/757          10                yes
9084      2560 x 1600  12            n/a                n/a                  1450/725          10                yes
9090      1280 x 800    8, 12       n/a                n/a                  7500/3750       20                yes
211         1280 x 800    8, 12       1                    700                 2190                  20                yes
311         1280 x 800    8, 12       1                    700                 3250                  20                yes
611         1280 x 800    8, 12       1                    500                 6242                  20                yes
711         1280 x 800    8, 12       1                    500                 7530                  20                yes
341         2560 x 1600  8, 12       1                    400                 12000                10                yes
641         2560 x 1600  12            1                    400                 16000                10                yes
M110     1280 x800     12            2                    500                 1630/815          20                yes
M120     1920 x 1200  12            1                    1400               730/365            10                yes
M140     1920x1200    12            1                    1400               400/200            10                yes
M310     1280 x 800    12            1                    500                 3260/1630       20                yes
M320     1920x1200    12            1                    1400               1380                  10                yes
M340     2560 x 1600  12            1                    1400               800/400            10                yes
1210      1280 x 800    12            1                    400                 12700/6350     28                yes
1610      1280 x 800    12            1                    400                 16600/8300     28                yes
1211      1280 x 800    12            1                    725                 12600/6300     28                yes
1611      1280 x 800    12            1                    525                 16600/8300     28                yes
PAGE | 268
PAGE | 269
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
Minimum
Interframe
time (ns)
FPS
(full frame)
Pixel Size
(microns)
Double
frame
trigger
mode
2511      1280 x 800    12            1                    375                 25600/12800   28                yes
VEO E
310
1280 x 800    12            1                    500                 3260/1630       20                yes
VEO E
340
2560 x 1600  12            1                    1710               800/400            10                yes
VEO
340
2560 x 1600  12            1                    1710               800/400            10                yes
VEO
440
2560 x 1600  12            1                    1710               1100/400          10                yes
VEO
640
2560 x 1600  12            1                    1700               1400/700          10                yes
VEO
710
1280 x 800    12            1                    395                 7500/3750       20                yes
VEO
410
1280 x 800    12            1                    480                 5200/2600       20                yes
1612      1280 x 800    12            1                    425                 1660/8300       28                yes
2012      1280 x 800    12            1                    425                 22600/11300   28                yes
2512      1280 x 800    12            1                    375                 25700/12850   28                yes
1840      2048 x 1952  12            .5                   142ns fast
option
4510 / 17240
(Binning)
13.5             yes
2640      2048 x 1952  12            .5                   142ns fast
option
6600/ 25030
(Binning)
13.5             yes
Lab
110
1280 x800     12            2                    500                 1630/815          20                yes
Lab
320
1920x1200    12            1                    1400               1380                  10                yes
Lab
120
1920 x 1200  12            1                    1400               730/365            10                yes
Lab
140
1920x1200    12            1                    1400               400/200            10                yes
Lab
310
1280 x 800    12            1                    500                 3260/1630       20                yes
Model
Maximum
Resolution
(pixels)
Bit
Depth
(bits)
Minimum
Exposure
Time (μs)
Minimum
Interframe
time (ns)
FPS
(full frame)
Pixel Size
(microns)
Double
frame
trigger
mode
Lab
340
2560 x 1600  12            1                    1400               800/400            10                yes
Lab
3a10
1280 x 800    12            1                    1400               1850/925          10                Yes
Connecting the Camera
l  Connect the power supply
l  Connect the Ethernet to the Agent PC
For 1Gb connection
Set the IP Address of the Agent PC Ethernet adapter to 100.100.xxx.xxx (for example
100.100.0.1). Set the Subnet mask to 255.255.0.0
For 10Gb connection,
Install 'Raw Packet reader for Phantom cameras with 10Gb Ethernet' found on the Dynam-
icStudio installation media or on DVD's that comes with the camera (With out this driver
installed on the PC the communication speed will be limited).
Set the IP Address of the Agent PC Ethernet adapter to 172.16.xxx.xxx (for example
172.16.0.1). Set the Subnet mask to 255.255.0.0
l  Connect the F-Sync via a BNC cable to the synchronizer
Some cameras doesn't have an F-Sync on it's back panel, in this case use the supplied sync
cable that came with the camera.
l  On models like the VEO, the XX12 and the xx40 series have programmable BNC con-
nectors. For these cameras the connectror I/O 3 needs to be used to synchronize the
frames.
PAGE | 270
PAGE | 271
It is preferable to have a dedicated network adaptor for communicating with the camera(s).
The camera does not support DHCP so when connecting the camera it is important to setup
the network adaptor as well.
This is done through Windows Network Connections properties for the specified adaptor con-
nected to the camera.
The subnet mask must match the one of the camera. It can be retrieved from the bottom of
the camera.
If it is necessary to alter the IP settings for the camera it can be done using the Vision
Research software supplied with the camera.
Connecting multiple Phantom cameras  to the Synchronizer
You can chose to connect multiple cameras to one or more outputs on the synchronizer.
If you chose to use one output for ex. two or more cameras, then when changing parameters
as ex. exposure time , the exposure time for all cameras connected to the same output will bi
changed in one operation.
Detecting the Camera
To control the camera from DynamicStudio, a Detector needs to be added to the device tree.
This is done by right-clicking the Acquisition agent and selecting 'Add New Device...'. From the
device list select "SpeedSense Ethernet camera detector".
Afther having added the detector, the detector will find connected cameras and automatically
add these to the setup.
Running the camera
The camera is operated like any other camera from Dynamic Studio but there are currently a
limitation. During acquisition (free run, preview or acquire mode) the display is updated with
preview images from the camera, this is a special way of getting images during acquisition. In
double frame mode it can not be guaranteed that frame one is really frame one. This is
because the preview image from the camera is a random image and could just as well be
frame two. This however is only during preview. The images acquired are arranged correctly.
On some cameras an F-Sync connector is supplied both directly on the camera but also on the
"Capture" cable. Always use the F-Sync directly on the camera.
Not that some of the functions shown below in the “Device Properties” may need a change of
the UI Level in order to find all the functions and settings used here. For more information on
selecting UI Levels please see (Link to "Device Properties" on page?82 )
Calibrating
The camera can be operated in two ways, Single frame and double frame mode. For both
operation modes, it is strongly recommended to perform a calibration when changing camera
setting such as image frame rate or exposure time.
To remind users when to calibrate a message will occur when a change was applied before an
acquisition or a preview starts. Therefore the following pop-up opens when the software has
detected that the sensor needs calibration.
In case you are fine with the camera sensor calibration you can press "Cancel", or "Acquire". In
case you want to update the sensor calibration please press preview and calibrate. and you
are guided through the sensor calibration procedure, where the software will automatically
select the needed method for your actual camera configuration.
This "Semi automatic calibration" can also be called from the device settings under "Image
Sensor Calibration"
In some seldom cases it still might be needed that you use other sensor calibration methods
as the "InCamera calibration" or the "Black Reference Calibration" for single or double frame
recording. But for most cases the "Semi automatic calibration" should work fine. Never the
less below the manual steps for a proper sensor calibration for single or double frame acquis-
ition are listed:
PAGE | 272
PAGE | 273
Single  frame  mode  (In  Camera Calibration):
DynamicStudio indicates automatically in the device properties window (property called "Per-
form in Camera Calibration") when a calibration should be performed.
If needed it will say "Calibration is old".
This property is a drop-down containing a button called "Ok". If the button is clicked, the sys-
tem will perform a calibration on the camera.
Even if the property doesn't say "Calibration is old" it is still possible to do a calibration.
After pressing the button the system will guide the user regarding further action.
It is advisable to make a black reference calibration when the camera has reached operating
temperature, even though the property doesn't say "Calibration is old".
Note: To give the best result the calibration must be done in preview mode, where live
images from the camera are presented in DynamicStudio.
Note: It is possible to have DynamicStudio do calibration on all cameras connected to the
same synchronizer in one operation. In this case DynamicStudio will ask if calibration is to be
performed on all or only the selected camera.
Double  frame  mode
There are two different procedures that can be folloewed. Which to choose depends on the
exposure time selected.
Exposure time as large as it can be for the given trigger rate:
This procedure is the most simple one. Here only in camera calibration is required just as
described fir single frame.
Make sure that "Raw Image" "Flat field Correction", "Noise background removal" and "Hot
Pixel Correction" is disabled since this only have affect using "Black reference calibration" and
"Flat field correction", calibration methods that are implemented for handling exposure time
different from the largest possible for the given trigger rate.
Other Exposure time :
When working in double frame mode the calibration performed by the camera is not enough.
The in camera calibration is designed only to work in single frame mode, and when invest-
igating the images acquired after having performed an in camera calibraion, you will notice
that the intensity of the second frame is dark compared to the first frame.
To overcome this the in camera calibration has to be "disabled", or have as little influence as
possible on the images acquired. Images correction is in this case handle in the software of
DynamicStudio.
Steps by step procedure:
1.   Set the camera in Single frame mode
2.   Click Free run
3.   Set the exposure time to minimum (entering a 0, and let the system validate this figure
resulting in the minimum exposure time for the camera).
4.   Set the Trigger rate to the maximum (entering ie. 99999, and let the system validate this
figure resulting in the maximum trigger rate for the camera)
Note:?If you have a laser or another devices connected to the same synchronizer as the
camera, that can not run as fast as the camera, you must disconnect this device from the
synchronizer in Synchronization cables diagram, right-click the device(s) in Synchronization
cables diagram and select "Disconnect")
5.   Make sure that "Raw image" is disabled
6.   Perform in Camera Calibration. the way this calibration is performed will to make sure that
the calibration have as little effect on the images acquired later by the camera. This is
necessary in order to have the software calibration work correct.
7.   Select double frame mode
8.   Set the exposure time to correct (needed) value
9.   Enable "Raw Image", making sure the that the images received from the camera, is as raw
as possible. (Even though you enable"Raw image" the in camera calibration performed in
step 6 still has some small influence, and a very high if step before step 6 is not performed
correctly)
10.   Perform Black Reference Calibration. The camera will close the internal shutter, but for
cameras that does not have this internal shutter you will have to put a cap in front of the
lens.
Let this calibration run until the image shown is stable.
11.   Optional for fine tuning calibration, Flat Field Correction Calibration can be performed. Put
a white balancing filter in front of the lens and extra light to get the Color Map Histogram
in the middle of the range.
The "In Software calibraion" is now done. You have full control over what part of the in Soft-
ware calibration should be enabled. To control this the following parameters are available:
Noise background removal
Enable or disable the use of noise background removal
Black reference Offset Value
When the background removal is performed an offset value is added. This value can be adjus-
ted here.
Flat Field Correction
Enables Flat Field Correction
Hot Pixel Correction
During Black reference calibration and Flat field calibration hot or defect pixels might have
been found. This pixels can be set to a value based on its neighbor pixel values.
PAGE | 274
PAGE | 275
Circular acquisition (event triggering)
The Phantom cameras can be triggered by an event, this means that it is possible to start the
acquisition and the camera will then acquire in a circular fashion and when an external trigger
is applied to the camera it will start recording post images. When done it will stop the acquis-
ition and the images can be browsed from the "Acquired Data" window. That images are
arranged so pre images will have a negative index and the post images will have positive index
("Examining Acquired Data after a Preview" on page?282).
To run the camera in circular mode just select it from the "Trigger mode" drop-down list in the
camera properties.
The input used on the camera is "Trigger".
Running circularly is not possible when recording to removable image buffer.
Electronic Shutter
When Electronic shutter is disabled the electronic shutter in the camera will be disabled, which
changes two features in the camera behavior :
l  Make the interframe time (the time between the two exposures in double frame mode)
shorter.
The reason why the interframe time is shortened is that the camera does not spend time
controlling the electronic shutter between two pulses.
l  The camera exposes quasi continuously.
This can be a disadvantage if acquisition is to be performed in areas with much ambient
light, especially if the double frame mode with low trigger rate. Here one frame will be
brighter than the other one.
This is illustrated in the following figures. Note that on those figures, only the camera trig-
gering is illustrated, for more specific details on Synchronization the reader is referred to
"Synchronization" on page?333.
The first one represents the timing diagram of a camera running with "Electronic Shutter"
Enabled. As it can be seen both images have same exposure time but the interframe time is
quite long.
The second figure represent the timing of the same camera running with "Electronic Shutter"
Disabled. As it can be seen, the interframe time as been reduced so that shorter "Time
between pluses" can be reached. It can also be seen that the exposure time of the second
frame is now longer than the first one.
To get to the minimum Time between pulses for the camera in Double frame mode "Elec-
tronic Shutter" has to be Disabled.
The default timing parameters for the camera in Double frame mode (Delay to open) is for
"Electronic Shutter" Enabled. If the camera is set to run in "Electronic Shutter" Disabled, the
timing parameters has to be changed. As illustrated in the following figure, the "Delay to
Open" has to be adjusted so that the laser lights flash into their corresponding frame.
The following will describe a way to find the correct value for Delay to open.
1.   Set the system to Double frame mode.
2.   Be sure that the Activation delay for the laser is correctly entered (Delay from rising edge
of the Q-Switch trigger signal to light out of the laser). The correct activation delay for the
laser can be found in the manual for the laser.
The activation delay for Flash pumped laser as are typically around 150-200 nanoseconds.
For a Diode pumped laser the activation delay can be several microseconds.
No cavity in a laser is exactly matching any other cavity therefore the activation delay for
one cavity will be different from the other one. ("Synchronization" on page?333).
3.   Set the trigger frequency of the system to the value at which you will do you acquisitions
4.   Set Time between pulses to 10 microseconds.
5.   Physically disconnect sync signal to cavity 2(and connect cavity 1.)
6.   Do an acquisition of 10 images
7.   Examine the acquired images. Check that the light from cavity 1 is only seen in the first
frame.
PAGE | 276
PAGE | 277
If the light is not to be seen in the first frame decrease Delay to open. Go to step 6.
8.   Physically disconnect sync signal to cavity 1(and connect cavity 2.)
9.   Do an acquisition of 10 images
10.   Examine the acquired images. Check that the light from cavity 2 is only seen in the second
frame.
If the light is not seen in frame 2 increase Delay to open. go to 9.
11.   Decrease Time between pulses and go to step 5.
At one point it will no longer be possible to adjust Delay to open so that there will be light on
both frames. At this point you have found the minimum Time between pulses for the camera
and also the correct Delay to open value.
As you reduce Time between pulses, the adjustments done do Delay to Open should be in 0.1
microseconds steps.
The following table indicates an estimated value of the Delay to Open to enter the software in
order to reach the minimum Time between pulses. Note that this value may be subjected to
changes, depended on the firmware of the camera, actual values of the laser's activation
delay...
Model
Delay  to Open
"Electronic Shutter"
Enabled
Delay  to Open
"Electronic Shutter" Dis-
abled
M310    1.0 μs                                      1.6 μs
M340    2.5 μs                                      1.6 μs
Time between pulses
The reset time of the camera (time that it takes to get ready to take the next image) gets
shorter if the image size gets smaller. This means that it is possible to set a shorter time
between pulses if the resolution is smaller.
Internal image buffer
The image buffer of SpeedSense cameras can be split up(formatted) in any desired number of
partitions. This is done by entering the corresponding value in the device properties. By using
multiple partitions it is possible to use the internal buffer memory as a remote storage before
retrieving the data to the PC through the network.
In order to use of this feature, the database handling options must be set accordingly by
unchecking "Save acquired data to main database" and checking "Do quick conversion" (in
Tools/Options/Acquisition).
Example of use :
l  Enter the number of desired partitions  (in that case 2)
The maximum number of images that can be acquired in one run is displayed in the device
property. It corresponds to the maximum number of images that the camera can store
divided by the number of partitions.
2 partitions appear in the properties, and the first one is indicated as "Active". This is the
one that is going to be used first for the acquisition.
The second partition is indicated as "Free".
l  Acquire the first run
l  Save in the data base
The data is remotely stored as indicated by the blue globe on the data ("Distributed Data-
base" on page?417).
The first partition changes status and is now indicated as "In Use", meaning that it contains
data. The second partition takes over and becomes active.
PAGE | 278
PAGE | 279
l  Acquire another run
As a portion of the memory is still free, it is possible to run another acquisition without hav-
ing to transfer the first acquisition to the PC.
Then both partitions are "In Use" and pop-up message will announce that no more acquis-
ition can be done before a memory partition is freed.
l  Free some memory  space
In order to acquire some more data, a partition must be freed.
There are 5 ways to free partitions :
- The camera can be turned of and back on. All images stored in the camera are lost.
- Number Of Partition is changed.All images stored in the camera are lost.
- Delete one of the quick converted image ensembles stored in the database.
- Move the partition contents to a Storage Device (CineMag or CineFlash) if it exists
("CineMag, CineFlash Storage devices" below).
- Collecting the remote content to the main data base as indicated in the following figure.
This can be very useful to save transfer time when several experiments has to be repeated.
CineMag, CineFlash Storage devices
If Storage device, such as a CineMag or a CineFlash is mounted on the camera, it will auto-
matically be recognized and appear in the Devices tree as in the following figure.
Higlighting the "SpeedSense Storage Device" will display its properties in the dedicated menu
:
PAGE | 280
PAGE | 281
l  Number of stored Recording
This parameter describes how many recording is stored on the Storage device.
The number of recordings can correspond to a number of ensembles in the active Dynam-
icStudio database. It can also represent ensembles linked to another database, that is not
currently in use.
l  Storage Size
Specifies the size of the attached Storage device. In this case a 60GB CineFlash is attached.
l  Storage Used
Specifies how much of the Storage device is used by the stored recordings.
l  Protected
Some Storage devices have the possibility to make the storage device Read only, pro-
tecting the data on the device. If the write protection is set, it will not be possible to write
data to the storage or to erase the storage contents.
l  Erase storage
When the storage device is full and all data is collected to the Database(s) you will have to
erase the storage device to get rid of the stored recordings. This is done by clicking “Erase
Storage…” and then click OK on the button that appears.
It is not possible to erase individual recordings stored on the Storage device.
Removable image buffer
The removable image buffer is a separate image storage module that can be attached to
some SpeedSense cameras. This opens up for much longer acquisitions than with just the cam-
era internal memory. To acquire images to the removable image buffer change the 'Image
buffer' property to 'Removable image buffer'. The Image count will update to the approx-
imate number of images that can be recorded.
An important note is that when recording images to the removable image buffer the max-
imum trigger rate is lower than when recording to internal memory.
The maximum trigger rate at full resolution are as follows:
l  SpeedSense 9060, 9070, 9072 and 9090: 700 fps.
l  SpeedSense 9080: 175 fps.
The trigger rate can be increased if the resolution is lowered.
The removable image buffer can only hold one recording at a time. This means that on every
start of acquisition the memory will be cleared if necessary. It is possible to clear the memory
beforehand using the 'Clear image buffer' property.
Images in the removable image buffer are always stored with a pixel depth of 10 bit no mat-
ter what pixel depth was specified by the user.
Due to the way the camera records images to the removable image buffer it is not possible to
make a normal acquisition by pressing the Acquire button. It is only possible do a free run and
a preview. The images shown during double frame preview is not sorted meaning frame 2 can
be shown as frame 1 and vice versa but the images are stored correctly and can be viewed cor-
rectly afterwards.
Examining Acquired Data after a Preview
When doing preview the acquisition is done synchronized to the laser. The camera is set to
use the internal image buffer as a circular buffer. When done Acquired Data will look like
below:
Images are actually acquired, but the frame index will have negative indexes. Below, the
range -3500 to -3000 is selected to be saved to the database:
Use camera Timestamp
It is possible to get image timestamp information from the camera, instead of the syn-
chronizer´s timestamp. Some Synchronizers does not provide hardware timestamp. In this
case it is possible to acquire an accurate timestamp information from the camera saved with
each image/double image. This is done by enabling “Use camera Timestamp” in properties for
the Image buffer.
Camera Timestamp adjust
Each camera has it's own internal clock. If more cameras are used with “Use camera
Timestamp” enabled, the camera internal clocks has to run synchronized. This is done by
choosing one camera as "Master" and connect the IRIG-out from this camera to each of the
PAGE | 282
PAGE | 283
other cameras IRIG-in. This has to be done in both physically and also in the Synchronization
cables diagram.
Depending on the cable length and electronics in the camera, the IRIG signal from the
"Master" will or can be delayed some nanoseconds. It is possible to adjust for this by entering
a value in "Camera Timestamp Adjustment" parameter found in parameters for the Image buf-
fer.
Sensor mode
Some cameras can run in different Sensor modes. Changing the Sensor mode is done by chan-
ging the parameter Sensor mode in the group Other parameters. The available mode is listed
in the drop down list.
When changing the sensor mode the DynamicStudio needs to be restarted. DynamicStudio
will automatically ask you for that restart after the change is applied.
Trouble shooting
Image quality issues
Due to the electronic shutter, if the trigger rate period is much longer than the exposure
time you may collect a little bit of light during that non-exposure period resulting in a brighter
image than set by the exposure time.
Problems finding camera?
In the case that the Phantom detector can't find the camera, it is most likely because the cam-
era needs a .stg file. This file is a setup file that holds the camera settings.
In the properties of the detector it is possible to set a path to this file. The path must be the
execution path of DynamicStudio meaning the location from where DynamicStudio was
executed. In the case of Agents the path must be the location from where the Agent was
executed.
Wrong light pulse in double frame
After an acquisition in double frame mode, the images can be arranged so that light 1 is in
frame 2 and light 2 is in frame 1. This can be corrected by shifting the images by 1 from the
device properties of the image buffer.
After changing Image shift, examine a new image set from the image buffer, the change of
Image shift only has an effect on new images examined from the buffer.
Corrupt images
The stg structure in the camera can sometimes become corrupt. If this is the case the images
might look corrupt or the settings of the camera does not mach the settings of Dynam-
icStudio.
To fix this you must have the Phantom software installed and restore the nonvolatile memory
settings of the camera with the factory .stg (located on the CD that came with the camera).
With more than one camera extra care has to be taken to insure that it is the correct .stg file
that is used to restore the camera with.
For further information on how to reset the camera to factory settings please se the Phantom
documentation supplied on the CD that came with the camera.
Light from laser 1 seen on both frames in Double frame mode
This issue is only seen on cameras that supports burst mode.
The cause of this issue can be that "Delay to close" is set too low. When set to low, it will look
like light from cavity 1 on the laser will show on frame 2 as well.
What is happening is that Dynamic Studio thinks that the camera is running in double frame
mode and therefore also capture 2 images on each sync signal send to the f-sync input, but in
fact only one image is captured.
To solve the issue you must increase the Delay to close. First try with a large delay to open, ex.
10 microseconds, and verify that the camera is now really capturing two images on each sync
signal. Then decrease delay to open until the issue is seen again, now Delay to close is at the
limit. Just increase the delay to open with 100 nanoseconds and the issue is solved.
14.5.26 PCO?Dimax cameras
The PCO dimax cameras are based on high speed CMOS?sensor.
The cameras support 12-bit resolution only.
ROI vs frame rate
PAGE | 284
PAGE | 285
ROI                                               Frame rate Single farme             Frame rate Double farme
2000 x 2000                               2277                                               1139
1400 x 1050                               5475                                               2743
1280 x 720                                 8219                                               3975
1000 x 1000                               7033                                               3524
800 x 600                                    23823                                             3435
640 x 480                                    17951                                             9017
320 x 200                                    46523                                             23413
Camera settings
The following settings can be adjusted for the camera.
Fast Timing mode
Sets the camera fast timing mode for a dimax. To increase the possible exposure time with
high frame rates it is possible to enable the 'Fast Timing' mode. This means that the maximum
possible exposure time can be longer than in normal mode, while getting stronger offset
drops. In case, especially in PIV applications, image quality is less important, but exposure
time is, this mode reduces the gap between exposure end and start of the next exposure will
be reduced to it's minimum
Noise filter
By enabling the Noise Filter the image data will be post-processed in order to reduce the
noise. This is done by the camera.
Image buffer settings
Number of images
When setting number of images different from "Default" you can here decide how many
images this camera is to acquired during acquisition
Use Camera Timestamp
When enabled the timestamp is collected from the camera instead of from the synchronizer.
14.6 Synchronizers
14.6.1   High  Performance Synchronizer and Performance Synchronizer
The two versions of the Synchronizer listed below have the same functionality described
below, it is only the time resolution that differs. Please read the "Hardware manual" delivered
with the Synchronizer before proceeding.
Note: To setup the synchronizer for e.g. external trigger of Burst start or to trigger individual
Image acquisitions it is recommended to use the Synchronizer Setup Wizard. Start the Wizard
by right-click on the Synchronizer symbol in the Synchronization Cables dialog window or via
the Device properties. Once the Wizard is started the user is guided to setup different acquis-
ition strategies, trigger options, cyclic synchronization using an encoder input, window trig-
gering and much more.
High Performance Synchronizer:
With a time resolution <= 1ns this device can handle all applications e.g. gating of intensifiers
to image very high speed phenomena or to capture weak and short fluorescence signals. Min.
pulse width<= 6ns
Performance Synchronizer:
This device can handle nearly all synchronization needs for e.g. PIV and VV. Time resolution <=
8ns. Min. pulse width<= 48ns.
PAGE | 286
PAGE | 287
Overview of the inputs  and output connections  to the synchronizer.
USB and Ethernet for communication:
The Synchronizer is equipped with a USB 2.0 port and a 100Mbit Ethernet port. Both of these
can be used for communication with the Synchronizer. With Ethernet communication, the Syn-
chronizer can be placed far away from the control PC.
Safety  Circuit:
The Synchronizer has a connector called Safety Circuit to be used e.g. as an interlock when con-
nected to the lab door lock. For increased safety, the Synchronizer holds a watchdog that
must be triggered regularly. Breaking this circuit will cause the Synchronizer to set all outputs
to a predefined, programmed, passive state.
Synchronization Outputs  (Sync Outputs):
There are thirty-two Sync Outputs on the Synchronizer. Sixteen of these are placed in the
front of the Synchronizer as individual BNC connectors. Another sixteen outputs are placed
on the back of the Synchronizer in one multi-connector. For the sixteen outputs on the front,
it is possible to set each output to either High Impedance or 50 Ohm Impedance. The sixteen
outputs on the front of the Synchronizer can be calibrated to make sure that all output signals
are able to switch from a low state to a high state simultaneously with a precision of better
than 250ps.
D/A Outputs:
The synchronizer has four D/A Outputs (Voltage outputs) to activate auxiliary equipment
needed for the application. The Voltage Output range is +/- 10V with a resolution of 4.88mV.
Maximum rate of change for the voltage of each individual output is 10kHz.
Control outputs:
The signals from these outputs can be used to indicate the status of an ongoing acquisition
sequence, for example; Start, Ready, Wait or Stop. The desired status of the outputs are set in
the software. The four Control Outputs are TTL High Impedance outputs (High >=4V; Low
<=0.8V).
Analog inputs:
The synchronizer has two Analog Inputs. The Analog Inputs share a common A/D converter
which is able to acquire 1M samples per seconds. The acquisition of analog values are sequen-
tially divided between the channels. This leads to a maximum acquisition rate of 500 kHz per
channel if used simultaneously.
Trigger Inputs:
There are four Trigger Inputs on the Synchronizer. The Trigger Inputs are TTL inputs with a
high impedance. It is possible to set the polarity of the Trigger Inputs to activate on rising or
falling edge. For each input, it is possible to setup pre-scaling. The pre-scaling of the Trigger
Inputs makes it possible to skip trigger pulses for improved flexibility.
Enable Inputs:
The two Enable Inputs can be used in code sequences defining, for example, an acquisition
sequence where a status signal is needed to change the execution of the program to jump to
another part of the program. It is possible to set the polarity of the Enable Inputs to Activate
High or Active Low.
Triggering options:
? Burst triggering (one trigger pulse activates the acquisition of e.g. 1000 images per burst)
? Ability to run the cameras at a different trigger rate than the laser
? Ability to do window triggering (phase-locked triggering)
? Ability to send single pulses, such as a start signal, to external devices
? Ability to warm up the laser with a known number of flash pulses and Q-switch pulses before
starting image acquisition
Options:
Light detector input: High accuracy  laser pulse time-stamping (Optional):
For laser pulse timing, the Synchronizer has a special feature which enables it to measure the
precise Q-Switch activation delays with a resolution of 1ns. The light from the laser pulse is col-
lected via the Light Detector Input - a fiber optic input. The Q-Switch activation delay is meas-
ured from the time the Q-Switch on the laser is activated (triggered) until the light from the
laser is detected. This feature allows for a high-accuracy laser (and image exposure) pulse
time-stamping.
PAGE | 288
PAGE | 289
Encoder Input for cyclic synchronization (Optional):
The Encoder Input is used in applications when synchronization to cyclic phenomena is
required such as measurements with engines, propellers or turbomachinery.
Advanced features:
Complex output timing pulse patterns:
The Synchronizer is capable of executing complex output timing pulse patterns at greatly vary-
ing update rates; ranging from nanoseconds to years, with a constant setting accuracy.
Installing the synchronizer
The procedure for installing the Synchronizer via USB is;
The driver for the Synchronizer will be installed during the installation of Dynamic Studio 6.5
or later versions. After installation connect the Synchronizer to the PC .
In the Device Manager the "Dantec Dynamics Synchronizer" should be found and it can be
seen if the driver is installed and if the device is working properly .
If the driver is removed for any reasons it can be reinstalled by Right Click on the Syn-
chronizer device and select Update Driver Software. Locate the Driver on the Dynamic Studio
MEDIA and select install.
Be sure to go online with the system in order to check if DynamicStudio has properly detected
the Synchronizer. For more information about device detection, please read the Getting Star-
ted guide. When the installation has completed correctly the Synchronizer should be shown as
displayed in the DS Devices list, see below:
If the Synchronizer is disconnected or powered-off the device Icon will be shown as "Disabled"
in the list
.
The procedure for connecting via Ethernet is;
To connect to a Synchronizer via Ethernet you first have to add a Synchronizer Ethernet
Detector in DynamicStudio Devices list as shown below.
Having installed this detector DynamicStudio will try to detect all synchronizer connected to
the LAN.
If you connect the Synchronizer to a LAN on which a DHCP server is present the Synchronizer
will be given an IP-address from the DHCP. In this way you do not manually need to setup IP-
address information.
( DHCP, = Dynamic Host Configuration Protocol, is a protocol used to provide quick, automatic,
and central management for the distribution of IP-addresses within a network.)
If you do not have a LAN with a DHCP server the Synchronizer IP settings need to done manu-
ally.
.
PAGE | 290
PAGE | 291
Setting up synchronization tasks  in DynamicStudio and connecting to equipment
Once the Synchronizer is active and devices( such as a Laser and a Camera) have been installed
the "Synchronization Cables" can be displayed. The devices can be connected by drawing
cables between the Synchronizer and device or by using default connections by right clicking
on the device symbol.
In DynamicStudio v7.1 a Software trigger input was added to be used for new features (Active
Target).
Once the cabling between the Synchronizer and the installed devices is completed in the soft-
ware the LEDs on the Synchronizer will light up the corresponding BNC-connectors.
With a laser and camera connected to the synchronizer the desired acquisition parameters
can be set in the System Control. In this example 5 Bursts with 50 images in each are acquired
at 10 Hz.
The system is running in Double Frame Mode and the Time between pulses is 300 μs.
By right clicking on the Synchronizer ( or selecting in the View drop-down list in the Tool Bar)
the Timing Diagram is displayed giving a visual presentation of the time between laser pulses
and the camera (double frames) exposure timings.
PAGE | 292
PAGE | 293
Control of the synchronizer
The synchronizer will be controlled through the standard interface for controlling devices in
DynamicStudio acquisition system.
The synchronizer will share the main parameters know from older DantecDynamic Syn-
chronizers with a few additional parameters e.g. related to the introducing of "Burst Mode"'.
System control
With the new Synchronizers (DS 6.5 and later) the “Number of Bursts" is shown in the System
Control entry. This entry will only be visible if a “Synchronizer” (2018 and later) is connected.
With the “Number of Bursts" = 1 the system will acquire all Images with the same trigger set-
tings and save in one ensemble in the Database.
When acquiring more than one Burst it is possible to save each Burst in separate Burst
ensembles or combined in one ensemble. The Burst Number (Index) is saved as part of the
Record Properties for the saved Images.
Overview of the synchronizer device properties:
All of the settings below can be accessed from the Device Properties panel. Click on the “Syn-
chronizer” icon on the “Devices” panel and open the “Device Properties” panel to change set-
tings.
Note that the DS User Interface Level can be set to Standard, Advanced or Expert under
Options in the DS tool-bar. The standard setting do not show the Laser Warm Up section. Addi-
tional settings are also shown when a specific trigger mode is selected to be active.
Note that the Encoder Setup section is only relevant if the optional "Encoder Input for cyclic
synchronization" is ordered with the Synchronizer.Encoder triggeringis described further
down.
PAGE | 294
PAGE | 295
Trigger mode  settings
Start
Indicates how to start each burst. Can be:
Automatically
Start on User action
Start on External trig
Start after time
Encoder position
Mode
Indicates how to trigger each images in the burst. Can be:
Internal
Trigger frequency is defined in “System Control Trigger rate” or by “Trigger
mode setup Trigger rate”
External
Trigger frequency is defined by the external trigger rate. The maximum trig-
ger rate depends on the connected devices.
External clock
Trigger frequency is defined by the external trigger rate. The maximum trig-
ger rate depends on the connected devices.
Window triggering
This mode is for laser that only runs at a fixed rate, but the user wants to trig-
ger the image acquisition by use of en external trigger pulse.
Trigger rate for the laser is defined in “System Control Trigger rate” or by
“Trigger mode setup Trigger rate”. When an external trigger falls in to a predefined window
time an images is captured.
Encoder triggering
This mode is described further down.
Trigger Rate
(Only shown in Mode: Internal)
Indicates the trigger rate of images in the burst
Use every  N’th pulse, N=
(Only shown in Mode: External Clock)
Divides the trigger rate from external equipment with the number N.
Trigger delay
(Only shown in Mode: External and External Clock)
Start after time
(only shown if mode is set to “Start after time”)
Start Time stamping on first image
(Only visible in Mode: External, External Clock, Window Triggering, Encoder)
Indicates when the timer for time stamping is started.
Yes: First image acquired will have time stamp 0
No: First image will have a time that depends on when the acquisition was initiated
Maximum Trigger Rate
(Only shown in Mode: External)
Specifies the maximum trigger rate the Synchronizer must accept
Trigger Window time
(Only visible in Mode Window triggering)
Specifies the size of the window in seconds. The trigger window size depends on the
laser. Lasers that require a speady frequence to run always accepts a certain jitter in the trig-
ger frequency, it is the size of this jitter that the laser accepts that should be entered.
Trigger Acq-devices  at Trigger rate/’N’
N specifies how often Cameras or AD sampling is to be triggered relative to the selected trig-
ger frequency.
PAGE | 296
PAGE | 297
This does not change how often the Q-Switches are to be triggered
Trigger Q-Switches  at Trigger rate/’N’
N specifies how often Q-switches are to be triggered relative to the selected trigger fre-
quency.
The rate for triggering cameras and AD sampling will be reduced similarly.
Time Between Systems
(only shown if one synchronizer is triggering another)
Specifies the time between two synchronizers if one synchronizer is triggering
another. Time difference is specified between T0 on the master Synchronizer and T0 on this
synchronizer.
Image  triggering  settings
(Only visible in Mode: External, External Clock, Window Triggering and Encoder triggering)
Image Trigger signal
(Only visible in Mode: External, External Clock, Window Triggering)
Can be set to:
Trigger input 1
Trigger input 2
Trigger input 3
Trigger input 4
Image Trigger signal polarity
(Only visible in Mode: External, External Clock, Window Triggering, Encoder and if Image trig-
ger enable signal is set to other than None)
Positive edge
Negative edge
Image Trigger Enable signal
(Only visible in Mode: External, External Clock, Window Triggering and Encoder triggering)
Can be set to:
None
Enable input 1
Enable input 2
Image Trigger Enable signal polarity
(Only visible in Mode: External, External Clock, Window Triggering, Encoder triggering and if
Image trigger enable signal is set to other than None)
Positive state
Negative state
Variable  Timing
Link to Online Video, can only be displayed in online HTML5 help or in Web Help systems
PAGE | 298
PAGE | 299
To adapt to high gradient flow velocity changes, it is possible to change the Time Between
Pulses (DeltaT) during an acquisition in Double Frame (DF) mode. To further optimize the
acquisition in DF mode (and use of camera memory) also the Trigger Rate can be changed.
When using Single Frame (SF) mode (or for Devices running in SF mode) only the entered Trig-
ger Rate is used (“Time between pulses” will be ignored).
Acquisition parameters in Double Frame mode(left) and Single Frame mode (right)
Variable Timing settings  in the Synchronizer setup wizard
By enable Variable timing in the Synchronizer setup wizard all parameters that are set to “Use
default” will use “Time between pulses” and “Trigger rate” specified in a user created list of
separate ranges of images. Each image range is defined by the Start Image Index and Num-
ber of Images. Up to 50 image ranges can be predefined in the list. In the figure below an
example of this is shown.
List of image range - for each image range in the list the Time Between Pulses and Trigger
Rate can be predefined to e.g. optimize the image acquisition to match an accelerating or
pulsating flow.
Total duration ? the time in seconds required to acquire the total number of images in the list
Update ‘Number of images’ with sum - if this box is checked (default) the sum of all images in
the list are displayed as the ‘Number of images” in the box further down and in the System
Control.
Preview index ? the Index (in the list) where the acquisition settings used when acquiring
images using Preview (in System Control) with Variable Timing enabled.
Variable Timing dialog in Synchronizer setup wizard.
Select a line by left-click on the line. Delete or Insert lines in the list by right-click on the line marker
to left.
System Control parameters  when Variable Timing is  enabled
When Variable Timing is enabled the values seen in the System Control for Time between
pulses and Trigger rate are overruled by the settings in the Variable Timing dialog (in the Wiz-
ard). All parameters that are set to “Use default” will use “Time between pulses” and “Trigger
rate” according to the list created in the Synchronizer setup wizard.
If the ‘Update Number of images with sum’ box is checked in the Variable Timing dialog (in the
Wizard) the sum of all images in the predefined ‘Variable Timing list’ are displayed in the Sys-
tem Control.
Note: If the Number of images in the System Control is larger than the sum of all images in
the predefined ranges of images the acquisition will continue with the acquisition settings in
the last predefined range of images.
Note: When Variable Timing is enabled, and Preview is used for acquiring images the acquis-
ition settings are determined by the settings used for the Image range matching the Preview
index (see above).
Variable Timing dialog in Synchronizer setup wizard and the connection to System Control set-
tings.
Dialog showing the used acquisition parameters in Preview mode with Variable Timing enabled
in the wizard.
Variable Timing settings  in the Synchronizer Device properties
The Variable Timing settings can also be found in the Synchronizer Device Properties, see the
figure below.
PAGE | 300
PAGE | 301
Variable Timing settings in Synchronizer Device Properties.
Burst triggering  settings
With the “Number of Bursts" > 1 the Burst Triggering settings defines the triggering and
image acquisition.
Minimum Time between Burst start
(Only shown if number of burst is 2 or more)
Specifies the minimum time between start of each burst.
When changing number images or trigger rate this figure will be set to the time it takes to
acquire the number of images. If in one of the trigger mode the Minimum time between
bursts will be set to 1/(maximum trigger rate for the system).
Burst Trigger signal
(Only visible in start mode: External, External Clock, Window Triggering)
Can be set to:
Trigger input 1
Trigger input 2
Trigger input 3
Trigger input 4
Burst Trigger signal polarity
(Only visible in Mode: External, External Clock, Window Triggering, if Image trigger enable sig-
nal is set to other than None)
Positive edge
Negative edge
Burst Trigger Enable signal
(Only visible in Mode: External, External Clock, Window Triggering and Encoder position)
Can be set to:
None
Enable input 1
Enable input 2
Burst Trigger Enable signal polarity
(Only visible in Mode: External, External Clock, Window Triggering, Encoder position and if
Image trigger enable signal is set to other than None)
Positive state
Negative state
Laser warm  up
Warm up unit
Defines 'whether to use Time' (in seconds) or 'Pulses' for the parameters "Flash lamp warm
up" and "Q-Switch warm up".
PAGE | 302
PAGE | 303
Flash lamp warm up
Number of pulses or seconds that the flash should be pulsed before starting the acquisition or
starting pulsing the Q-Switch for warm up.
Q-Switch warm up
Number of pulses or seconds that the Q-Switch should be pulsed before starting the acquis-
ition.
Settings  active  when  more  than  one  Synchronizer is  connected
Pulse mode
only shown if more Synchronizers are present and if number of reference times is set to 1
Indicate if the system is in Double Frame or Single frame mode. In Double Frame mode, the
Synchronizer will deliver double pulses I.e. pulse both cavities in a PIV?laser.
Time between Systems
Only visible if more Synchronizers are present in the acquisition system and if number of ref-
erence times is set to 1
Analog  output channel  1-  4
Use Analog output 1,2,3,or 4
Enabled
Specifies if the analogue output is in use.
Activation time
Time relative to T0, Example 500 μs
Voltage state change train
Here the state time train is specified; “time; time;…; time. Example: “10000;
20000; 10000 μs”
Voltages
Here the voltages for each state is specified; “Voltage; Voltage;…; Voltage”.
Example “5; 10; 0 V”
Used during laser warm up
true/false
Analog  input
The two analog input channels need to be enabled in the Device list to be activated ( see fig-
ure below).?Once enabled they can be selected in the acquisition dialog.
Analog input 1  or 2
Sample Frequency
(only visible if either Analogue input 1 or 2 is in enabled)
Specifies the sample rate, samples/second, for the analogue sampling.
Note:The sample Frequency must be the same for both channels .
Samples per channel
PAGE | 304
PAGE | 305
Samples per channel for each corresponding acquired image
Activation time
The start time relative to T0
Reference  times
Reference times  are seen as  independent sub systems  in where each reference time
has  its  own Delta T (=  "Time between pulses" in the System Control
Reference Times
Indicate the number of Reference times. Maximum 7.
Reference times are seen as independent sub systems in where each reference time has its
own Delta T.
Reference Time 1  to 7
(only shown if more then one Reference Time is shown
Name
Name of the Reference time
Frame mode
Can be Single frame; Single frame double exposure or Double frame mode
Time between pulses
(Only shown if in Double frame mode)
Devices (…)
Lists the devices, which is part of this Reference Time. When clicking on one
(…) a dialog will be presented in where it is possible to select devices for this reference time
(see Example below).
Delay to previous Reference time
Specifies the time between the previous Reference Time T0 and current Refer-
ence Time T0.
Sync Outputs
For the sixteen outputs on the front of the synchronizer, it is possible to set each output to
either High Impedance or 50 Ohm Impedance.
Only the Sync Outputs in use will be displayed:
Connected to
Shows what the sync output is connected to.
Output Impedance
Can be set to High or 50 ohm.
PAGE | 306
PAGE | 307
Calibration delay
Shows the calibrated delay. The calibrated is 0 until a valid calibration has been doen
TrueTime
If the Synchronizer has the "TrueTime" add-on this section of the device properties will be vis-
ible.
TrueTime is based on measuring the time from a Q-Switch input is triggered until light is emit-
ting from the laser. These delays are measured in nanoseconds.
The synchronizer starts a stopwatch at the time it trigger’s the laser and is stops when the Syn-
chronizer Optical input receives a light pulse.
In order to correct Time Between Pulses in double frame (Time stamp in single frame
mode)?the synchronize system needs to be calibrated.
Corrected Dt =?Dt +(CalibratedDelay1 - T1) +(T2 - CalibratedDelay1)
Where
Dt is Time between pulses set in the system
CalibatedDelay1 is the calibrated time measured during Calibration for the first cavity in the
laser.
CalibatedDelay1 is the calibrated time measured during Calibration for the second cavity in
the laser.
Parameters are
Time Between Pulses  correction
Used to enable and disable the True Time functionality
Calibration
Used to enable calibration. After enabling calibration do a preview.
Time stamp information will now include measured delays. When enough data has been col-
lected an OK will appear in the Time stamp information.
Control outputs
Output 1  ?  4
Signal
Can be set to one of the following functions:
None
Stopped
Warming up laser
Warming up laser Flash
Warming up laser Q-Switch
Acquiring
Waiting for Burst trig
Waiting for Image trig
Exposing Image
Exposing Image 1
Exposing Image 2
PAGE | 308
PAGE | 309
Active state
Specifies if the signal should go High or Low when active.
Can be set to:
High
Low
Pulse width
Specifies how long the signal should be. Can be set to:
[Active while in signal state]
a user defined time in seconds. Example: 10μs
Synchronization timing adjustment related to delays in coax cables and electronics
For precise adjustment to delays related to coax cable length and electronics in a specific
setup the delays need to be measured using an oscilloscope.
Activation time
Device  information  and  version
Full name
Serial number
Manufacturer
Part number
Time  stamp  information
The time stamp information will hold the encoder position; this possibility was introduced
together with the introduction of the Cyclic Synchronizer.
Encoder Setup and encoder related acquisition settings  - cyclic synchronization
Encoder Setup
The Synchronizer Encoder Setup dialog window can be reached via the Device properties as
shown below. It can also been reach by using the Synchronizer Setup Wizard (recommended).
With an the Encoder Input for cyclic synchronization installed in the Synchronizer and an
encoder connected the following settings can be made in the Encoder Setup dialog window.
PAGE | 310
PAGE | 311
If the encoder is connected correctly and the 5V?Power output on encoder connector is
checked (if required, depends on encoder specifications) and the encoder shaft is rotated the
Current position in this dialog will keep updating. If this is working you can continue to the next
step below.
Test the Encoder signal stability  prior to measurements
An encoder sends X no of pulses during one revolution and one reset signal each completed
revolution. The encoder reset signal is used in the Synchronizer Encoder Setup dialog for the
user to test if noise is influencing the encoder signal. By opening the Synchronizer Encoder
Setup dialog, the user can Click-on Measure many times to check if the correct number of
counts are detected during one full rotation of the encoder. If too many or too few counts,
compared to the encoder specification (number to be entered in the Encoder Setup dialog
window), are detected during a rotation this is due to noise or possibly a too high rotation
speed.
Note: For the Synchronizer to calculate the current encoder position, the encoder first must
move a full rotation, passing the Reset position on the Encoder.
Note: Encoder signal debounce time A signal must be steady at-least the time specified here
otherwise it will be considered as a glitch.
In the example below (top part of the Synchronizer Encoder Setup dialog shown) the user has
Clicked Measure and 359 pulses are received. As 360 pulses are expected the Last error
count= -1, as one pulse has been lost during the Measure-revolution.
Encoder reset signal ?  used for correction of position if noise is  present
During measurements the encoder reset signal is used to, for each revolution, assure that
the present position is correct. When the reset signal is received the Synchronizer position will
always be correct (or corrected if noise is present).
During image acquisition where a cycle is a multiple of an encoder revolution (see the Virtual
Multi-turn encoder section below)the reset signal is used to correct the position if too many or
too few counts are detected during a rotation due to noise.
Example without noise: The encoder delivers 500 pulses per revolution (499 pulses + 1 reset
signal). The experiment cycle (360 degree) corresponds to 1000 counts (pulses). This is
achieved by setting the Gear ratio =2 in the Encoder Setup dialog. Here, the desired Encoder
Trigger position= 270 degree (= 750 counts).
If there is no noise the synchronizer will receive 500 pulses during the first revolution of the
encoder. After passing the Encoder reset position the synchronizer will continue to increment
the received pulses 499, 500 (reset), 501, 502… when 750 is reached (corresponding to the
Trigger position = 270 degree) the image acquisition will be triggered correctly.
Example with noise: Assume that noise is present during the first revolution of the encoder, in
this example 10 pulses are missing (lost due to noise). The synchronizer receives 490 pulses
during the first revolution of the encoder. After passing the Encoder reset position the syn-
chronizer will correct the position (490 is corrected to 500) and thereafter continue to incre-
ment the received pulses 490!, 500! (reset), 501, 502… when 750 is reached (corresponding
to the Trigger position = 270 degree) the image acquisition will be triggered correctly. Note: if
additional noise is encountered during the second revolution, from 500-750 pulses, of the
encoder this cannot be corrected for. It is therefore very important to investigate the influ-
ence of noise on the encoder signal as explained above.
Virtual Multi-turn encoder
The Virtual Multi-turn encoder part of the Synchronizer Encoder Setup dialog is shown below.
Here the Gear ratio can be set to a value different to 1 if one cycle of the experimental setup
will cover more (or less) than one revolution of the encoder. In the example below an
encoder with 360 pulses per one encoder revolution is used. A revolution of the cyclic process
(experimental device) is here specified to be 360 deg and that the 360 degrees corresponds
to 1024 counts (pulses) from the encoder. In this case the Gear ratio = 2.844 (calculated by the
system).
PAGE | 312
PAGE | 313
Fixed Position Window Triggering
How to setup Window Triggering video
Use the Wizard to setup the system for Window Triggering. This mode is designed to be used along
with those devices with fixed trigger rate (some lasers) and a warmup period of 10 minutes (or even
longer). The laser flash-lamps are kept warm, running at a fixed trigger rate, and an image acquisition
is only performed at the selected Encoder Trigger position.
In this trigger mode the Synchronizer sends signals at a predefined fixed rate to flash the
laser (only the laser flash-lamps are activated), keeping the laser warmed up. The Syn-
chronizer will continue to send signals to flash the laser even if no rotation of the Encoder
shaft takes place. When the Synchronizer starts to receive Encoder Reset and Increment sig-
nals (pulses), and the frequency and position starts matching the predefined trigger signal the
Synchronizer will try phase-locking to the external signal, until eventually the encoder rotation
now drives the flash of the laser signal. When phase-locked the Synchronizer starts to send
out sync signals to the laser Q-switch (generating a light pulse for measurement) and camera
to acquire images. If for some reason the external signal from the encoder disappears the
Synchronizer will take over and again driver a steady sync frequency to flash the laser (keep-
ing the laser warmed up) and the trigger signals to laser Q-Switch and camera will stop.
Trigger window width: Specifies the window size in which the system can be triggered from
the cyclic world (in time). This should be referred to the specifications of related devices. For
example, if a laser's trigger rate has to be 10±0.4 Hz, this window size should be (1000/9.6-
1000/10.4)=8 ms.
Note: This mode is designed to be used along with those devices with fixed trigger rate (some
lasers) and a warmup period of 10 minutes (or even longer).
Note: To make Window triggering work, the trigger rate of the laser must be slower than
what the maximum trigger rate of camera. In other words, the exposure period time of cam-
era must be lower than the period time (1/trigger rate) of the laser. (The period time of the
camera in double frame mode is of cause double as high as in single frame mode). If this rule
is broken the Synchronizer will stall during acquisition.
Calculating the actual position at which the acquisition will take place
Flash-lamp pumped lasers: When aquiring images for PIV (or for other laser based meas-
urements), the laser is fired and an image (or double image) is acquired. But the process of
triggering a laser to at a defined time generate a light pulse (and the camera to aquire
images) takes time.
The very first thing that is happening is the triggering of Flash lamp 1 in the laser. Usually the
Flash pumped laser has to be pumped around 200 microseconds before the Q-switch can be
triggered and then light is emitted from the laser. In this example this is the period that takes
the longest time to prepare for an acquisition. In these 200 microseconds the rotation con-
tinues, and therefore the actual acquisition will not take place at the position specified!
Example: To calculate the actual position at which the acquisition will take place lets try the fol-
lowing:
Rotation speed is 1000 RPM = 16.666 RPS ; Corresponding to a position speed of 6000 deg/s.
With the Flash to Q-switch delay of 200 micro seconds (which for this example is the time
needed before acquisition can take place) the rotation will continue:
200e-6 s * 6000 deg/s = 1.2 deg
Note: Have a look at the timing diagram for the Synchronizer to see what the actual delay
from an external Trigger to the laser is fired.
Encoder related acquisition settings
Trigger mode setup
Start
Indicates how to start each burst. For encoder triggering select:
Encoder triggering
Mode
PAGE | 314
PAGE | 315
The encoder position is used to generate a trigger signal to acquire an image, in other words
the rotation speed of the encoder gives the trigger frequency. The maximum trigger fre-
quency is defined by the connected devices.
Encoder position
Image triggering based on Encoder position -Synchronizer Setup Wizard
Start the Wizard by right-click on the Synchronizer symbol in the Synchronization Cables dialog
window or via the Device properties. It is recommended to use the Wizard!
Image triggering based on Encoder position -Device properties settings
Encoder trigger position
(Only shown in Mode (image trigger): Encoder)
Specifies at which angel to trigger an image
Image Encoder position shift
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
A number that defines an increment of the trigger position on each image. Can be pos-
itive or negative
Image Position shift correction
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information and only if
any Encoder position shift is entered)
[correction 1; correction 2; correction 3;…; correction N] (and start all over)
leave it blank if no correction is needed.
Burst triggering based on Encoder position - Synchronizer Setup Wizard
Start the Wizard by right-click on the Synchronizer symbol in the Synchronization Cables dialog
window or via the Device properties. It is recommended to use the Wizard!
Burst triggering based on Encoder position - Device properties settings
Note that the Wizard is the easiest way of setting up the triggering but all information is also
found in the Synchronizer Device properties. Below follow an explanation of the possible set-
tings.
Encoder trigger position
(Only shown in Start(Burst trigger): Encoder)
Specifies at which angel to trigger a burst
Burst Trigger Encoder Position
(Only visible if Start trigger mode is set to Encoder Position)
Specifies at which angel to trigger a Burst
Burst Encoder position shift
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
A number that defines an increment of the trigger position on each burst. Can be pos-
itive or negative
Burst Position shift correction
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information and only
if any Encoder position shift is entered)
[correction 1; correction 2; correction 3;…; correction N] (and start all over)
leave it blank if no correction is needed.
PAGE | 316
PAGE | 317
General encoder related settings
Include Encoder position in Time stamp information
(will be set to true if in trigger mode Encoder and in this case it can not be changed)
True or false,
Encoder full rotation increment counts
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
An integer number that represents a full rotation of the system at which the encoder
is connected.
It does not matter how many counts the connected encore has per rotation. It is the
number of counts of a full rotation of the system at which must be entered the Synchronizer
will handle this internally.
Encoder full rotation Range
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
A number that specifies a full rotation. Can be set to 6.28318530718, 360, 500 or
what ever is most useful.
(It is within this range the encoder trigger position is set.)
Encoder full rotation Unit
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
A string that specifies the unit: example “deg”
Current encoder position
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
Displays the current Encoder position within the Full range
Encoder position Reset
(Only shown in Mode: Encoder or Include Encoder position in Time stamp information)
Set the current Encoder position to a specific value. (Can be any value within the
Encoder Full rotation range)
14.6.2 Setting up the Synchronizer for PIV?measurements
The Synchronizer can be used to control a number of different devices, which all require TTL
input for the triggering. As the possibilities for connecting the box are numerous, only a few
are described here. Generally it is recommended to use the “Create Default Connections”
function in the “Synchronization Cables” panel of Dynamic Studio. Please read the getting star-
ted guide for more information.
Example>?Connecting a laser and a camera for PIV  measurements
For PIV?measurement a typical 2D-2C system is built around a flash lamp pumped lasers, such
as the DualPower 145-15 and a CCD camera such as the FlowSense EO 4M-32. The dual cavity
laser can deliver two laser pulses (separated in time with "Time between pulses", Dt) with a
repetition rate of 15 Hz. In double frame mode the FlowSense EO 4M-32 camera can run at
15 Hz ( max. 32 single frames per second). The diagram below shows an example of how to
connect the devices to the Synchronizer.
PAGE | 318
PAGE | 319
Acquiring PIV data in Burst Mode (5 Bursts with 10 images in each burst)
With a selected trigger rate of 10 Hz it will take 1 s to acquire 10 images. The minimum time
between burst start must therefore be > 1 s, in this example Time between burst start = 5000
ms = 5 s.
After the data has been acquired it is, in this example, saved in separate ensembles in the
Database. Alternatively, all images can be saved in a common ensemble.
14.6.3 Synchronization  setup
Internal synchronization, internal trigger
The default setting for running the system is with Internal synchronization and internal trig-
ger. The system will run at the trigger rate specified in the "System Control" panel. The sys-
tem will start when the user presses the acquire button.
Use the following settings in the “Device properties” panel, trigger mode setup:
PAGE | 320
PAGE | 321
External synchronization, internal trigger
In this setting the system will run with a trigger rate, which is given by an external syn-
chronization device through Trigger input 2 on the synchronizer.
Use the following settings in the “Device properties” panel for Trigger mode setup and Image
Triggering:
Internal synchronization, external trigger
Here the system starts when a trigger pulse is received on Trigger input 1. The system runs at
a frame rate specified in the “System Control” panel.
Use the following settings in the “Device properties” panel, trigger mode setup:
l
External synchronization, external start, gated synchronization
The system starts when a trigger signal is received on Trigger Input 1 (Burst Trigger Signal)
and runs at a rate given by the synchronization pulses on Trigger Input 2 ( Image Trigger Sig-
nal) . However, a signal high (+5 volts) must be present on Enable Input 1 ( Image Trigger
Enable Signal) in order for synchronization pulses to have an effect. If this signal high (gating)
does not exist, the system will stop running until the signal is high again
Use the following settings in the “Device properties” panel, trigger mode setup:
PAGE | 322
PAGE | 323
14.6.4   Running two cameras synchronized but at different trigger rate
Using individual Reference times  for each camera to allow different trigger rates  to be used
In this example two different cameras are used. One camera can run at max trigger rate of 30
Hz (30 fps, single frame mode) whereas the second camera can run at 300 Hz. To allow the
two cameras to be triggered synchronized in an acquisition it is possible to assign individual
Reference times, one for each camera, and thereafter reduce the trigger rate of the slowest
camera with a factor N.( It is also possible for each camera to use individual Delta T when run-
ning in Double frame mode.) In the figure below it is shown how this is done in the Device
properties for the Synchronizer. Here N=10 for the slowest camera whereas the fast camera
is triggered at default Trigger rate set in the System control dialog.
As the cameras will acquire different number of images it is necessary to set
the Image Buffer size of the slowest camera to the correct number of images. Here the Num-
ber of images in the System Control is set to 1000 and is valid for the camera running at 300
Hz. The second camera running at 30 Hz will acquire 100 images over the same time period.
Accordingly the Image Buffer is set to 100 images for this camera. If this is not done the
acquisition may not stop automatically and the user needs to press Stop in the System control
dialog. After this the acquired images can be saved to the database.
PAGE | 324
PAGE | 325
After acquisition the Acquired Data dialog shows the number of images for the two cameras
and the user can scroll and display the images on the screen. Note that the last synchronized
image pair is Image Index100 and Image Index 991 for the camera running at 30 and 300 Hz
respectively. Once the date is saved in the database the Images can be investigate and ana-
lyzed.
14.6.5 Additional settings
Safety  Circuit
l            The Synchronizer has a connector called Safety Circuit to be used e.g. as an inter-
lock when connected to the lab door lock.
For increased safety, the Synchronizer holds a watchdog that must be triggered
regularly. Breaking this circuit will cause the Synchronizer to set all outputs to a
predefined, programmed, passive state.
14.6.6 How  to use the TrueTime Add-on  and the Light Trap
Hardware:
The Light Trap is mounted directly on the Light Sheet Optics, with the screws already holding
the unit together, as shown in the left image below. The optical fiber is connected to the Light
Trap and to the Synchronizer Light detector input (see right image below).
Left) Light Trap with the optical fiber mounted on the Light Sheet Optics.
Right) The optical fiber should be connected to the Light detector input.
Note: The Synchronizer needs to be ordered with the Synchronizer TrueTime Option or
upgraded with this option. If your synchronizer doesn’t have this option installed the Light
detector input will be covered by a plastic cover as shown below.
PAGE | 326
PAGE | 327
DynamicStudio, TrueTime:
Once the fiber is connected to the synchronizer and the Light Sheet Optics you are ready to
start. In the TrueTime settings in the Synchronizer Device Properties the “Calibration” must
first be Enabled. Make sure the laser is warmed up before continuing with the calibration.
Calibration
Before you can start to use “Time Between Pulse Correction” a calibration needs to be per-
formed.
- Enable Calibration.
- Make sure the laser is warmed up. Use the feature “Warm-up Laser” activated in the Syn-
chronizer Device properties. Warm up the laser at least 100 pulses (lamp and Qswitch). A
warm-up phase will begin before the calibration starts.
- Run the laser and camera in Preview mode, let it run at least 100 pulses/images. Use the
acquisition settings you will used during aquisition.
If the laser for some reason is not flashing or e.g. the optical fiber is not connected the fol-
lowing message will be displayed.
If the calibration is performed correctly you will see that the “Total measured delay in the elec-
tronics and BNC cables is updated in the Sync Output 1 and 2 (outputs connected to the Q-
switch of the two cavities). See the numbers marked in yellow in the dialog below.
After a calibration (running preview with at least 100 images) a message will appear on the
screen saying the calibration is OK. The Calibration will be automatically be Disabled so that the
calibration values are not overwritten. The stored values will now be used during ”Time
Between Pulse Correction” measurements during image acquisition. If the aquisition settings
is changed a new calibration should be performed. This is done by Enable the Calibration in the
TrueTime properties.
Time Between Pulses Correction
After a valid calibration has been performed the Time Between Pulses correction is auto-
matically Enabled (and the Calibration is Disabled).
Time Between Pulses Correction of acquired images.
- Acquire a set of images and note the” Time Between Pulses Correction” information added
to the images time stamps below the images.
- Inspect the stored images and the corrected Dt for each image pair. The correction made, jit-
ter of the laser, is shown for both cavities as well.
Finally:
During evaluation of the vector field the corrected, Dt, Time Between Pulses will be used and
will improve the PIV accuracy.
Background Theory
Time Between pulses correction is based on measuring the time from a Q-Switch input is
triggered until light is emitting from the laser. These delays are measured in nanoseconds.
The synchronizer starts a stopwatch at the time it triggers the laser and stops the stopwatch
when the Synchronizer Optical input receives a light pulse.
PAGE | 328
PAGE | 329
Since the Jitter errors introduced are systematic in nature, these errors can be corrected for
in a straightforward manner.
For a dual cavity laser (PIV in double frame mode) the time from the Trigger pulse (Q switch) and
the light (PIV exposure) will be influenced by ”jitter”.
Calibration:
In order to correct Time Between Pulses in double frame (Time stamp in single frame mode)
the synchronize system needs to be calibrated.
Corrected_Dt = Dt +(CalibratedDelay1 - T1) + (T2 ? CalibratedDelay2)
Where:
Dt = Time between pulses set in the system
T1: Measured Time 1
T2: Measured Time 2
CalibratedDelay1= the calibrated time measured during Calibration for the first cavity in the
laser.
CalibratedDelay2 = the calibrated time measured during Calibration for the second cavity in
the laser.
Correction done for every “double pulse”:
In order to correct Time Between Pulses in double frame (or the Time stamp in single frame
mode) the following is done:
Corrected_Dt = Dt +(CalibratedDelay1 - T1) + (T2 ? CalibratedDelay2)
14.6.7 Scanning Light Sheet controller
Scanning Light Sheet internals
The internal of the scanning light is build up around a Galvanometer. The galvanometer
rotates a mirror, thereby creating the light sheet scan as illustrated below:
The mirror is never still (except at when the direction of the rotation is changed), it will always
be rotating in one or the other direction.
The rotation of the mirror can be seen as sine wave....
Detection of the scanning Light Sheet Controller
DynamicStudio automatically detects the Scanning Light Sheet Controller. No extra steps are
necessary.
PAGE | 330
PAGE | 331
A device in the acquisition system Device tree will be created representing the Scanning Light
Sheet controller.
The Scanning Light Sheet Controller in Synchronization Cables diagram:
Connecting Sync cables  for using the Scanning Light Sheet Controller
Signal name    Description                                    Usage                                        Recommended
Connection
Cycle                This signal is high during the
scan period. -The positive
edge indicates Scan period
start and will go high together
with the first sheet pulse.-The
negative edge indicates that
previous sheet scan has
ended,
This signal can be used to
start the synchronization
on the synchronizer.
Should be con-
nected to Timer-
Box In2 (Start
/Enable)
Sheet               This signal goes high at each
sheet position. A burst of
sheet pulses will be gen-
erated, one per sheet The tim-
ing offset of these pulses
relative to the actual sheet
position will calculated so that
the timing fits the System(1).
This signal is used to trigger
a System(1).
Should be con-
nected to Timer-
Box In1 (Trigger
in)
Arm                 External signal that tells the
Scanning light sheet to send a
Start signal on output Start
when ready.
Start                 A signal that can be used to
trigger cameras to start there
acquisition.Only one pulse is
generated. The position is
placed in between two sheet
bursts.
should be con-
nected to the
trigger input of
the camera
1. System: A system usually consists of a Synchronizer (preferable the TimerBox) a Camera
and a Laser.
Properties  for the scanning Light Sheet controller
Property
name
Description                                    Default
value
Min                   Max
Number of
light sheets
Specify the number of light
sheets to generate
1                     1                     Depends on the
connected Sys-
tem(1)
Volume width  Specify the with of the
volueme to place the light
sheet in
40                   1mm              80 mmDepend
also on the Scan
Frequency
Scan Fre-
quency
Specify the scan frequency.
(One scan included Number
of light sheets)
5                     10Hz              500HzDepend
also on the
Volume width
Single sheet
position
Incase only one light sheet
is to be generate, you can
specify the position in the
volume to place the sheet. 0
mm is in the middle of the
volume. Valid values are (-
Volume width)/2 to (Volume
width)/2
0                     -40mm           40mm
Sheet Fre-
quency
Shows the calculated Light
Sheet frequency(Read only)
-                      -                      -
Start method   Specifies how the Start sig-
nal is activated. (Some sys-
tems does not require this
signal, in that case just
ignore this property)
“Ignore” No
start signal
will be gen-
erated.
“Time”The
system will
after the
specified
time gen-
erate the
Start signal.
“User
Action”After the
acquisition as
been initialized
and is ready for
acquisition the
user will be
prompted.When
the user click
“OK” the Start
Signal will be
generated
Start time         Specifies the time from the
System is ready for acquis-
ition to the Start signal is
activated. (If Start method is
5000 mil-
liseconds
5000 mil-
liseconds
-
PAGE | 332
PAGE | 333
set to “Time”)
Start signal
polarity
Specifies the polarity of the
start signal to generate.
Positive          Negative       Positive
1. System: A system usually consists of a Synchronizer (preferable the TimerBox) a Camera
and a Laser.
Setting up the System
1.   Connect and turn on all devices connected to the PC.
2.   Startup DynamicStudio and enter Acquisition mode.
3.   Let the system detect all devices.
4.   In the Synchronization cables diagram connect
-TimerBox In1 to Sheet Sync on the Scanning Light Sheet Controller
-TimerBox In2 to Period on the Scanning Light Sheet Controller
-Connect the Laser to the TimerBox
-Connect the Camera(s) to the TimerBox
Depending on the type of camera connect Start on the Scanning Light Sheet Controller to
Trig in on the camera(s) (Not sync in)
5.   In properties for the TimerBox, set:
-Start on External trig
-Mode External
6.   In properties for the Scanning Light Sheet Controller, set:
-Number of pulses
-Volume width
-Scan Frequency
-Start mode(Depending on the type set Start mode to Time, User action or External trig.)
7.   Do a preview to see if images are acquired and focused.
8.   Do acquisition
14.6.8 Synchronization
Synchronizing devices in an imaging system is not trivial. Below is shown a timing diagram that
illustrates a simple synchronization between one camera and a double cavity laser.
The diagram looks simple, but what has to be considered in order to generate the signals for
this setup is much more complex. The diagram below shows the trigger signals that are sent
to the devices, and some of the different time definitions that have to be taken into the cal-
culation.
1.   Camera exposure timer
2.   Delay to open (most often in the range from 1 to 10 microseconds)
3.   Camera trigger pulse start time relative to T0
4.   Camera trigger pulse width (most often in the range from 1 to 10 microseconds)
5.   Flash 1 to Q-Switch 1 delay (most often in the range from 100 to 200 microseconds)
6.   Activation delay Q-Switch 1 (most often in the range from 100 to 200 nanoseconds)
7.   Flash 2 to Q-Switch 2 delay (most often in the range from 100 to 200 microseconds)
8.   Activation delay Q-Switch 2 (most often in the range from 100 to 200 nanoseconds)
Every device that needs a synch signal has at least one parameter that must to be defined in
order to calculate the right time of the signal. As an example let’s look at a gated light source.
In order to shape the light pulse in the right way three timing figures have to be defined:
1.   The wanted light pulse time (Pulse width)
2.   The time from the trigger signal is sent to the light source until the light starts coming out
of the light source (Activation delay)
3.   The time from the trigger signal ends until the light stops coming out of the light source
(Delay to close)
PAGE | 334
PAGE | 335
There is a DeviceHandler that represents each device in the Acquisition System. A
DeviceHandler can have multiple synchronization inputs and outputs.
Timing Calculation in DynamicStudio Acquisition System
The timing calculation in the DynamicStudio acquisition system is done in each of the device
modules. Only the device module that handles a specific device knows all the timing definitions
that are needed to calculate the right timing signal for the device. The calculation is done
based on two definitions:
T0 : The time at which the first (or only) light pulse is fired and is at its highest intensity. This is
the only fixed time in the system. The value of T0 is “0.0”. (If no light source is part of the sys-
tem, then think of this time as just T0.) The time before T0 is negative and the time after T0 is
positive.
T1 : The time at which the second light pulse is fired. T1 = T0 + “Time between pulses” (”Time
between pulses” can be changed in “System Control” dialog of DynamicStudio or at the Syn-
chronizers if more synchronizers is connected to the system)
In the synchronization diagram below T0 and T1 are inserted:
During start of acquisition the DeviceHandler is asked to calculate signal shape and timing for
each of the input connectors on the device. If the input connector is not connected to another
device then no calculation will be done.
If the Synchronization Between Devices  is  not Correct
Usually if synchronization between a camera and the laser is not correct you should start out
by checking if the laser timing specification is correct. Cameras that are integrated in Dynam-
icStudio are tested very thoroughly with lasers that are very well defined, when it comes to
timing. When we integrate lasers we usually rely on the manufactures specifications. Most
often it is necessary to fine tune lasers settings.
Another thing that can influence the synchronization is noise. We have seen that if the laser
power supply is standing too close to the cameras the magnetic emission from the laser, at
the time the flash lamp is fired, can trigger the camera. The camera then ignores the real sync
signal sent to the camera from the synchronizer since it has already started the exposure.
Cable length can also in rare situations have an influence on timing. Every 1 meter cable intro-
duce approximately 5 nano seconds delay.
14.6.9 USB  Timing HUB
The USB timing HUB is a plug and play device which will allow you to generate trigger pulses
and thus control your image acquisition system. The Timing HUB is equipped with 8 output
port and two input ports and can thus be trigger both internally and externally. The Timing
HUB runs at an internal clock of 50 MHz, allowing for a pulse positioning accuracy of 20 nano-
seconds
Installing Drivers  for the Timing Hub
The Timing HUB uses a USB connection to the PC.
The driver for the Timing HUB?can be found on the instalation MEDIA. The procedure is:
1)?Connect the Timing HUB?to the PC.
2) Open Device Manager
3) Right Click the Timing HUB device and select Update Driver Software
4) locate the Driver on the MEDIA and select install.
When the installation has completed correctly the timing HUB should be shown as displayed in
the device tree below:
The default properties of the HUB are shown below:
PAGE | 336
PAGE | 337
The HUB is setup to start acquisition automatically (when the user starts an acquire from the
"System Control" window).
The mode that the HUB will use is Internal (the HUB will supply its own clock to generate
pulses to devices attached to it).
Trigger rate is set to default meaning that it will use the trigger rate set in the "System Con-
trol" window.
14.6.10 Start options
The HUB has three different ways of starting an acquisition:
l  Automatically
l  Start on user action
l  Start after time
Note:?For start on User action and Start on Time the laser connected to the timing hub will be
flashing while waiting. If this is not the case examine the Timing diagram to see if a camera is
the first device to be triggered in the trigger sequence. in the case that the camera is
triggered as the first device add a Pulse receiver to the system and make sure that this Pulse
receiver is triggered before the camera.
Automatically
The HUB will automatically start generating pulses when an acquisition is started from the "Sys-
tem Control" window.
Start on user action
The HUB will start as if "Automatically" was selected except that a message box will appear and
it will not trigger the cameras connected to it until the button on the message box is pressed.
All other devices will be triggered.
Start after time
The HUB will start as if "Automatically" was selected except that it will not trigger the cameras
connected to it until the specified time "Start time" has elapsed.
14.6.11 Mode options
The HUB has three different ways of running when an acquisition has been started and is
ongoing:
l  Internal
l  External
l  External clock
Internal
The HUB will use its internal clock to trigger devices attached to it. The frequency at whish the
HUB will pulse devices is determined from the trigger rate set in the "System Control" win-
dow.
The HUB can also specify it own trigger rate rather than using the "System Control". To do so
type in a different value than "Default" (Default means it uses the one specified in the "System
Control" window). This is only available when more than one synchronizer is detected.
PAGE | 338
PAGE | 339
External
The HUB will use an external signal to trigger devices attached to it.
When this mode has been selected, it is possible to specify on what input the external signal is
received (either A or B).
External clock
The HUB will use an external signal to trigger devices attached to it.
When this mode has been selected, it is possible to specify on what input the external signal is
received (either A or B).
It is also possible to skip pulses in this mode (the minimum pulses to skip is 4).
If 6 has ben entered it means that every 6th external pulse will pass to trigger devices.
All other pulses will be suppressed by the HUB.
14.6.12 Limitations
Some cameras like Photron and SpeedSense Ethernet cameras use hardware timestamps
from the synchronizer in certain modes. The HUB does not supply hardware timestamps, the
timestamp on images from these cameras can be incorrect.
This is the case in the following situations:
Running External mode and supplying an external trigger signal with a rate different from
what is specified.
Running External mode and pausing or disconnecting the external trigger signal.
Running External clock mode.
These situations can be overcome by using the Timer Box (80N77) instead.
14.6.13 High  Resolution  Synchronizer
The High Resolution Synchronizer Pulse/delay generator can be used as a Synchronizer in the
DynamicStudio Acquisition system. The High Resolution Synchronizer can be used to control a
number of different devices. The BNC device can be connected to the PC in two different
ways. You can connect the BNC via a RS232 port, or you can connect the BNC device via a USB
port.
Detection
Once DynamicStudio has been stated and Acquisition mode has been entered, DynamicStudio
will find all COM ports on the PC. Each COM port will be listed in the Device Tree of Dynam-
icStudio. If the High Resolution Synchronizer is connected via USB, a virtual COM port is estab-
lished in the Windows operating system.
For each COM port detected, the system will try to detect if a device is connected.
Turn on the BNC device. After a while the High Resolution Synchronizer is detected and the
device is listed just below the COM port on which it was detected.
If the High Resolution Synchronizer is not detected, the High Resolution Synchronizer might
have a wrong Baud Rate set in the communication settings of the device. To solve this do the
following:
1. On the front panel of the High Resolution Synchronizer press "Func"(yellow button) and
then "System"(numeric button 3).
Do this until the display looks like this:
[ ] System Config
Interface: RS232
Baud Rate: NNNNNN
Echo :?Disabled
2. Press Next until the cursor is at the number specifying the current Baud Rate of the High
Resolution Synchronizer.
3. Change the Baud Rate to 115200 and press "Next".
High Resolution Synchronizer Trigger modes
The BNC575 offers three different trigger modes that will be described in the follow sections.
Trigger mode Internal
By using this setting (default) the system will run at the trigger rate specified in the "System
Control" panel. The system will start when the user presses the acquire button.
PAGE | 340
PAGE | 341
Trigger mode External
In this setting the system will run with a trigger rate, which is given by an external signal
through input trig on the High Resolution Synchronizer.
NOTE: THE LAST OUTPUT (Out H for a 8  channel BNC and OUT D for a 4  channel BNC)
and "Gate" has  to be physically  connected using a BNC cable. Out 8/4  together with
Gate functionality  is  used to generate "Dead time" in which trigger pulses  on Trig
input are inhibited.
Trigger mode External clock
This trigger mode is able to skip external trigger pulses. The parameter "Use every N'th
pulses; N=" define how many pulses to skip before triggering an image capture.
NOTE: It is  up to the user to set a high enough N value - too small N value will cause
unexpected additional synchronization pulses  sent to devices, this  will result in addi-
tional laser pulse exposure in Frame 2  especially  with CCD cameras
NOTE: THE LAST OUTPUT (Out H for a 8  channel BNC and OUT D for a 4  channel BNC)
is  used for counting incoming signals. This  output can not be used for synchronizing
devices.
NOTE: Trig Acq-device- and Trig Q-Switches  - at Trigger rate/N can not be used in trig-
ger mode External clock
Trigger mode External frequency  multiplication
This trigger mode is able to multiply the incoming frequency with an integer number. The sys-
tem will then trigger all devices using this new trigger rate.
The "Trigger input frequency" must be measured and entered by the user.
The integer number "Burst pulses" for the frequency multiplication is calculated base on the
parameters "Trigger input frequency" and "Trigger rate".
The "Trigger rate" is first entered by the user, then "Burst pulses" is calculated based on "Trig-
ger input frequency" and "Trigger rate"."Trigger rate"is then validated and corrected to the
nearest valid frequency.
Trigger mode External burst trigger
This trigger mode will generate a number of trigger pulses on each trigger input. This new
'trigger rate' will be used to trigger all devices in the system.
During execution of the burst triggering, all incoming trigger signals will be ignored. After the
execution of the burst triggering the system is ready of a new Burst trigger.
The integer number "Burst pulses" specifies how many pulses are generated on each trigger.
The "Trigger rate" defines the time between each burst pulse.
Start modes
To start an acquisition four different start modes are available :
l  Automatically
The acquisition will start as soon as all devices have been configured and are ready.:
l  Start on user action
When all devices have been configured and are ready for acquisition, the laser will start
flashing, but the sync outputs for cameras and A/D boards will not be enabled. A message
box will appear asking the user to start the acquisition. When OK is clicked all sync outputs
for cameras and A/D boards are enabled.
l  Hard start on user action (can be used in Trigger mode Internal only)
When all devices have been configured and are ready for acquisition, the system waits for
a software trigger. No devices are triggered. A message box will appear asking the user
to start the acquisition. When OK is clicked all sync outputs will start to pulse the connected
devices.
l  Start on external Trig(can be used in Trigger mode Internal only)
When all devices have been configured and are ready for acquisition, the system waits for
an external trigger pulse on Trig input. No devices are triggered. A message box will
appear asking the user to start the acquisition. When OK is clicked all sync outputs will start
to pulse the connected devices.
PAGE | 342
PAGE | 343
l  Start after time.
When all devices have been configured and are ready for acquisition, the laser will start
flashing, but the sync outputs for cameras and A/D boards will not be enabled. Now the
system is waiting for the time "Start time" to expire. When the time has expired all sync
outputs for cameras and A/D boards are enabled.
The High Resolution Synchronizer cannot enable more than one output at a time. Each
enabling is done via a serial command to the BNC device, and the execution of a command
takes about 10 ms. This means that if more cameras and analog inputs are connected to dif-
ferent outputs it can not be guaranteed that acquisition for each device will start at the same
time. One camera can be enabled just before a sequence starts, and the next camera just
after, leading to different start time, since the second camera will not see the first trigger.
This issue applies to all Start modes except Automatically.
Parameter "Trig Acq-devices  at 'Trigger rate'/N" and "Trig Q-Switch at 'Trigger rate'/N
When either of these parameters are enabled, the parameter 'N' is enabled. N specifies how
often Q-switches or Cameras are to be triggered relative to the selected trigger frequency.
NOTE: Trig Acq-device- and Trig Q-Switches  - at Trigger rate/N can not be used in trig-
ger mode External clock
Laser warm up
It is possible to define how many times or for how long the Laser Flash lamp and Laser Q-
switches are to be pulsed before cameras and A/D sampling are enabled. The Parameter
"Warm up unit" defines 'whether to use Time' (in seconds) or 'Pulses' for the parameters
"Flash lamp warm up" and "Q-Switch warm up".
Pulse Mode
Determines whether the High Resolution Synchronizer should follow the default setting or an
internal setting:
l  Follow default pulse mode:Follows default setting as set in the “System Control” panel. If
the system control window is set to double frame, the timer box will deliver double pulses,
i.e. pulse both cavities in a laser.
l  Single pulse: The High Resolution Synchronizer will generate single pulses regardless of
settings in the “System Control” panel.
l  Double pulse: The High Resolution Synchronizer will generate two pulses regardless of set-
tings in the “System Control” panel.
Outputs
Each output on the BNC 757 can be defined to output either "TTL/CMOS" signals or a voltage
from 2 to 20 volts as High. (Low will always be 0 volts)
"Not able to generate required signals" issue
This is a known issue and usually it is because the system requires more timers than the High
Resolution Synchronizer can offer. To solve the issue remove the connection to Out H. You
are probably using Trigger mode External or External Clock.
Time Stamping
There are two ways of starting the time measurement that is used for Time stamping images
acquired by the system. This is control via parameter "Start Timestamping on first image".
When the synchronizer is set to trigger mode External, the timer used to time-stamp images
is started when the system starts the acquisition. This will make the first image have a time-
stamp depending on how much time has passed since the acquisition started. If you select Yes
the first image will have time-stamp 0.000s
14.6.14 Timer Box
One of the timing devices, which can be used together with DynamicStudio, is the Timer Box.
The TimerBox exist in three different versions 80N75, 80N76 and 80N77. 80N77 is the most
recent type and replaces the two other devices.
Systems delivered before February 2007 will be equipped with 80N75, 80N76 or 80N48 (USB
Timing HUB). 80N77 has increased functionality, which will be described at the end of this doc-
ument.
14.6.15 Installing the TimerBox
The timer box is driven by a National Instruments PCI-6601 or PCI-6602 board (the latter is
for 80N77 only). It is very important that the correct National Instruments driver is installed
before DynamicStudio is installed. Please refer to the DynamicStudio release note to learn
which driver to install.
Please install the National Instruments driver before installing the physical card in the PC.
After the driver has been installed, switch of the PC, install the card and restart the PC. Make
sure that the new Board has been properly detected by opening National Instruments "Meas-
urement & Automation Explorer" and inspecting the list of devices in the "Devices and Inter-
faces\Traditional NI-DAQ(Legacy) devices" folder.
After the National Instruments driver has been installed, you can install DynamicStudio. Be
sure to go online with the system in order to check if DynamicStudio has properly detected
the board. For more information about device detection, please read the Getting Started
guide.
Connect the Timer Box to the Timer board using the shielded cable. Make sure that the
screws are fastened securely. Check if DynamicStudio has detected the timer box properly.
PAGE | 344
PAGE | 345
14.6.16 Connecting the timer box
The timer box can be used to control a number of different devices, which all require TTL
input for the triggering. Additionally, the timer box can also be used as an interlock control via
the DSUB 9 plug on the backside of the box. As the possibilities for connecting the box are
numerous, only a few are described here. Generally it is recommended to use the “Create
Default Connections” function in the “Synchronization Cables” panel of Dynamic Studio. Please
read the getting started guide for more information.
Connecting the 80N75  box
This TimerBox - also called the DC PIV timer box - is primarily intended to be used together
with lash lamp pumped lasers, such as the NewWave Solo PIV and a CCD camera such as the
HiSense MKII. The diagram below shows an example of how to connect the devices to the
box.
Connecting the 80N76  box
This timer box is the TR-PIV equivalent of the 80N75. It has been designed primarily to be
used with TR-PIV systems consisting of a diode pumped pulsed lasers and CMOS cameras. It is
however possible to use it with standard PIV systems as well. The default cabling is slightly dif-
ferent from that of the 80N75. Please see the example below.
Connecting the 80N77  box
This timer box is the newest timing device from Dantec Dynamics. As opposed to 80N75 and
80N76 it has eight separate output channels and is thus more versatile. The cabling for this
timer box can also be chosen more freely. In addition to running in normal mode, this Timer
Box has the following capabilities:
l  True hardware time stamp
l  External trigger
l  External Synchronization
l  External trigger enable signal
l  Possibility of using double gated devices (Intensifiers and diode light sources)
A typical connection diagram for the 80N77 is shown below:
External triggering
In addition to a more flexible functionality, the TimerBox features some advanced trigger
functions. These are described briefly in the following.
All of the settings below can be accessed from the Device Properties panel. Click on the
“Timer Box” icon on the “Devices” panel and open the “Device Properties” panel to change set-
tings.
PAGE | 346
PAGE | 347
14.6.17 Synchronization  setup
Internal synchronization, internal trigger
By using this setting (default) the system will run at the trigger rate specified in the "System
Control" panel. The system will start when the user presses the acquire button.
Use the following settings in the “Device properties” panel, trigger mode setup:
l  Start: automatically
l  Mode: internal
l  Use trigger enable input: NO
External synchronization, internal trigger
In this setting the system will run with a trigger rate, which is given by an external syn-
chronization device through input port 1 on the timer box. It is necessary to add a pulse gen-
erator to the system to make DynamicStudio recognize the pulses. This is done by right hand
clicking the “Acquisition Agent” icon in the “Devices” panel and selecting “Add new device ?
Pulse generator”
Use the following settings in the “Device properties” panel, trigger mode setup:
l  Start: automatically
l  Mode: external
l  Use trigger enable input: NO
Trigger delay: To delay the measurement from an external trigger go to the parameters of
the Synchronizer. When set to Trigger mode External you will have the possibility to set a trig-
ger delay. The delay you specify is from the external event to the first light pulse from the
laser(T0). If you examine the Timing Diagram in DynamicStudio you will see all delays including
the trigger delay relative to the first light pulse.
Internal synchronization, external trigger
Here the system starts when a trigger pulse is received on IN2(for 80N75, 80N76 use IN1).
The system runs at a frame rate specified in the “System Control” panel.
Use the following settings in the “Device properties” panel, trigger mode setup:
l  Start: Start on external trig
l  Mode: internal
l  Use trigger enable input: NO
External synchronization, external start, gated synchronization (80N77  only)
The system starts when a trigger signal is received on input port 2 and runs at a rate given by
the synchronization pulses on input port 1. However, a signal high (+5 volts) must be present
on input port 2 in order for synchronization pulses to have an effect. If this signal high (gating)
does not exist, the system will stop running until at occurs again
Use the following settings in the “Device properties” panel, trigger mode setup:
l  Start: Start on external trig
l  Mode: External
l  Use trigger enable input: Yes
14.6.18 Synchronizing two TimerBoxes
To synchronize two Timerboxes, the first must be set to use 'Start on user action' or 'Start on
external trig'. The second Timerbox must be set to use external mode (external syn-
chronization). This is done by connecting Out 6 to In 1 as shown below. It is vital that it is out 6
that is used and not any of the other outputs.
PAGE | 348
PAGE | 349
When starting an acquisition, depending on the start mode, the user will either be prompted
to start the acquisition or he will need to supply an external signal on In 2 of the first Timer-
box.
14.6.19 Additional settings
Pulse Mode
Determines whether the TimerBox should follow the master setting or an internal setting:
l  Follow default pulse mode: Follows master setting as set in the “System Control” panel. If
the system control window is set to double frame, the timer box will deliver double pulses
I.e. pulse both cavities in a laser
l  Single pulse: the TimerBox will always generate single pulses
l  Double pulse: The TimerBox will always generate two pulses
Safety  Switch Control
It is possible to connect a shutter or an interlock cable to a laser using the DSUB 9 plug on the
backside of the Timer Box. The drawing below shows ho the cable should be connected. The
Safety Switch Control controls when the safety shutter is closed.
l  On when online: Closes the switch when the system is online
l  On when acquiring: Closes the switch when the system is acquiring
Trigger Rate
If the TimerBox is set to run on internal synchronization, the trigger rate can be chosen here.
This is equivalent to setting the trigger rate in the “System Control” panel.
14.6.20 Cyclic Synchronizer and Linear Synchronizer
The Cyclic Synchronizer is a device that on the input can connect to a Rotary Incremental
Encoder (or Rotary Incremental Shaft Encoder), and on the output side sends synchronization
signals to an ordinary Synchronizer, which will synchronize the measurement system.
The Linear Synchronizer is a device that on the input can connect to a Potentiometer, and on
the output side sends synchronization signals to an ordinary Synchronizer, which will syn-
chronize the measurement system.
PAGE | 350
PAGE | 351
Note: The Cyclic and Linear Synchronizers are not designed to synchronize lasers and cameras
directly; an additional ordinary synchronization device is needed,e.g., TimerBox or BNC575.
Input and Output Specification
Encoder input (on Cyclic Synchronizer)
The Cyclic Synchronizer supports any Rotary Incremental Encoder (or Shaft Incremental
Encoder) that provides either a TTL or Differential output of the signals Index (one pulse per
full rotation) and Angle Increment (Angle Inc. for the rest of this manual, which is a fixed num-
ber of pulses uniform distributed in one full rotation, e.g., 3600). The maximum number of
angle increments counts per full rotation is 32768.
The Cyclic Synchronizer is working on the edges of the Index pulse and Angle Inc. signal
pulses. It is possible to select which of the edges to work on (either positive or negative going
edge).
Note: The Cyclic Synchronizer is not able to tell which direction the encoder is moved; there-
fore the Cyclic Synchronizer does not support pendulum-moving systems.
Note: In order for the Cyclic Synchronizer to calculate the current encoder position, the
encoder has to move a full rotation, passing the Reset position on the Encoder.
The maximum rotation speed of the encoder must be kept below 15000 RPM.HUSK
Below are the specifications of the Encoder Inputs & Outputs:
・ BNC inputs Index and Angle Increment
Both are TTL (5V) inputs.
・ Differential DB9 input
5V differential signals as described below:
Pin 1: + Angle Inc.
Pin 2: - Angle Inc.
Pin 3: + Index
Pin 4: - Index
Pin 5: 5 V DC (I max: 250 mA)
Pin 6: GND
Pin 7: GND
Pin 8: Nc.
Pin 9: Nc.
Max Differential Voltage: -5 to +5 V
Sensor input (on Linear Synchronizer)
The Linear Synchronizer is typical delivered with a Potentiometer, but in special cases it is only
delivered with a Cable that fits the Sensor input of the Linear Synchronizer.
The Sensor Input cable for connecting the Potentiometer has three inner wires:
Black goes to 1
Brown Goes to 2
Blue goes to the slider
After a calibration is done, the Linear Synchronizer will have made sure that if you move the
slider to position 2 you will measure around 4.09V at the slider. If you move the slider to pos-
ition 1 you will measure 0V at the slider.
・ Start Acq. and Trigger Input
These inputs are used in different ways, depending of the mode of operation, which will be
introduced later.
Both are TTL (5 V) inputs.
・ Encoder Out 1 and Out 2 Outputs
These are the outputs where the synchronization signal for the connected synchronizers are
generated.
Both are TTL (5 V) outputs.
・ Specification TTL (5 V) Input and Outputs
Note: All specifications below use the following notation:
Output Low = OL
Output High = OH
Input Low = IL
Input High = IH
VIL 0.8 V
IIL 20 μA
VIH 3.2 V
IIH 55 μA
VOH 5 V
IOH 0.5 A
VOL 0.025 V
IOL 0 A
・ USB port
For communication with PC, the Cyclic Synchronizer is fitted with a USB port. In order to com-
municate with the Cyclic Synchronizer a device driver has to be installed.
PAGE | 352
PAGE | 353
The device driver will create a Virtual Serial COM port on the Operating System when the Cyc-
lic Synchronizer is connected via the USB port.
When a Virtual COM port is created it will be persistent and will be visible in the Device Manger
even if the Cyclic Synchronizer is not connected or powered on.
Moving the Cyclic Synchronizer to a new USB port on the PC will not create a new virtual COM
port.
Detection of the Synchronizer
When starting up DynamicStudio and entering Acquisition mode, DynamicStudio will detect
and create a Serial COM port device and add it to the Acquisition System Device tree in Dynam-
icStudio.
After having detected the COM ports DynamicStudio will try to detect if a Synchronizer is con-
nected to the PC. Having detected a Synchronizer a new device representing the Syn-
chronizer is added to the Acquisition System Device tree.
After detecting the Synchronizer, the Synchronizer has to be updated with information about
the encoder connected to the Synchronizer. The detailed instructions on how to set up the
encoder are given below.
Setting up Encoder information Cyclic Synchronizer
The Encoder Setup must be done before any operation: The following information about the
connected Encoder must be updated: (make sure the calibration mode in Mode Setup of
Device Properties is selected in this step)
・ Encoder Revolutions  per Cycle
Specifies the number of full rotations the Encoder has to take for one full "flow cycle". For
instance, if the measurement object is a four stroke internal combustion engine, this number
should be 2.
Note: This number should be positive integer.
・ Increment Pulses  per Encoder Revolution
Specifies the number of Angle Inc. pulses sent out from the Encoder during one encoder
revolution. This number should refer to the properties of the encoder used, and are not to
be changed unless the encoder's own settings are changed.
・ Full Cycle/Range
The functionality of the parameter is to describe the size of the Full Range for the Poten-
tiometer or Full cycle for a rotary encoder.
For a rotary encoder, full cycle would typically be 360 with the unit deg.
Another example is a potentiometer that has a circular movement of maximum 270 degrees.
Then set “Full Cycle/Range” to 270 and “Position Unit” to deg
One example is for a linear Potentiometer which is 500 mm long from one end to the other,
then enter 500, and set the parameter “Position Unit” to mm.
.
・ Encoder Index Signal Polarity
Specifies which edge (positive or negative) of the Index pulse from the encoder the Syn-
chronizer is to use.
・ Angle Increment Signal Polarity
Describes the encoder angle Increment signal polarity: Positive or negative going edge.
Below are four situations most likely to be encountered when using an encoder:
For the two cases at the top of above figure, the Encoder Index Signal Polarity should be pos-
itive and the Angle Inc. Signal Polarity should be Negative. For the other two cases at the bot-
tom of above figure, the Encoder Index Signal Polarity should be Negative and the Angle
Increment Signal Polarity should be Negative.
・ Zero Position Offset
Specify the zero position offset of the cycle in the hardware, i.e., if the tester wants to define
the top dead center of an internal combustion engine as the start of the fluid cycle, this zero
position offset is the offset between the angle position of the encoder when the piston is at
top dead center with the angle position when the encoder send out Index pulse. This number
could be either input arbitrarily or systematically determined in the calibration operation. For
the systematical determination, please refer to the instruction of Calibration Mode.
・ Encoder Input Connector
Specifies which of the encoder input connectors is used.
o BNC: The Index and Angle Inc. signal is provided to the BNC Index And BNC Angle Inc.
inputs.
o DB9: The Index and Angle Inc. signal must be provided via the Differential DB9 input con-
nector.
Note: Please be aware of that the definition of 1 cycle in this application. 1 cycle means the
period in which the whole cyclic phenomena has happened, and it doesn't need to be 1 revolu-
tion - 360 degrees in many cases. For instance, 1 cycle for a 4 stroke internal combustion
engine covers two revolution of the engine crankshaft, which is 720 degrees. These
PAGE | 354
PAGE | 355
parameters should be clarified in Encoder Setup - Encoder Revolution per Cycle and Encoder
Setup - Full Cycle.
Calibration Cyclic Synchronizer
After setting up the Encoder information the Cyclic Synchronizer needs to know where the
"Cycle Zero Position" of a cyclic phenomena is relative to the physical world.
Note: The encoder will give an INDEX pulse when it pass the encoder inherent zero position.
However, this encoder inherent zero position might not be the "Cycle Zero Position" for the
application.
This is done by selecting mode Calibration. Using this mode the Synchronizer is brought in to a
state where a physical button on the Cyclic Synchronizer is used to indicate where "Cycle Zero
Position" is located.
Start by rotating the system one full rotation, then move the system to where “Cycle Zero Pos-
ition” is to be defined, and then press the button.There should be a blink from the LED below
the ZERO button, and the number corresponding to the 'Zero Position Offset' in the Encoder
Setup will be updated if you click you mouse between different modes in Mode Setup.
Note: It is important not to move the system backwards, since the Cyclic Synchronizer is not
able to detect the direction of the movement, only that the system is moved.
Note: In order for the Cyclic Synchronizer to calculate the current encoder position, the
encoder has to move a full rotation, passing the Zero position of the Encoder.
Note: This procedure has to be done each time (and only if) the Cyclic Synchronizer has been
power cycled.
After defining “Cycle Zero Position” using the button on the Cyclic Synchronizer the zero pos-
ition can be also adjusted using the “Zero Position Offset” parameter entry in Encoder Setup.
Setting up Encoder information Linear Synchronizer
・ Encoder Revolution per cycle
Specifies the number of full rotations the Encoder has to take for one full "flow cycle". For
instance, if the measurement object is that needs to go forth and back 2 times in order to
have performed a full cycle, this number should be 2.
・ Position Unit
Here it is possible to enter the Unit in which the position is given, this can be any given string.
here are some examples: "deg", "mm" or "m2.
・ Full Cycle/Range
The full range is a number that represents the distance between position (1) and position (2).
You could choose to enter 100 and set "Position Unit" to "%", or you could measure the dis-
tance in millimeter, enter the number found, and set "Position Unit" to "mm"
・ Zero Position Offset, End Position Offset and Range
In order for the Linear Synchronizer to work correctly two positions must be specified. The
Slider must pass both these positions, first the Zero position and then the End position, and
going back the other direction, first the End position and then the Zero position.
These positions must be within the "Full Cyle/Range" value.
These position can also be set using the button on the Linear Position.
1. Move the slider to where the Zero position are to be set, and press the button on the Lin-
ear Synchronizer. When the button click is recognized, the Linear Synchronizer will light the
Zero Position LED one time, indicating that the Zero position is set.
2. Move the slider to where the End position are to be set, and press the button again. When
the button click is recognized, the Linear Synchronizer will light the Zero Position LED Two
times, indicating that the End position is set.
The area between the two position is called the Working Range. The size of the Range can be
seen in parameter "Range".
・ Calibrate
Is used to calibrate the potentiometer to be used with the Linear Synchronizer.
Calibration of the Linear Synchronizer
Before staring to setup anything else on the Linear Synchronizer, the Potentiometer has to
be known to the Linear Synchronizer. This is done performing a calibration. This is done in 3
steps, following the instruction presented during the calibration.
The two position referred to during the calibration are the following:
(1) The end at which the Black wire is connected to.
(2) The end at which the Brown wire is connected to.
Note:If for some reason the calibration fails, try to move the slider to the other end than what
was expected.
Important:?Measure the distance between the last two positions, and enter this value into the
property "Full Cycle/Range".
Connecting Synchronization cables
Note: There are two types of connection of synchronization cables, depending on which oper-
ation mode is selected. Only software connection is introduced below, but please also switch
the connection in both software and hardware when you switch between relative operation
modes.
Capture mode
In Synchronization Cables connections diagram, make a connection from the Synchronizer to
the Synchronizer “Trigger” input.
Preferable connect the Synchronizer to the same output as Q-Switch 1 on the laser (or the out-
put that triggers the first light pulse in the PIV system). This will insure that the position cap-
tured is done at the same time as light from the laser is emitted, and the particles illuminated
Capture mode and Start mode
If the laser has a warm-up period in which the Q-switch signal is also triggered, it is preferable
to use the "Start Acq" input.This will insure that these leading trigger signals does not provide
PAGE | 356
PAGE | 357
capture of position information which will introduce misalignment between position inform-
ation and image capture.
Connect the "Start Acq." input on the Synchronizer to the same output on the Synchronizer as
the Camera (or one of the cameras). And then in Operation Setup set Start Mode to “External”
and the Start Trigger polarity to the same as camera trigger polarity.
Other modes Connections
In Synchronization Cables diagram connect the “OUT 1” output of the Synchronizer to the
External Trigger input of the Synchronizer that handles synchronization of the camera and
laser.
Note: In the setup for the Synchronizer that handles synchronization of the camera and laser,
check that the trigger mode is set to be External.
Mode selection
The Synchronizer has many different modes in which it can run. In the following each mode
will be described in detail. (Mode Calibration will not be described here, see section Calibration
for more information). In each mode, various parameters should be setup in the Operation
Setup below Mode Setup.
Capture
In this mode the Synchronizer receives sync signals from both an ordinary Synchronizer and
the encoder.
As long as the acquisition sync signal is receive by the Synchronizer from the ordinary Syn-
chronizer, the Cycle count and position information is acquired from the encoder and saved.
The position information is added to the Timestamp information of the acquired image and
when images (or Analog data) is presented the position at which the image was acquired can
be monitored.
Note:For the Linear synchronizer, all position on the slider will be used and not only those
with in the Working Range. The Zero position is automatically moved to Position (1), no matter
what Zero position has been entered.
Operation Setup - Trigger Polarity
Specifies which edge of the Angle Inc. signal to use to trigger the saving information needed.
Continuous Fixed Position
In this mode the Synchronizer is set to continuously generate a sync signal at a specific pos-
ition on each function cycle.
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization starts.
・ Mode Setup - Skip Cycles
Specifies a number of Cycles to skip in between each sync signal send to the Synchronizer that
handles the camera and laser.
・ Mode Setup ?  Position
Specifies the Position at which to synchronize the system.
Burst Modes
The definition of a burst is a predefined number of sync signals sent from the Synchronizer to
the connected Synchronizer. When the predefined number of signals has been sent a new
burst can take place.
For all burst modes the following parameters can be used:
・ Operation Setup ?  Trigger Mode
This parameter describes how each burst of sync signals are to be started.
・ Internal
The Burst will start automatically after the Synchronizer receives the first Index pulse from
the encoder.
・ External
The Burst will start on the first pulse received on Trigger input. (Remember to Set Trigger
polarity correctly).
・ User Action
A prompt will be presented in DynamicStudio. Here it will be possible to start a burst or stop
the acquisition.
・ Operation Setup ?  Burst Interval
Using this parameter it is possible to describe a minimum time between each start of a new
burst in milliseconds. If the time set is less than the duration of a full burst period, this setting
will be ignored.
・ Mode Setup - Number of Sync Signals  per Burst
Specifies the number of sync signals per burst.
PAGE | 358
PAGE | 359
Burst within Cycle
In this mode a burst of sync signals is send to the Synchronizer. All sync signals are within the
same Cycle.
Whenever a burst is over and done, the next burst can be started. The starting position in
each burst/Cycle will always be the same, as well as the following positions in the burst.
Note: Take care that the resulting trigger frequency does not exceed the maximum trigger
frequency for the Synchronizer that handles synchronization of the camera and laser.
The generated trigger frequency can be monitored in Status - "Sync Output Frequency" - as
shown below:
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization can star a burst.
・ Mode Setup ?  Position
Specifies the starting Position at which to start the burst in the cycle.
・ Mode Setup - Position Increment
Specifies the position increment that will be added to previous position position.
Burst Fixed Position
This mode is very similar to Continuous Fixed Position, the difference is that here a burst of
sync signals are send to the connected synchronizer, one sync signal per rotation.
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization can star a burst.
・ Mode Setup ?  Skip Cycles
Specifies the number of cycles to be skipped in between each sync signal sent to the Con-
nected Synchronizer.
・ Mode Setup ?  Position
Specifies the Position at which each sync signal is sent to the connected Synchronizer.
Burst Position Increment per Sync
This is similar to Burst within Cycle, except that only one sync signal per full rotation is send to
the connected synchronizer.
PAGE | 360
PAGE | 361
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization can start a burst.
・ Mode Setup ?  Skip Cycles
Specifies if number of cycles is skipped in between each sync signal sent to the Connected Syn-
chronizer.
・ Mode Setup ?  Position
Specifies the starting Position at which the first signal in the burst triggered.
・ Mode Setup - Position Increment
Specifies the position increment that will be added to previous position for the next sync sig-
nal in one burst.
Burst Position Increment per Burst
In this mode the first burst is of signals will be send at the same position, only one per each
Cycle. The synchronization signal out from the Synchronizer in next burst will be at the pre-
vious position plus the position increment.
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization can start a burst.
・ Mode Setup ?  Skip Cycles
Specifies if non or more cycles is skipped in between each sync signal sent to the Connected
Synchronizer.
・ Mode Setup ?  Position
Specifies the starting Position at which the first burst of signals will be created.
・ Mode Setup - Position Increment
Specifies the position increment that will be added to previous position for the next burst of
signals.
Engine Multi Operation Sync.
This mode provides nearly-full control of each synchronization pulse sent out from the Syn-
chronizer. In this mode it is possible to define a number of Burst periods for one full Burst
Cycle. Each signal in the burst period can be set at an arbitrary position(maximum one position
for each cycle). Also it is possible to define on which (or both) output (OUT 1 , OUT 2) the sync
signal is to be set.
It is particular useful for measurement on an optical internal combustion engine. Normally an
optical engine cannot be run with combustion for more than several minutes, due to the
thermal limitation of the optical glass. Therefore it needs to run on motored mode - without
combustion after the fired mode to avoid over-heating of the optical engine. Therefore, in
order to continue the measurement during the motored operation, it is desired to have vari-
ous synchronization signal for different operation mode of the engine.
Note: although it is called "Engine Multi Operation Sync.", it doesn't mean that this mode could
be only selected for engine measurement. It provides very large freedom to control each syn-
chronization pulse sent out from the Synchronizer, and thus could be also selected for other
application desire complex synchronization!
PAGE | 362
PAGE | 363
・ Mode Setup - Start Cycle
Specifies the Cycle number at which the synchronization can star a burst.
・ Mode Setup - Number of Burst Periods  Per Full Cycle
Defines the number of “burst periods” in the one Burst Cycle.
・ Mode Setup ?  Burst Period #x (x is  the index of each burst period)
For each “Number of periods per full cycle” defined above a Period entry will emerge in para-
meters for the Synchronizer.
In each period it is possible to define the following:
・ Period Name
Here specifies the name of this period. Default name for Burst Period #1 is "Fired", and for
Burst Period #2 is "Motored".
・ Cycles in Burst Period
Here it is defined how many cycles this Period covers.
・ Sync pulses
Defines the number of Sync pulses to be created during the period.
・ Skip cycles
For each sync pulse to be created a number of Skip cycles can be defined in between each
sync pulse. This defines how many cycles to skip before next sync pulse output.
・ Positions
For each Sync pulse to be created a the position must be defined. This is done here.
・ Enable Sync1 and Enable Sync2
These parameters define if the sync pulses are to be created on one or both of the Sync out-
puts. If it is selected as true, means this channel is enabled, and vice versa.
Example
Burst Period #1: 4 | 2 | 1; 1 | 90; 90 | True | False
Burst Period #2: 1 | 1 | 0 | 120 | False | True
In this setup, there will be two burst periods in one burst cycle. The first one will contain 4
cycles. Among these 4 cycles, sync pulse will be generated in the first one and the third one,
because 1 cycle is set to be skipped after each sync pulse. The position for each sync pulse
where it is sent out in one cycle is 90. The sync pulse will be sent out from OUT 1 because only
this channel is enabled.
The second burst period only contains 1 cycle, and only one sync pulse without any skip cycle.
The position for this sync pulse where it is sent out in one cycle is 120. It will be sent out from
OUT 2 because only this channel is enabled here.
Fixed Position Window Triggering
In this mode the Synchronizer sends signals at a predefined fixed rate on OUT 1. The Syn-
chronizer will continue to do this even though no rotation of the Encoder shaft takes place.
When the Synchronizer starts to receive Encoder Reset and Increment signals, and the fre-
quency and position starts matching the predefined trigger signal The Synchronizer will try
phase-lock to the external signal, until eventually the encoder rotation now drives the OUT 1
signal.
When in phase-lock the Synchronizer starts to send out sync signals on the OUT 2.
If for some reason the external signal disappears the Synchronizer will take over and again
driver a steady sync frequency on OUT 1 and the OUT 2 signal will stop.
Note: This mode is designed to be used along with those devices with fixed trigger rate (some
lasers) and a warm up period of 10 minutes (or even longer)
・ Mode Setup ?  Position
Specifies the starting Position at which the sync signals is created.
・ Mode Setup ?  Sync Signal Frequency
Specifies the frequency at which the system will be driven.
・ Mode Setup ?  Sync Signal Window Size
Specify the window size in which the system can be triggered from the cyclic world (in time).
This should be refer to the specifications of related devices. For example, if a laser's trigger
rate have to be 10±0.4 Hz, this window size should be (1000/9.6-1000/10.4)=8 ms.
Start Mode
The “Start Acq.” input on the Synchronizer can be used to trigger the starting point of when to
start triggering the Synchronizer (or in Capture mode when to start the capture).
For all start mode goes that a valid RPM has to be measured before any operation can start.
There are three different ways to start:
・ Internal
The operation will start automatically
・ External
The operation will start on the first pulse received on Start Acq. input. (Remember to Set Start
trigger polarity correctly)
・ User Action
A prompt will be presented in DynamicStudio. Here it will be possible to start the operation or
cancel the acquisition
PAGE | 364
PAGE | 365
Valid RPM Range
Here it is possible to specify a RPM range in which the system will synchronize to the cyclic
experiment. If the rotation speed is lower or higher than specified no synchronization will
take place.
Current RPM measured by the Synchronizer can be monitored in Status, which will be intro-
duced below.
Status
The Status section of the properties for the Synchronizer provides information about the cur-
rent state of the Synchronizer.
Sync Output Frequency
When the Synchronizer has been setup and is providing sync signals to a connected Syn-
chronizer the maximum frequency of the Sync output can be monitored here.
When setting up the system, make sure that the frequency specified here does not exceed
the maximum trigger frequency for the Synchronizer attached.
Rotation Speed (rpm/Hz)
When the Synchronizer starts to receive Index and Position Increment pulses the rotation
speed can be monitored here.
Cyclic repetition Speed (rpm/Hz)
This is monitor of cyclic repetition speed, which is calculated from the rotation speed shown
above.
Note: To update the above two status information, please click the mouse between two
devices in Synchronization Cables panel.
Internal Temperature
The internal temperature of the Synchronizer can be monitored here.
Triggering the Synchronizer
When triggering a synchronizer in PIV mode, the Laser is fired and an image (or double
image) is acquired. But the process of triggering a laser and camera takes time.
Below is a timing diagram of what happens when the synchronizer is triggered (See syn-
chronization in general for detailed information on this diagram):
As can bee seen above, the very first thing that is happening is the triggering of Flash lamp 1
in the laser. Usually the Flash pumped laser has to be pumped around 200 microseconds
before the Q-switch can be triggered and then light is emitted from the laser. In this example
this is the period that takes the longest time to prepare for an acquisition.
In these 200 microseconds the rotation continues, and therefore the actual acquisition will
not take place at the position specified!
Ex: To calculate the actual position at which the acquisition will take place lets try the following:
Rotation speed is 1000 RPM = 16.666 RPS => position speed 6000 deg/s
Flash to Q-switch delay is 200 micro seconds (which for this example is the time needed before
acquisition can take place):
200e-6 s * 6000 deg/s = 1.2 deg
Note: Have a look at the timing diagram for the Synchronizer to see what the actual delay
from an external Trigger to the laser is fired.
14.6.21 Using Two Synchronizers
In some applications it is necessary to use more than one synchronizer. This could be if one
synchronizer does not supply enough outputs to accommodate all devices needed or if some
devices in a system needs to run at a different speed than others.
For a description on how to setup a system using two synchronizers, refer to the help for the
specified synchronizer:
Timer Box
PAGE | 366
PAGE | 367
14.6.22 Pulse Receiver
In some applications Dynamicstudio needs to synchronize external equipment. In this case a
Pulse Receiver can be added to the Acquisition system to represent the external device.
Using the Pulse Receiver you can define synchronization pulses that the other equipment
need.
Adding the Pulse Receiver to the Acquisition system
The Pulse Receiver is added as any other device that is not automatically detected by the sys-
tem:
1.   Right click the Acquisition Agent under which the Pulse Receiver is to run
2.   Select "Add new Device..."
3.   From the list of devices select "Custom Synchronization -> Pulse Receiver", and the Pulse
receiver is added to the system
Using the Pulse Receiver
The following parameters are used to define a pulse train to the external equipment. The
description of the parameters refers to T0 and T1. T0 is defined to be the first light pulse from
the light source in the system. For furhter details regarding T0 and T1 please refer to "Syn-
chronization" on page?333
Activation time
All devices in the system calculates activation time relative to T0. If this device needs to be
activated (triggered) before T0 a negative time can be entered.
Activation time relation
If the system is running in Double Frame mode, it is possible to set the specified Activation
time relative to T0 or T1 (first or second light pulse). If the system is running in Single frame
mode the Activation time will always be relative to T0.
Activation pulse train
Here a pulse train for the external device is defined: The pulse train must always contain one
or more pairs of timing values, where each pair consists of a pulse width and a dead (off) time
following the pulse:
Pulse width 1; Dead time 1; Pulse width 2; Dead time 2;? ... ;Pulse width N; Dead time N.
To define one pulse with an active time of 10 micro seconds and a dead time of 100 micro
seconds, enter "10; 100".
To define a pulse train with first an active time of 10 micro seconds, then a dead time of 100
micro seconds, then an active time of 20 micro seconds and a dead time of 10 micro seconds,
enter "10; 100; 20; 10".
Activation signal polarity
Here the active time polarity is defined. If the active time is to be high select Positive, else
select Negative.
Sync. with TBP
If two pulses have been defined in the Activation pulse train, then selecting Yes for this prop-
erty will automatically place the second activation time relative to T1 (the second light pulse).
This ensures that with later changes in Time Between Pulses, the pulse train will automatically
be updated accordingly.
Sync at every Nth signal
Specifies how often the pulse is to be generated. If N = 1 then its every time. If N = 2 then
every second time, and so forth.
Start sync at Nth signal
Specifies when to start the generation of pulses. If N = 1 then start right away. If N = 2 then
start at second signal, and so forth
Examples:
Ex.1
Given the following input:
Will result in the following output for the external device:
Ex.2
Given the following input:
PAGE | 368
PAGE | 369
Will result in the following output for the external device:
14.7 Illumination  Systems
The DynamicStudio Imaging System supports many methods of producing stroboscopic light-
sheets; pulsed lasers, continuous wave lasers and electro-optical shutters are some typical
examples.
Note
The DynamicStudio Imaging System has the capability of supporting pulsed laser or a shutter
device combined with a continuous laser system.
Please read about "Laser Safety" (on page?29).
14.7.1 Brilliant B  laser
The Brilliant B laser is a powerful flash lamp pumped, Q-switched, single cavity Nd:YAG laser,
well suited for intensity based measurements. It can be used as a stand alone laser source
using the second, third or fourth harmonic for various applications such as soot diagnostics or
tracer-LIF. Optionally it can also be used as a pump laser source for a tunable dye laser such as
the TDL+ dye laser models. This approach is used for LIF measurements of e.g. combustion
species.
To learn more about the control of the TDL+ laser, please see "Tunable Dye Laser" on
page?377
The Brilliant B laser has the option of being controlled from either the DynamicStudio soft-
ware or a control pad. To control the laser from the software, the laser needs to be con-
nected to the PC with a serial cable (RS232) and the option has to be enabled from the control
pad. [If the PC does not have an RS232 port it is possible to use a USB-to-Serial converter com-
patible with the operating system].
After adding the Brilliant B laser to the Acquisition Agent, DynamicStudio will try to detect it.
The image below shows the detected laser in the device tree:
Even if the laser is not detected it is possible to fire the laser by connecting synchronization
cables to a synchronizer and setting up the laser using it's control pad.
When the laser is detected it is possible to monitor and control it from DynamicStudio using
the property editor.
The Q-Switch status can be controlled from the properties. This makes it easy to enable or dis-
able the Q-Switching during a preview while keeping the Flash lamp running.
When an interlock is tripped, a message box will pop up describing the problem. The problem
can also be seen in the properties of the Brilliant Control.
In the property help, at the bottom of the property editor, a description of each interlock can
be seen.
For more information on the Brilliant B laser, see the manual that came with it.
PAGE | 370
PAGE | 371
14.7.2 DualPower Lasers
Software Control
The flash pumped DualPower lasers have the option to be controlled through the Dynam-
icStudio software or a control pad. To control the laser via the software, the laser needs to
have its serial cable (RS232) attached to the PC. If the PC don't have an RS232 connector it is
possible to use a USB-to-Serial converter compatible with the operating system used.
To control a DualPower laser from DynamicStudio, just add the laser to the "Device tree" by
right-clicking the "Acquisition Agent" icon and select "Add New Device...". When added the
device tree will look like this:?
The DualPower device is specifying the laser trigger properties. These properties are used by
DynamicStudio to setup correct timing for acquisition.
The Laser control device is for controlling the laser.
When the Laser has been added it will automatically try to establish connection with the Laser
control. If it can't be found on the default COM port it will notify with a message where it is pos-
sible to disable the Laser control. This would generally be the case if the laser is to be con-
trolled by a control pad. The software control of the laser can also be disabled by right-clicking
the Laser control device and selecting "Disable".
Normal View
In "Normal View" the only property accessible is the laser energy level (for the DualPower 10-
1000,15-1000 and 20-1000 this property is called "Laser diode current"). This is in percent of
the maximum and controls the energy of the laser light.
Advanced and Expert View
In "Advanced" &?"Expert" view it is possible to change the communication port used to control
the laser. It is also possible to view the status of the laser (and see for example if water pump
is on, if any interlocks have been tripped, etc ...).
Tripped interlocks will in most cases prevent the laser from firing, so interlock status can be
useful for troubleshooting. For further information on the interlocks, please refer to the
laser operators handbook.
14.7.3 Time resolved lasers
For the DualPower 10-1000, 15-1000, 20-1000, 23-1000 and 30-1000 there are some extra
properties in advanced view mode:
PAGE | 372
PAGE | 373
Laser Repetition Rate: Used if the Trig 1 and Trig 2 are set to use the internal trigger.
RF On: The RF switch turns on or off the Radio Frequency generator for the Q-switches. This
must be on for pulsed operation of the laser. It is recommended to leave the RF switch con-
stant on (default) whenever the pump diodes are on. If the RF is switched off the laser will
emit continuous (CW) at unspecified power, and the Q-switch trigger will have no function.
Trig 1 Externally: If this is set to false the laser will be triggered with the internal repetition
rate.
Trig 2 Externally: If this is set to false the laser will be triggered with the internal repetition
rate.
14.7.4 Evergreen  Laser
The laser Evergreen laser can be control via connection from the PC COM port and the COM
port if the laser power supply (PSU).
Auto detection
The laser Evergreen PSU will auto detected by DynamicStudio. There is no need to setup Baud
rate stop bits and such, DynamicStudio will setup the COM port before trying to connect to
the laser.
When detected the following devices will be added beneth the Com port on which Dynam-
icStudio detected the laser:
Parameters
Status and Alarms
l  Fault
Default value is 'None'. If this parameter shows any other status than 'None' the Laser sys-
tem is malfunctioning
l  Warning
Default value is 'None'. If this parameter shows any other status than 'None' the Laser sys-
tem is malfunctioning
Settings
l  Laser 1 Energy
Select the energy level that Laser 1 should use.
l  Laser 2 Energy
Select the energy level that Laser 2 should use.
l  Alignment mode
Reduces the output energy to 20%
Device information and version
l  Full name
The devices’s full name/description
l  Serial number
Devices’s serial number
l  Driver software
The version of the driver software
l  Firmware version
The firmware version
l  Controller firmware version
Starting and stopping
Start
When clicking “Acquire” or “Preview” the following things will happen:
1. The laser Trigger source will be set to: External Flash and Expternal Q-Switch control
2. The Q-Switches will be enabled
3. The Trigger input impedance will be set to High impedance
4. Laser energy will be set as defined in properties for the laser
5. The laser will be set into Run mode
Stop
Stop of the system will do the following:
1. The laser will be put in to passive mode
2. The Q-Switches will be disabled
PAGE | 374
PAGE | 375
14.7.5 Lee Lasers
Products described on this page are all registered trademarks of Lee Laser Inc. For more
information visit http://www.leelaser.com.
14.7.6 Microstrobe
The microstrobe light source is a gated light source. By definition the gated light source is illu-
minating during the period of time that it is fed a pulse. The image below shows the relation
between the pulses fed to the light source, and the lighting that it emits.
It is possible to configure the two pulses independently from one another, with the exception
of the position of the pulses relative to time zero (shown in the image above as the dotted red
line) and the delay to close, which are shared settings.
The Activation Delay is the time duration from when the light source receives a pulse till it illu-
minates.
The sum of the Pulse Width and the Delay to Close is the time duration that the light source will
illuminate.
Example:
Activation delay: 1 us
Pulse width: 3 us
Delay to close: 1 us
Trigger polarity: positive
(the time bar unit is us)
14.7.7 New  Wave Lasers
Pegasus and Solo are trademark of New Wave? Research, Inc. For more information please
visit http://www.new-wave.com.
14.7.8 Pulsed Lasers
The synchronization part of the DynamicStudio Imaging System controls the generation of
trigger pulses. Two trigger pulses are generated for each laser cavity to fire a light pulse.
Some synchronization units also feature a failsafe watchdog timer, which can cut the laser
interlock to shut it down in case of system failure.
It is also recommended that you use the following accessories when working with the Nd:YAG
lasers
l  Laser goggles
l  Laser power meter
14.7.9 Dual Cavity pulsed lasers
When running Dual Cavity lasers in single frame mode you will in most cases have the pos-
sibility to use both cavities of the laser, making it possible to run the laser at the double fre-
quency as in Double frame mode.
This functionality has to be enabled in parameters for the laser. Below there is an example of
parameters for DualPower 132-15 Laser:
PAGE | 376
PAGE | 377
To use both cavities in single frame mode, enable the parameter "Use both cavities in single
frame mode".
14.7.10 Shutter Devices
Instead of a pulsed laser, a shutter device may be used. This shutter can be used together
with a continuous wave laser and the trigger signals from the synchronization unit of the
DynamicStudio Imaging System can be employed to open the shutter (and close it again after
a specified period of time).
14.7.11 Tunable Dye Laser
The TDL+ laser is a tunable dye laser, capable of producing laser radiation within a very wide
wavelength range from ultra-violet through visible light and up to infra-red. This laser needs
to be pumped by another laser source in order to emit laser light, and cannot be used as a
stand-alone laser. The most commonly used pump laser source for the TDL+ is the Brilliant B
Nd:YAG laser. This laser combination can be used for LIF measurements of e.g. combustion
species.
To learn more about the control of the Brilliant B laser, please see "Brilliant B laser" on
page?369"Brilliant B laser" on page?369
The TDL+ laser has the option of being controlled from either the DynamicStudio software or
a control pad. Both options will allow you to control the output laser wavelength and set a
wavelength scanning speed.
To control the laser from the software, the laser needs to be connected to the PC with a serial
cable (RS232). [If the PC does not have an RS232 port it is possible to use a USB-to-Serial con-
verter compatible with the operating system].
The TDL+ laser is added to the device tree by right clicking the laser source that pumps it and
selecting 'Add new device...'. When it has been added DynamicStudio will automatically try to
detect the laser.
The TDL+ can be controlled either from it's properties or from a dialog. The dialog is opened
by clicking the button (...) in the 'Wavelength control' parameter.
Parameter description
l  Current wavelength:
Shows the current output wavelength of the TDL+ laser.
l  Fundamental wavelength:
Shows the current fundamental wavelength using the selected UV extension scheme (see
below).
l  UV extension scheme:
The dye laser will typically emit visible light. If necessary this can be converted into UV light
in a number of ways, selectable by the user.
It is possible to select between 4 different schemes:
Not used: λf = λout.
Frequency doubling: λf = 2 x λout.
Frequency mixing: λf = 1 / (1 / λout - 1 / λIR).
Mixing after doubling: λf = 2 / (1 / λout - 1 / λIR).
λf : The wavelength (in nanometers) of the fundamental dye laser beam.
λout : The wavelength (in nanometers) of the output laser beam.
λIR : 1064 nm.
PAGE | 378
PAGE | 379
l  Stop:
Stops any ongoing movement in wavelength.
l  Go to wavelength:
Enter the wavelength that you want to go to as fast as possible (1 nm/s)
l  Scan start:
Enter the wavelength that you what a scan to start from.
l  Scan end:
Enter the wavelength that you what a scan to stop at.
l  Scan speed:
Select the speed of the fundamental wavelength that you want to use during a scan.
l  Start scan:
Starts a scan.
For more information on the TDL+ laser, see the manual that came with it.
Using the handheld controller while connected to DynamicStudio
If the TDL+ laser is operated via the handheld control pad while connected to DynamicStudio,
the properties are not updated. To update the properties press the stop button.
14.7.12 Quantel Q-smart laser
The quantell qsmart laser is a single cavity laser that runs at 10Hz only. The laser can be con-
trolled from within DynamicStudio.
Adding the laser to DynamicStudio Acquisition System
To add the laser to the acquisition setup right click the Acquisition Agent in the Device tree
under which you want the laser to be located, and select "Add New device...".
From the list of devices displayed in the dialog that pups up select the "Quantel Q-smart laser".
When the device is added it will be detected if it is possible to communicate with the laser, if
not this will be indicated as a warning in the device tree:
If the communication is in order the warning “<Not connected> will be removed.
Setting up communication with the laser
First thing that has to be done before DynamicStudio can communicate with the laser is to
setup the laser to accept command from the Ethernet port. This is done using the Qtouch
panel. Go to the configuration tap and click “Computer Control Control”:
The communication with the laser is done via the Ethernet connector on the laser. By default
the laser will have the IP address “169.254.0.1” and port number “10001”.
If for some reason it the IP address or port number has changed, change the IP address and
port number in DynamicStudio accordingly.
Having turned on the laser and the correct IP Address and port number has been entered
(the default value is usually correct) go to the parameters for the QSmart power supply and
find the entry “Settings”->”Connect” and click the “button down” that will appear when select-
ing the parameter:
Click Ok and the system will try to connect to the laser.
Note:To help find the correct IP address (and also to map the Ethernet port on the laser to a
virtual COM port) a tool named “CPR Manager” can be used. This utility is found on the
memory stick that comes with the laser.
Note:To help find the correct IP address (and also to map the Ethernet port on the laser to a
virtual COM port) a tool named “CPR Manager” can be used. This utility is found on the
memory stick that comes with the laser.
The first thing you will see is a login prompt. The default username and password are empty
strings.
After this you will see a page looking like this:
PAGE | 380
PAGE | 381
Using the menu entry to the left named “Network” it is possible to change the IP Address.
To change the port number use the menu item named “Connection”. In here change the “End-
point Configuration->Local Port” entry.
Laser Status  and Alarms
If a warning, fault or an alarm is reported from the laser it will be showed in the device tree
that the laser needs attention:
All laser status and alarms are reported in parameters for the laser:
Any warning, fault or alarm will be displayed here. When the issue has been resolved, it is pos-
sible to clear the fault, Alarm or Warning state by selecting the parameter “Clear” selecting
the down button that will appear and then click “Ok”.
NOTE: In the example above the manual shutter is closed. To fix this issue open the manual
shutter and clear the states.
14.8 Microscopes
14.8.1 Leica  M165FC microscope
Stereo Microscope Properties
Please refer to the Stereo Microscope Installation and User’s guide for the hardware install-
ation and cable connections. The Stereo Microscope is connected to the system controller (PC)
via a USB connection.
In the acquisition mode, the Stereo Microscope should be detected automatically in the
device tree and its device properties should be displayed:
PAGE | 382
PAGE | 383
Depending on the individual components of the system, the device tree may not contain
some of the items in the list.
System Magnification
The Microscope is consists of different components that introduce a magnification. Based on
the different components of the Microscope the full magnification will be calculated and made
visible to the user in a parameter named “Microscope Magnification”.
The full microscope magnification is:
Mtotal = MZoomDrive * MLED * MCameraAdapter * MMainObjective
Things on the Microscope that introduce a magnification is the following parts:
Camera Adaptor (magnification can be manually entered) LED CXI, (introduces a magnification
of 1.5X), zoom drive, main objective (magnification can be manually entered).
In order to calculate the exact FoV, DOF and the Correlation Depth the system magnification
must be known.
Field of View (FoV)
Field of view for the camera is calculated based on the following formula:
FieldOfView = lCamSensorWidth * lCamPitchX / M; lCamSensorHeight * lCamPitchY / M
Depth of Field
Depth of field is calculated based on the following formula:
Where:
n: index of refraction (input from the user)
lambda: wavelength (input from the user)
e: pixel pitch for the camera (depending on the camera connected with the microscope)
NA: Numerical aperture (NA= n sin (alpha)
M: total system magnification
Correlation Depth
Half the Correlation Ddepth is calculated based on the formular shown below
PAGE | 384
PAGE | 385
Where
? is image intensity ratio (input by the user)
n is index of refraction (input by the user)
dp is particle diameter (input by the user)
NA is Numerical aperture
M is System magnification
lambda is wavelength
Stereo Microscope Control
Clicking on the “…”under Focus Drive will bring up the Control Dialog for the Stereo Micro-
scope. Through this dialog it is possible to
l  Switch between relative and absolute position
l  Change the focus drive speed
l  Move the microscope up and down by clicking in the “control focus drive” area
l  Turn on and off the white CW LED illumination
l  Change the LED illumination intensity
l  And read the filter, iris and zoom settings
14.9 Traverse  systems
14.9.1 Traverse Control
The following traverse systems are supported by DynamicStudio:
l  "Isel" (on page?388)
l  "Tango" (on page?394)
l  "Thorlabs" on page?395
To interface with other traverse systems please contact Dantec Dynamics.
For troubleshooting click here.
Setting up the hardware
See the manual that came with the traverse system.
PAGE | 386
PAGE | 387
Setting up the Traverse Agent
To control a traverse system you need to set up a Traverse Agent. This is done from the menu
Tools->Configuration Wizard. The wizard will guide you through the process. The next time
you enter Acquisition mode in DynamicStudio the Traverse Agent is started automatically and
is visible from the Windows System Tray.
When entering Acquisition mode the Traverse Agent will initialize and by default load the Isel
traverse driver. A communication error message may appear if an Isel system is turned off,
not connected or if you have a different traverse system. Just ignore this error.
Loading the traverse driver
To load the driver right click the Traverse Agent icon and select the 'Show' item in the context
menu.
This opens the Traverse Agent dialog. From the menu File->Load Driver... it is possible to load
any traverse driver supported.
Drivers are using the following naming convention: Traverse.Driver.[traverse name].dll
Isel
When the driver is loading error messages may appear. Typical reasons for error messages
are:
l  The traverse system is not connected and turned on.
l  The traverse system is not connected to the default communication port of the driver (set-
ting the right communication port is part of).
Configuration
Configuration of the Isel driver is done through the 'Configuration' menu item shown in the
image above.
The different tabs in the configuration is described below.
COM port?
To be able to control the traverse, the driver needs to know which port the traverse system is
connected to.
Here it is possible to see all COM ports on the PC and their status. Select the one that the tra-
verse system is connected to.
Determining the COM port used can often be done by inspecting the connector on the PC (a
number near the connector).
It is possible to change the Baud rate. If the wrong Baud rate is used communication with the
traverse system will fail.
PAGE | 388
PAGE | 389
Axis Assignments
It is possible to assign and disable the physical axis.
Instead of rearranging the cables from the traverse controller to the axis it is possible to
assign the axis via software.
Example: If the axis assignment above causes the physical X-axis to move when the Y-coordin-
ate is changed in the software, then reassign 'Y pos' to axis #1. Apply similar tests for the
other axes.
Speed and Calibration
It is necessary to specify the speed at which each axis should move and also the number of
pulses/mm (the latter can be read from the physical traverse axis).
In this case axis 1 is set to a speed of 40 mm/s and a calibration factor of 80 pulses/mm. This
means that when moving this axis 1 mm, the traverse controller needs to give 80 pulses to
the axis motor and the motor receives 320 pulses/s.
If the calibration factor is set wrong, each movement will be wrong.
Ramp, specifying the acceleration ramp of the traverse start and stop movements, is only sup-
ported on a few controllers. (For example the iCM-S8 controller)
Software limits
It is possible to predefine a set of limits for each of the axes on the traverse system.
PAGE | 390
PAGE | 391
A minimum and a maximum position can be entered to ensure that the traverse keeps within
the specified bounds. It is also possible to disable this feature by placing a checkmark in 'Dis-
able limits'.
General?
In both the Traverse Agent and the Traverse Control dialogs there is a reset button. The oper-
ation of this button depends on the Reset mode.
There are two options:
l  Reset reference at current position.
The software will set the current physical traverse position to (x, y, z)=(0, 0, 0).
l  Move to home position and reset reference.
The software will move the traverse to its home position (home position is a physical
switch on each axis) and then set the position to (x, y, z)=(0, 0, 0).
l  Simulate reference movement.
The software will simulate a movement to the home position without actually moving the
traverse, and then set the position to (x, y, z)=(0, 0, 0). This option is only available for the
iMC-S8 ISEL controller.
Programming
Some Isel controllers have the ability to be programmed. If a program is loaded in to the Isel
Controller, then when starting Acquisition, the program will be executed.
The program is saved in a file with the extension ".out". This is a text file holding the instruc-
tion that are executed when the program is running.
Below is an example on such a program with some description:
PAGE | 392
PAGE | 393
Code                                                                                 Description
p0,128,0                                                                Turn off LED's
71                                                                           move to reference position (home)
m10000,5000                                                      move to start position in steps, and with a
speed in Hz
n1                                                                           reset position to 0
j300                                                                        set start/stop frequency to 300Hz
J80                                                                          acceleration forward in Hz/ms
p0,128,1                                                                Turn on LED 1 turn off LED 2
m16021,12000                                                    move forward to end position
p0,128,2                                                                Turn off LED 1 and turn on LED 2
m0,23535                                                             move backward to reset position
3 20,-4                                                                   loop the last 4 lines 20 times
p0,128,0                                                                Turn off LED's
9                                                                              end programming
For details on the programming of the Isel controller please refer to the user manual for the
Isel controller.
To load the program enter the full path for the program, or click Browse and selected the file
by browsing to the right file to load.
Click Load and Start Program to load the program in to the Traverse controller.
NOTE:?If a valid program is loaded into the controller, the program will be executed when
starting an acquisition in DynamicStudio.
To make sure that no program is loaded in to the Traverse controller click "Unload Program"
Tango
To setup the Tango traverse driver see the manual on the CD that came with the system.
The Tango integration is using RS-232 communication. Use COM1 on the PC to communicate
with the Tango Traverse.
Note
If COM1 is used for another purpose it is possible to change the port used for communicating
with the traverse controller. This is done in Registry:
Run the program Regedit.exe. Find the following node: HKEY_CURRENT_
USER\SOFTWARE\Dantec Dynamics\DynamicStudio\Traverse\Tango. If the string value "Port"
does not exist, add the string value. Set the string value "Port" to "COMX" where the 'X' is the
port number you want to use for the Tango Traverse. Example: "COM2".
Note
For microscopic measurements, when pressing the home button, the motor used for focus-
ing will run for a while because there is no stop switch on that axis and by definition the home
position is the switches on the axis.
PAGE | 394
PAGE | 395
Thorlabs
The driver is designed to handle different types of Thorlabs traverse controllers:
l  KDC101: Brushed DC Servo Motors Traverse Controller
l  KBD101: Brushless DC Servo Motors Traverse Controller
l  TDC001: DC Servo Motor Motors Traverse Controller (Legacy)
l  TBD001: Brushless DC Servo Mothres Traverse Controller
Configuration Thorlabs traverse controllers
setting up the Thorlabs controllers is done by opening the Configuration for the controllers.
Click File->Configuration... and the following dialog will appear:
If more stages are connected then the lowest serial number will be listed first.
Note:?Please do not enable more stages than what is available. If only one state is available
then enable the first and disable the two next. If two stages are available then enable the two
first stages then disable the third.
Stage scale
In order for the controller to able to move the axis or stages correctly a scale value has to be
entered. See below:
Brushed DC Controller (TDC001 and KDC101) driven stages:
l  MTS25-Z8: Stage scale = 34304
l  MTS50-Z8: Stage scale: 34304
l  PRM1-Z8: Stage scale: 1919.64
l  Z8xx: Stage scale: 34304
l  Z6xx: Stage scale: 24600
Brushless DC Controller (TBD001 and KBD101) driven stages:
l  DDSM50: Stage scale: 2000
l  DDSM100: Stage scale: 2000
l  DDS220: Stage scale: 20000
l  DDS300: Stage scale: 20000
l  DDS600: Stage scale: 20000
l  MLS203: Stage scale: 20000
Rotary stages:
l  DDR100: Stage scale: 3276800
l  DDR05: Stage scale: 2000000
Controlling the traverse
There are tree ways to control any supported traverse system.
l  Controlling directly from the Traverse Agent.
l  Controlling from DynamicStudio Traverse Control.
l  Controlling from DynamicStudio Acquisition Manager.
Controlling directly from the Traverse Agent
Controlling the traverse system from the Traverse Agent is usually just to see that the con-
nection is working and every thing is set up and configured correctly.
PAGE | 396
PAGE | 397
l  Read button
Reads the current position of the traverse. If the position of the axis has been manually
moved without the use of the software then the position will not be correct.
l  Move button
Moves the traverse to the position specified in the number fields on the left of the dialog.
l  Stop button
Stops an ongoing movement of the traverse.
l  Reset button
Resets the control and traverse system. The functionality of this button depend on the Tra-
verse system used and the way its driver is configured (See Reset mode of the Isel driver
as an example).
l  Home button
Moves the traverse to its home position which is defined to be the position of the physical
stop switches on each axis.
l  Invert Axis
Inverts the position that you enter around 0
Controlling from DynamicStudio Traverse Control
To control the traverse system from the Traverse Control in DynamicStudio you need to
setup DynamicStudio to automatically load the Traverse Agent, See Setting up the software.
After setting up DynamicStudio and Acquisition mode has been entered, the Traverse Agent
can be seen from the Device tree.
From the properties of the Traverse Agent it is possible to specify the traverse driver either
by selecting from a list or by entering the path and name to the driver.
The Traverse Control can be found in the menu Run->Traverse Control.
Number fields and buttons correspond to the ones in the Traverse Agent, see Controlling dir-
ectly from the Traverse Agent.
Controlling from DynamicStudio Acquisition Manager
The Acquisition Manager can be found in the menu Run->Acquisition Manager. See Acquisition
Manager for more info.
Troubleshooting
Problem                                                                                   Action
When loading the driver an error
message comes up
1.   Check that the Traverse system is turned on
and connected to the PC.
PAGE | 398
PAGE | 399
Problem                                                                                   Action
2.   Check that the right communication port has
been selected in the configuration of the
driver.
For the Isel driver see COM port.
For the Tango driver see the manual that came
on the CD with the system.
Traverse system does not respond
to any commands from the Tra-
verse Agent
Using the Tango traverse. Pressing
Home button seems to freeze the
software
The motor used for focusing will run for a while
because there is no stop switch on that axis and by
definition the home position is the switches on the axis.
Wait for the motor to stop.
When pressing the Move button
the traverse does not move
1.   Check that none of the axis on the traverse
have overshot the home switches. If so, manu-
ally move the position to the other side of the
home switch.
2.   When using the iMC-S8 ISEL Controller, remem-
ber to move to home before moving the axis.
Press the Reset button and ensure that the
Reset Mode is set to Home or Simulate Home.
Axis movement does not cor-
respond to the entered value
From the configuration of the traverse driver make
sure that the right calibration constants have been
entered.
The traverse system does not
respond to any commands from
DynamicStudio
1.   Make sure that the Traverse Agent and the
right traverse driver is loaded. See Setting up
the software for more info.
2.   Leave the Acquisition mode and exit the Tra-
verse Agent from it's context menu and enter
the Acquisition mode again. This will start the
agent again.
3.   Power the traverse system off and back on.
4.   Restart DynamicStudio.
The wrong axis is moving when
pressing the Move button
1.   In the configuration of the traverse driver
check that the axis assignment is correct. For
the Isel driver see Axis Assignments.
2.   Rearrange the physical connectors.
Controlling the traverse
There are tree ways to control any supported traverse system.
l  Controlling directly from the Traverse Agent.
14.10 Other  Devices
Various other devices for specific purposes.
14.10.1 Active Target
Active target is used for creating camera calibrations directly from the acquisition system. The
Active target consists of LED's are arranged in a grid. The LED's can be turned on individual,
this is used to encode the position of the LED on the target. In order to do this a number of
images needs to be acquired for each target position.
One major advantage is that not all markers needs to be visible on the acquired images. If
only parts of the target markers are visible the system will still be able to determine the
marker location.
Other aspects of the Active target are:
l  The active Target can be stitched together to form a larger target.
l  The Active Target is waterproof and can therefor be submerged .
l  Also a PC screen version of the Active Target exists.
Active Target Traverse
Installation of Drivers
The driver is installed together with DynamicStudio.
Detection
The Active Target Traverse is detected when connected to the PC via one of the USB ports.
The system adds a DevicesHandler to the system. In the Device Tree the device will look like
this:
Parameters for Active Target Traverse
Parameter description:
PAGE | 400
PAGE | 401
Name                                                             Description
Stroke length                                      Describes the max stroke length of the traverse
Move to position                                Entering a number in mm will make the Travers move to
the position specified. Outer most position will be 0 ref-
erence and going positive towards inner most position
Move to Reference                            position Moves the traverse to the reference position.
The reference position is 0.0mm. Movements from here
can be -4mm to 4mm
Serial number                                     Device's serial number
Device's serial number                      Driver software The version of the driver software
When ‘Move to reference position’ is needed.
After having performed a power cycle on the traverse it is important to move the traverse to
the reference position.
Leaving the power to the traverse turned on (even when the PC is shut down) the traverse
will remember the position at which it was left.
The software will check if a move to reference position is needed by checking if the traverse
electronics has been initialized. If not so the string “Move to reference position is needed!” is
written into parameter ‘Move to Reference position’.
If a movement of the traverse is initiated while ‘Move to Reference position’ the system will ini-
tiate a move to reference position and then move to the new position.
Physical specifications
Position to position accuracy: +-0.02 mm
Traverse force: moves easily 16Kg weight up and down
Approx. 3840.6417 full steps/rotation
Approx. 989363.63 micro-steps/rotation.
One rotation is 1mm movement of the piston.
Active PC Screen Target
Adding the device
Adding the target is done like adding a laser to the system. Right click the Acquisition Agent in
the Device list dialog and select “Add new device”. The following dialog will appear:
Select the “Active Target for Calibration usding a PC creen” and click OK
(Active PC Streen Target can not be added when a physical Active target is already part of the
system)
When the system adds the devicehandler for handling the PC screen target the system will
also add the device “Active Target Control”:
Parameters for Active PC Screen Target
Parameter description:
PAGE | 402
PAGE | 403
Name                                                             Description
Physical Size                                         The physical size of the dot grid
Marker grid size                                 Defines the number of markers to generate on the
screen
Marker Size                                         Defines the size of the marker in pixels. A size of 3 will
make markers that are 3x3 pixels in size.
Measured Physical Size                      Measure the and enter the Physical size of the dot grid
drawn on the scren. The system will then update 'Screen
Pixel Pitch' according to the new value entered and
redraw the dot grid.
Screen Pixel Pitch                               Here the pixel pitch can be set in μm. If the size of the
dot grid drawn on the screen is not correct pixel, pitch
should be correct.
The Target drawn on the Screen using the above parameters:
How to setup Screen Pixel Pitch
Measure the distance from the lower left dot on the screen target to the lower right and the
upper left.
Enter the measured distance in “Measured Physical size”.
The system will now correct Screen Pixel Pitch to fit the entered measured value.
Active Target (the physical target)
When connecting the physical target to the system the Target will be detected and added to
the system.
When the system adds the devicehandler for handling the Active Target the system will also
add the device “Active Target Control”:
(the Active target will act as a comport which will be added first and beneath this device the
Active Target will appear)
Parameters for Active PC Screen Target
Parameter description:
Name                                                             Description
Marker grid size                                 Number of markers on the target
Marker distance                                 The marker distance
Marker diameter                                The marker diameter
Status                                                   Shows the status of the Active Target
Led intensity                                       Led intensity in % (0-100)
Target selftest and Identification
When the target is connected it will do a self test. This is is seen as each row on the target is
turned on one by one. When the self test is done the target will show the target index drawn
withe dots.
Active Target Control
The first time a target is connected to the System Active Target Control is active as well. The
role of the Active Target Control is as it name says, to control the targets connected to the sys-
tem during acquisition of images to do a calibration.
The device handler will appear in Device list as shown below:
PAGE | 404
PAGE | 405
Parameters for Active Target Control
Parameter description:
Name                                                             Description
Cameras focused on Target             Lists the cameras, which are focused on the target. When
clicking on one (…) a dialog will be presented in where it is
possible to select cameras.(see example below)
Do calibration                                      Enable: a calibration will be performed and saved to the
database. Disable: no calibration will be performed. (This
can be done afterwards with Multi camera Calibration) (In
both cases the acquired image data can be saved to the
database)
Imaging model                                   Pinhole, 3rd order XYZ-polynomial, or Direct Linear
Transform
Target positions                                 Holds the number of target movements. (When chan-
ging this number, the number of images in system con-
trol will change to fit the number of images needed to
perform a calibration.)
Traverse Positions                             Here it will be possible to enter positions for the target in
the Z directions. If an Active Target Traverse is con-
nected the movement will be handled by the traverse. It
is possible to enter one position for each Target Position.
These z positions will be used during “3rd order XYZ-poly-
nomial” and “Direct Linear Transform” calibration.
Use Reference                                    Dot Disable: 0.0 will be the center of the total target.
Enable: Select to use a specific dot on the target to be the
reference point specified in 'Reference Dot' to be the
0.0.
Reference Dot                                    Reference Dot defines where on the total target 0,0 (tar-
get origin) is located. (Use Define Axis to move the ref-
Name                                                             Description
erence to a user defined point I space)
(hidden if ‘Use Reference Dot’ is disabled.)
Show Reference dot                          Defines if all dots are displayed during 'Free Run' or 'Pre-
view'. Disable: All dots will turn on. Enable: Only the ref-
erence dot will be displayed.
(hidden if ‘Use Reference Dot’ is disabled.)
Targets relative position                   Will hold the value button with ”…”. When activated a dia-
log will appear where it is possible to position 2 or more
targets relative to one another (See example of the dia-
log below)
Physical Size                                         The size of the total target (all targets included and taking
in to account how the targets are arranged).
Dot grid size                                        The total Dot grid size (all targets included and taking in
to account how the targets are arranged)
Example of the Camera selection dialog:
Example of the Target relative position dialog:
Relation between Active Target Control and Synchronizer
A connection between the Synchronizer and the Active Target Control has to be made in Syn-
chronization cables diagram:
PAGE | 406
PAGE | 407
This connection is not a physical cable but used to tell the system which synchronizer to use
during the acquisition for the calibration.
When active Target Control is created it will seek out the first Synchronizer and try to make
this connection automatically.
If more Synchronizers is in the system then the user must check that the right Synchronizer
has been chosen.
working with the Active Target to get a calibration
In parameters for the Active Target Control -> Cameras focused on target, add all cameras
that need to be calibrated.
Lining up the Active target with the ligth sheet of the laser is performed using the dot’s and
mirror on the side of the target. This is harder using the PC screen target, here the best fit
has to be done.
Synchronization cables diagram
The “Active target control” will be presented on in “Synchronization cables diagram”.
The “Active target control” will have a trigger output named “Software trigger output” that
must be connected to a new trigger input on the Synchronizer named “Software trigger
input”.
The cable is not a physical cable but an information to the system which Synchronizer to use
during the acquisition of the calibration images.
The connection will be created automatically.
Free run / Preview to focus on target.
All LEDs on the target will be turned on. In this mode it will be possible to focus the camera on
to the target.
A reference dot can be selected and shown on the target. This is useful if the position of the
target is to fit a custom location.
If Preview is selected the given trigger rate will be used to trigger the camera.
If an “Active Target Traverse” is present - It is important to move the traverse to its outer pos-
ition to check focus.
Acquisition
Number of target positions and Traverse positions are entered.
During acquisition, the LEDs on the target will be turned on and off from picture to picture,
this to encode the position of each LED.
The predetermined number of images (depending on number of LED’s, number of target pos-
itions) will be acquired.
If an “Active Target Traverse” is present movement of the target is done automatically.
If no “Active Target Traverse” is present a dialog will be displayed telling the user to move the
target to the next position ? for Pinhole calibration the movement should also be to a new tilt
position.
When the acquisition of images has been done, and if “Do calibration” is enabled, a calibration
for each camera selected in “Cameras focused on target” will be created and saved to the data-
base.
The user can choose to save the acquired data to the database and perform calibration using
“Multi Camera Calibration”.
Information on image type and position is saved together with each image.
Note:?Remember to save the setup to the database. This is done in System Control clicking the
"Save" icon.
Having saved the setup to the database is possible to later load the setup back in to the acquis-
ition system and perform a calibration again .
14.10.2 DualScope
The DualScope is a optomechanical device to be mounted on a camera lens. It will provide two
separate views of the same scene projected onto separate halves of the camera sensor.
Optionally a separate filter can be used for each of the two views allowing you to use a single
camera to measure simultaneously at different wavelengths.
Adjustable mirrors inside the DualScope allows you to align the two views for optimal overlap.
PAGE | 408
PAGE | 409
DynamicStudio can split the images immediately after acquisition to make them appear as if
they had been acquired by two separate cameras.
Select 'Image Format' for the camera in the device list and locate 'Split Image' in the Device
Property List:
l  'Vertical Split' will separate left and right halves of the image.
l  'Horizontal Split' will separate upper and lower halves of the image.
Camera images that are split in two like this will be stored as Linked Ensembles, meaning that
analysis and display are synchronized.
As long as the ensembles remain linked displaying one will automatically display the other also
and zooming, panning etc in one display will synchronously apply to the other as well.
Visualization of vapor and liquid phase of a gasoline spray using DualScope. Courtesy of
Chalmers University of Technology.
In the example above DualScope has been used in an Exciplex experiment on a fuel injector.
The left image is the vapor phase of the fuel, the right image shows the liquid phase. Both
images are acquired simultaneously by the same camera and split in two by the acquisition sys-
tem. Ensemble linking ensures that if you zoom in on an area of interest in one image the
other image will synchronously zoom in on the same area.
Read more about "Ensemble Linking" on page?97.
PAGE | 410
PAGE | 411
15 Analysis
After images and related data is acquired an Analysis can be applied, selecting Analyze... from
the menu. DynamicStudio includes a wide range of common analysis for image and vector pro-
cessing, LIF, particle sizing, spray diagnostics etc.
Note
The different analysis methods available are dependent on the add-on' enabled and installed
on your system.
15.1 Analysis  Sequence
An Analysis Sequence is a combination of different analysis steps. The sequence can be
applied to an ensemble just like a normal individual analysis.
All the individual steps in the sequence is executed until the end-result is achieved.
15.1.1 "Pickup"  an  Analysis Sequence
Before creating an Analysis Sequence all the individual analysis steps must be added and
executed manually. When settings in each individual analysis is adjusted to give the expected
end-result, the analysis Analysis sequences can be picked up for later use. This is done in the
following way: Select all results in the sequence you just created (Hold down 'Ctrl' key and click
the results). Right click on one of the steps in the sequence and select "Add Analysis Sequence
to Sequence Library" from the context menu.
When you select "Add Analysis Sequence to Sequence Library" the dialog "Add Analysis
Sequence" will appear. Here the sequence is displayed as well as all external dependencies.
External dependencies are other analysis results needed for the sequence to be executed. All
internal dependencies are handled by the Analysis Sequence itself . You can view any depend-
encies that any single analysis step has by clicking in the analysis step. You can also see which
analysis step requires what external dependency by clicking on any of the external depend-
encies.
15.2 Analysis  Sequence  Library
When an Analysis Sequence is picked up it is placed in the Sequence Library. This library holds
all previously defined Analysis Sequences. The sequence library is global and is available from
all databases.
When you want to apply an Analysis Sequence to your data, select Apply Analysis Sequence
from the Context menu, and select the sequence from the library. Please note that you can
only apply a sequence if the parent data matches, meaning f.ex. that if you have defined a
sequence starting with a single frame, you will only be able to apply this on single frames.
From the library you are able to manage your sequences: Each sequence can be given a
name, and description, you can also remove unneeded sequences.
Duplicate an Analysis  Sequence
It is possible to duplicate an analysis sequence. This is done by selecting the sequence that you
want to duplicate and clicking the Duplicate button, alternatively click Ctrl+D or right click- the
sequence that you want to duplicate and select "Duplicate" in the context menu that appears.
The name of the duplicated sequence will be "copy of " + the original name.
Import/export an Analysis  Sequence
The analysis sequence can be exported to and imported from a .sequence file .
Rename an Analysis  Sequence
It is possible to rename an analysis sequence. This is done by selecting the sequence that you
want to rename and Clicking the Rename button, pressing F2 or by right clicking the analysis
sequence and selecting "Rename". The sequence name will now be editable.
Changing the description of an Analysis  Sequence
You can change the description of an analysis sequence. This is done by clicking in the Descrip-
tion text box and editing the contents. Once the content is changed, press the Tab button, or
click elsewhere in the dialog. You will be asked if you really want to change the contents or
not, and if you click yes the changes are saved.
Change recipe setting for a method in an Analysis  Sequence
For many Analysis Methods in an Analysis Sequence it is possible to change recipe settings.
To change the recipe for a method in a sequence, select the method in the sequence tree. If
recipe settings can be changed the button “Show Recipe” will be enabled, click it to see and
possibly modify the recipe settings of the analysis method.When done, click "OK" and you will
be asked if you really want to save the changes to the analysis sequence or not.
15.3 Using  Analysis  Sequences
When a Analysis Sequence is to be applied, select the ensemble from where the analysis is to
take place. This ensemble is referred to as the "root node" of the sequence. If the sequence
has multiple root nodes select these as Input to Analysis. All external dependencies must be
selected as Input to Analysis before you right click and choose "Apply Analysis Sequence...".
At that point you will have the dialog "Analysis Sequence Library" displayed. Here you can
choose the Analysis sequence that you want to apply. When you select a sequence the
sequence will be displayed in the Sequence view and all the external dependencies for the
sequence will be displayed in the external dependencies view.
If some external dependencies are missing, they will be marked with ">>> MISSING <<<" and
it will not be possible to start the execution of the sequence. You will have to leave the
sequence library and make sure that all external dependencies are selected.
PAGE | 412
PAGE | 413
You can examine what dependencies an Analysis step requires by clicking on an Analysis step.
By clicking on any of the external dependencies you will have the analysis step highlighted
that requires this external dependency. External dependencies and the root nodes will also
be highlighted in the database view. By clicking any of the root nodes in the sequence you can
see where in the database the analysis will start.
In some cases it is necessary to change the default association of external dependencies or
root nodes with the Input for Analysis. This is done by right clicking the root node or the
external dependency. A list will be shown from where you can choose the right input for ana-
lysis.
15.4 Predefined  Analysis  Sequences
A number of Predefined Analysis Sequences exists. The only difference between predefined
and user defined analysis sequences is that the predefined sequences can not be deleted.
The Predefined Analysis Sequences are marked with "(predefined)" at the end of the
sequence name.
15.4.1 Use of Analysis Sequences
Analysis Sequences can be applied in many useful situations, for example:
Applying the same analyses  to many  ensembles  (Batch processing)
You have made a lot of measurements, and saved the data in multiple ensembles. Use the
first ensemble of data to experiment with your analysis settings until you get the result you
need. Then save the sequence of analyses and re-apply this to all the remaining measurement
ensembles.
Batch processing is however not possible if the Sequence has multiple root nodes. Instead if
you have the Add-on "Distributed Analysis" you can add one sequence at a time to the job
queue and continue to the next.
Frequently  used results
If you repeatedly is looking for the same results in your data, you can save a sequence of ana-
lyses that specifically finds your answer. Since the sequence can be built up using all com-
binations of existing anayses, you can adapt your sequence to return precisely the result you
seek.
15.5 Context menu
A context menu is a menu that pups up when either right clicking an object or at the time an
object is selected and clicking on the Application key on the keyboard. The menu offers a list
of options which vary depending on the context of the action, and the item selected.
15.6 Distributed  Analysis
Distributed Analysis is a way to reduce time spent analyzing data by utilizing a network of PC's .
Each PC will receive a dataset and calculate a result. This means that if 4 PC's were used for ana-
lysis, then in theory all datasets of an ensemble will be calculated 4 times faster than if only
one PC was used. There is, of course, some overhead involved in transferring datasets over
the network.
Distributed analysis has a close relation to the distributed database capabilities of Dynam-
icStudio since by default results are saved locally where they have been calculated.
15.6.1 Installing Analysis Agent software
A PC that is to be used during distributed analysis must have DynamicStudio installed.
It is not necessary to install a dongle on the remote agent, the system will always use the
dongle installed on the DynamicStudio PC.
15.6.2 Configuring Distributed Analysis.
Before distributed analysis is to be used the system has to know which PC's to use during ana-
lysis.
To tell DynamicStudio which PC's to use, start up Agent Host as described in "Installing Analysis
Agent software" on each of the PCs. Then startup DynamicStudio and from the Tools menu
select "Configuration Wizard".
The Configuration Wizard will guide you through the selection of Analysis Agents to be used.
Note that it is not possible to disable the local Analysis Agent.
Options  for Distributed Analysis
Options for Distributed analysis can be found in the Options dialog. To have the Option dialog
displayed select Options form the Tools menu.
The tab Analysis of the Options dialog holds four checkboxes related to Distributed Analysis:
l  Use Distributed Analysis (when checked enables distributed analysis)
As described in the dialog this enables you to queue up analysis jobs. If you remove the
checkmark the analysis will be carried out immediately using the traditional analysis
engine.
Distributed Analysis can also be enabled/disabled in then Select Analysis Method dialog.
l  Close Pending Analysis  Jobs  dialog on all jobs  done(default checked)
This lets you decide if the 'Pending Analysis Jobs' dialog is to be close automatically when
all jobs has been done.
l  Start Analysis  jobs  Automatically (default checked)
This lets you decide if the execution of the job queue is to be started automatically.
If you uncheck this option you must manually start the execution of the analysis jobs
queue in the Pending Analysis jobs Dialog.
l  Local Agent Only (default unchecked)
This lets you decide if analysis is to be assigned to the local agent only.
l  Save Analyzed data in Main Database(default unchecked)
By default all results are saved on the PC where analysis took place. This setting lets you
decide if you want all remote agents to save results in the main database.
PAGE | 414
PAGE | 415
(Note: It is possible to specify for each Analysis Agent if results are to be saved distributed
or in the main database. This is done in the Pending Analysis Jobs dialog.)
15.6.3 Analyzing using Distributed Analysis
When distributed analysis is enabled all analysis is added to a queue of analysis jobs. This job
queue is accessed via the "Pending Analysis Jobs" dialog. The "Pending Analysis Jobs" can be
opened from the Analysis menu by selecting "Analysis Jobs..."
Pending Analysis  Jobs
The dialog is divided in to three tabs:
l  Analysis  Jobs
This tab displays the content of the job queue. It is possible to start/stop the execution of
the job queue and it is also possible to remove individual jobs as well as clearing the entire
job queue. (Note: it is not possible to remove a job that is being analyzed).
After having started the execution of the jobs in the queue, and after the first job is fin-
ished the system tries to give an estimate on how long time it will take to do the rest of
the jobs in the queue. This estimate is presented just below the job content.
l  Job progress
This tab displays the overall progress of the current job being analyzed. Also, after the
very first result is saved, the system tries to estimate the remaning time.
The job is split in to several Tasks. Each Task represent the analysis of one dataset in the
ensemble. The progress of each Task is also displayed.
Some methods are implemented to utilize all CPU?cores found on the PC, and some are
not. In the case a method is not implemented to utilize all CPU?cores, Distributed Analysis
will instantiate one analysis Task for each CPU that will run in parallel.
Below are two examples of Distributed Analysis. The first one (on the left) is a display a single
task running on several CPU cores. This task can run on several cores thanks to a specific imple-
mentation in DynamicStudio. The second example (on the right) is a display of a job split in mul-
tiple tasks that are running in parallel.
l  Analysis  Agents
This tab holds information about the Analysis Agent connected. Each Analysis Agent has a
PAGE | 416
PAGE | 417
row in the table shown. There are four columns :
l  Host
The host name of the agent's PC.
l  Status
The connection status of the agent. If for whatever reason an agent disconnects you can
attempt to reconnect by clicking on 'Connect Agent'.
l  Save remote
A checkmark that can change whether to save analysis results. If checked the Analysis
agent will save the result at the PC where it is running. If not checked the result will be
saved in the main PC.
15.7 Distributed  Database
DynamicStudio Distributed Database has the ability to manage remotely stored data. A dis-
tributed database is a database where part of its contents is not saved at one single PC but is
spread over a number of PC's.
There are two ways a database can end up holding remote data:
l  a Distributed Acquisition agent saves acquired data locally
l  a Distributed Analysis agent analysis results saves data locally
If a database holds remote data it is visualized in the database tree. The example below shows
a part of a database tree where one ensemble has remote content. Ensembles with remote
data are marked with a globe.
The globe indicates that all or parts of the ensemble is stored remotely.
If for some reason it is not possible to get access to parts of the ensemble the globe will
change color:
l  Yellow indicates that one ore more remote PC's are not reachable.
l  Red indicates that all remote PC's are not reachable.
The example below show a database where all the remote PC's that hold part of a ensemble is
not reachable.
To get an overview of remote PC's involved in a distributed database, select the root icon of
the database tree and examine record properties of the section Distributed information. In
the example below the remote PC 'tst_xp' is involved. This is indicated in the 'Remote Agents'
property. If one ore more PC's is not reachable they are be listed at 'Unreachable Agents'
15.7.1 Collecting remotely stored data
Remote contents can be moved to the main database. Hereby we can ensure that all data is
stored in one location. Collecting remote data is done from the context menu of the root, a
branch or a single ensemble in the database tree by selecting "Collect remote contents to
main database".
For example, right clicking on a project of the database tree and selecting "Collect remote con-
tents to main database" will collect all remotely stored data, that is included in that specific pro-
ject. "Collect remote contents to main database (Recursive)" will also collect all child records of
the remotely stored data.
All remote data will be copied to the main database and then deleted at the remote PC. When
all remote data has been collected, the globe disappears from the database view.
15.7.2 Troubleshooting Distributed Database
There can be a number of reasons why it is not possible to get in contact whit a remote PC.
Here is some guide lines as to how to solve some of these issues:
l  Check that AgentHost is started on the remote PC (if AgentHost is running it has a notify
icon in the system tray). AgentHost can be started by clicking Start->All programs->Dantec
Dynamics->Remote Agent for DynamicStudio.
l  Network changes. Check that the PC's involved are located on the same subnet.
l  Firewall and Firewall settings on the PC's involved. During installation a number of firewall
exception are added to the Windows Firewall exception list. If these exceptions have been
deleted or another Firewall tool is used, it might not be possible to reach remote PC's. For
experienced users and administrators the exceptions can manually be added. You can also
choose to reinstall DynamicStudio. Finally you can choose to disable the firewall (not
recommended).
PAGE | 418
PAGE | 419
15.8 Custom  Properties
Every record in the database; projects, runs, ensembles and datasets contains properties. The
Record Properties can be displayed in the Record Properties view by selecting Alt+1. The prop-
erties are a collection of fixed properties defined and maintained by the system, and custom
properties which can be added dynamically.
Custom Properties can be manually added by selecting Custom Properties... in the context
menu on the record. Datasets automatically inherits the custom properties of the parent
ensemble, but the values of the properties can be changed individually.
15.8.1 Example
Custom properties are used in a number of situations. During calibration of LIF images cus-
tom properties can be used to store condition values like temperature or pressure. Another
example is the Sort option which stores the sort value as a custom property.
15.9 Timestamp
Timestamp or acquisition time is an indication of when data was acquired. DynamicStudio
provides a very precise way of determining exactly when images and analog data is acquired
using hardware time stamping. The timestamp is assigned every acquired dataset, and is if
possible, maintained and transferred through the analyses.
15.10 Selection  (Input to Analysis)
Selection are indicated in the database with a check icon    .
A Selection is a way to mark one or more external ensembles as additional inputs to an Ana-
lysis. Some analysis methods requires that not only the parent data is available, but also a ref-
erence to an external dataset.
A Selection in the database tree is made by selecting "Select Toggle" in the context menu, or
by pressing the Space bar on the ensemble.
Un-selection is done similarly to select , but you also have the possibility to click        Unselect in
the tool bar.
15.10.1 Example
One example is applying a mask to a series of images. The mask is created using the Define
Mask analysis method, and the later applied to the images using the Masking Image analysis
method. In this case the ensemble containing the mask must be selected as user selection for
the masking routine to work.
15.11 Fixed  Selection  (Input to Analysis)
Fixed Selection are indicated in the database with a check icon    .
Fixed Selection is as normal Selection(see above). The difference is how the selection and un-
selection is performed.
A Selection in the database tree is made by selecting "Select Fixed Toggle" in the context
menu, or by holding down the CTRL key and pressing the Space bar on the ensemble.
Normally when clicking        Unselect all selected record will be unselected, but not the Fixed
Selected records. To unselect Fixed Selected record bye clicking        Unselect, you must hold
down the Ctrl key.
The advantage using Fixed Selection is (as the name described fixed) meaning that the normal
way of clearing selection does not influence the Fixed selected records. This means that if a
database holds records that are use frequently you can chose to Fix select these records, and
they will stay selected until manually un-selecting these records.
PAGE | 420
PAGE | 421
16 Analysis methods
After images and related data is acquired an Analysis can be applied, selecting Analyze... from
the menu. DynamicStudio includes a wide range of common analysis for image and vector pro-
cessing, LIF, particle sizing, spray diagnostics etc.
You can access online analysis help and documentation from DynamicStudio.
1.   Select a raw or processed data set or ensemble in the database tree
2.   From the context menu select analysis
3.   Press the help icon in the Recipe dialog
Note
The number of analysis methods available depend on the add-ons enabled and installed on
your system.
16.1 2D  Least squares  matching  (LSM)
2D Least Squares matching (LSM) is a method for determining 2D velocity fields in highly
seeded flows in water and air. The input data consists of double-frame images or of time-
resolved single-frame images and the output data are equally spaced vector fields. Inter-
rogation areas from within the image are analyzed to determine local affine transformations.
16.1.1 Background information
The fundamental theorem of Helmholtz states that every infinitesimal motion of a fluid ele-
ment can be decomposed in translation, rotation and deformation. In the last decades several
investigations have been performed to experimentally describe these fluid motions. In clas-
sical PIV based on correlation techniques, 2D cross-correlation is most frequently applied to
extract the zero order translational velocity components neglecting the higher order terms
of rotation and deformation. The assumption is that the flow field is smooth and not sig-
nificantly influenced by rotational or shear displacements, thus yielding the zero-order trans-
lational displacement field with an additional measurement uncertainty due to neglecting the
higher-order terms. The measurement uncertainty can be reduced by window deformation
techniques, which require manipulation of the raw particle images. The higher-order motion
terms are then indirectly estimated on discrete grids by finite difference schemes. The
assumption is that the higher order fluid motion of an element is only affected by the trans-
lational velocity components of the neighboring elements.
In contrast to correlation based techniques, LSM shifts, rotates and stretches a fluid element.
For this purpose, the LSM algorithm iteratively compares gray value information of an inter-
rogation area in the first time step with the gray value information in the second time step.
This is an iterative least squares procedure applying a proper transformation on the inter-
rogation areas. In 2D this results in six transformation parameters. The advantage of LSM is
that whilst calculating the zero order translational velocities, the first order terms of motion
are simultaneously optimized increasing the accuracy of the velocity field. The resulting dis-
placement gradient tensor includes parameters like rotation, shear and strain of the inter-
rogation area resulting from the particle displacement within the area.
2D LSM performs a geometric and radiometric transformation between two successive states
of the same system. In the case of a gray value filled interrogation area its state is a gray value
distribution of the pixel elements. For this purpose, the transformation is optimized such that
the gray value differences between a template area and a search area reach a minimum. Com-
pared with conventional 2D cross-correlation, LSM considers the deformation of a fluid ele-
ment for the calculation of the displacements. This results in a more accurate calculation of the
velocity field. The velocity gradient tensor and as result the rotation and deformation rate
tensor can be calculated without applying central difference schemes. In LSM all this is done
without manipulation of the raw particle images.
16.1.2 Usage
The use of 2D?LSM?is very similar to that of standard cross-correlation. The recipe dialog is
shown below. You can define a Threshold level such that only areas of the image are used
which show a variation in intensity above the chosen treshold. The Interrogation are size can
be chosen independently in x- and y-direction, and any odd number may be chosen. Also the
spacing (Shift)?between the vectors (i.e. overlapping of the interrogation areas) can be
chosen independently in both directions, and any even number of pixels is allowed.. The start
position (Grid Start) can be chosen to be different from the lower left corner. The size of the
vector field can be specified with the parameter Grid Width Height. These two last para-
meters allow to compute the vector field on the smaller area then the whole image, if
desired.
The Search factor defines the size of the second interrogation area, which the first inter-
rogation area is matched to. The iterative procedure is stopped when all parameters have con-
verged or when the number of Maximum iterations has been reached. In the latter case
these vectors may be rejected by checking the corresponding check box. A significance test
can be applied to check if the calculated shear, rotation and scale parameters are significant.
Please note, that in case they are not significant they will be replaced with zero-entries in the
data set.
As an iterative method LSM needs starting values for the affine parameters that are to be
determined. If no foreknowledge exists about the velocity field, all parameters are initialized
with Zero. To speed-up the processing and reach better convergence behavior the LSM pro-
cess can be initialized with the results of a preceding cross-correlation step (Use cross-cor-
relation as  input). To avoid false initialization by outliers, a Universal Outlier Detection
validation can be enabled. Its parameters (Neighborhood size and Normalized Residual)
can be adjusted. An alternative method to provide initialization is the Pyramid approach. It is
usually faster to use the cross-correlation as an input, and it is recommend to use this ini-
PAGE | 422
PAGE | 423
tialization to ensure the reliability of the calculated vector field.
16.1.3 References
[1] J.Kitzhofer, G. Ergin, V. Jaunet, "2D Least Squares Matching applied to PIV Challenge data",
16th Int Symp on Applications of Laser Techniques to Fluid Mechanics Lisbon, Portugal, 09-12
July, 2012
[2] V. Jaunet, J. Kitzhofer, T. I. Nonn, B. B. Watz, P. Dupont, and J.-F. Debieve, "2D Least
Square Matching: an alternative to the cross-correlation technique for PIV applications",
European Fluid Mechanics Conference, Rome 2012.
[3] V. Jaunet, J. Kitzhofer, T. I. Nonn, B. B. Watz, P. Dupont, and J.-F. Debieve, "2D Least
Squares Matching, une alternative a la correlation pour la velocimetrie par images de partic-
ules", 13ieme Congres Francophone de Techniques Laser, CFTL 2012 - ROUEN, 18 ? 21
Septembre 2012
[4] Jerry Westerweel and Fulvio Scarano. Universal outlier detection for PIV data. Experiments
in Fluids Volume 39, Number 6, 1096-1100, DOI: 10.1007/s00348-005-0016-6
16.2 2-Frame  2D  PTV
The new two-frame, two-dimensional Particle Tracking Velocimetry (2-Frame 2D PTV)
algorithm can achieve high accuracy velocity measurements in double-frame images with mod-
erate to high particle densities, typical to PIV raw images. The 2-Frame 2D PTV analysis is cur-
rently only possible after an “Adaptive PIV” analysis in the following situations:
? A single Adaptive PIV vector map
? An ensemble of Adaptive PIV vector maps
16.2.1 1.  How  it works
The Adaptive PIV analysis iteratively deforms the interrogation windows between consecutive
frames in order to get a better correlation peak during cross-correlation analysis. In addition
to this, from DynamicStudio version 5.0 (2016a) onwards, the Adaptive PIV analysis performs
particle matching for each interrogation window after the final deformation step. The particle
matching algorithm implemented here is a modified version of Sciacchitano et al. (2013)1 ,
which is originally used for PIV uncertainty quantification. In the current implementation the
particle disparity vectors calculated for individual particle pairs are stored after each Adaptive
PIV analysis. This information is later used for uncertainty quantification and for 2-Frame 2D
PTV analysis methods.
16.2.2 2.  Graphical Use Interface
The 2-Frame 2D PTV GUI contains user inputs and metrics on the left and the visual inform-
ation on the right. The user inputs are provided by sliders and by using the keyboard. One
can:
? Browse different images in the ensemble
? Set a threshold value for background noise removal
? Perform a second pass for particle matching (optional)
? Perform outlier detection (optional)
? Review different metrics
The image matrix on the right contains the following information:
? The input image
? The input vector map calculated by Adaptive PIV processing
? The matched pairs in the 1st pass
? The matched pairs in the 1st pass (if selected)
1PIV uncertainty quantification by image matching, by Andrea Sciacchitano, Bernhard Wieneke and
Fulvio Scarano http://iopscience.iop.org/article/10.1088/0957-0233/24/4/045302/meta
PAGE | 424
PAGE | 425
16.2.3 3.  Typical use
Image Index: In some situations it is not desirable to select the 1st image pair to test dif-
ferent analysis settings. The slider can be used to select any image pair (or vector map) in the
ensemble.
Threshold value: A proper threshold value can be selected to remove some background
noise in the image pair before 2nd pass particle detection. If set to 0, raw images are used for
particle detection.
2nd Pass  Particle matching (Optional): In certain situations (e.g. in highly turbulent flows)
it is desirable to perform a second pass to increase the number of matches between two con-
secutive frames. If selected, the algorithm finds each unmatched particle in frame 1, takes
the average displacement of the neighboring vectors in a certain radius (consensus sample
radius set by the user) and searches for a match in frame 2 within a search radius (once again
set by the user). The consensus sample radius and the search radius is defined a percentage
of the final interrogation area size and the consensus vector size, respectively.
Outliers: Similar to PIV analysis, outliers may occur in the PTV analysis. The detection and
replacement of outliers are performed similar to Universal Outlier Detection for PIV data,
using the guidelines provided by Duncan et al. (2010)1 . The detection threshold and nor-
malization levels here are actually weighted inversely by the distance of the neighboring vec-
tors in the unstructured grid.
1Universal outlier detection for particle image velocimetry (PIV) and particle tracking velocimetry
(PTV) data, by J Duncan, D Dabiri, J Hove and M Gharib, http://iopscience.iop.or-
g/article/10.1088/0957-0233/21/5/057002/meta
When a change is registered in one of the settings a red circle appears on the images in order
to indicate that the algorithm is working on the calculation with the new settings.
Metrics: In order to give an idea about the success of the algorithm, some metrics are
provided:
? Number of particles detected in frame 1 and 2
? Number of matches found in the 1st pass
? Additional number of matches found in the 2nd pass
? Total number of matches found after both passes
? Match ratios after the 1st and 2nd passes
Typical results: It is expected that the PTV results are more accurate and have more res-
olution than the PIV results since they operate on individual particles instead of a collection of
particles. PIV (left) and PTV (right) results of a toroidal vortex mid-plane calculated using syn-
thetic particles is shown below:
16.3 Time-Resolved  2D  PTV
The Time-Resolved 2D PTV analysis method is a 2D velocimetry technique and reconstructs
the particles trajectories of the measured scene.
It can be performed on particles inside a light sheet illuminated volume that is captured using
single frame camera recordings. The analysis techniques is designed to be fast and for low to
PAGE | 426
PAGE | 427
moderate seeding densities. The quality of the results are greatly dependent on the thickness
of the light sheet, A thinner light sheet reduces tracking ambiguities that arise from the fact
that the method is a 2D method
The Time-Resolved 2D PTV consists of two main parts, in a first step the particles are detected
and in the second the particle tracking algorithms matches the particles in time. For more
information on the algorithm please see Analysis Process section.
16.3.1 Applying the Time-Resolved 2D PTV
For all Velocimetry methods it is important that the image quality is of a good level, and that
the images are preferably pre-processed and cleaned to a degree where only the particles in
the images have a non-zero gray-value.
.
16.3.2 Interface and settings
The user interface for Time-Resolved 2D PTV is split into 2 sections:
The first part of the recipe contains the settings for the tracking:
The "Tracking Area". Here the dimensions and the center of the tracking area with respect to
the image is entered in pixels.
The "Search Parameters". Here limitations to the initial particle association can be made to
inform the tracking algorithm of a direction of a mean flow for example
The "Image Processing". Where simple image processing can be applied to the raw images to
enhance the tracking results.
The Second part of the interface contain the display which is a preview of the current pro-
cessed image, based on the chosen processing level in the Image Processing section
described above, and a bounding box it showing the area to be processed, based on the selec-
tion in the Tracking area section described above.
The tracking method used is the same as the one used for 3D tracking. For further inform-
ation on the tracking method please refer to the Analysis process section of the 3D
PTV?tracking analysis documentation.
3D Particle Tracking Velocimetry.htm
16.4 3D  Particle  Tracking  Velocimetry
The 3D Particle Tracking Velocimetry (3D PTV) analysis method is a volumetric "Volumetric
Velocimetry (Legacy method)" on page?925 technique and reconstructs the particles tra-
jectories of the measured scene.
It can be performed of particles inside a 3D illuminated volume that is captured using single
frame camera recordings from at least 3 different camera viewing angles. However, the
PAGE | 428
PAGE | 429
more cameras are used, the higher the seeding concentration can be, since, ambiguities in
the particle triangulation can be reduced. The analysis techniques is designed to be fast and
for low to moderate seeding densities. The seeding density is typically measured in particles
per pixel (ppp), which is measured on the camera sensor plane. Typically for a four-camera
set-up the maximum seeding density with 3D Particle Tracking is about 0.01ppp. that means
independent form the measurement depth the number of particles allowed in the volume is
constant. This results in a better spatial resolution the thinner the volume is. If higher seeding
densities need to be analysed by a tracking approach, the "Tomographic Particle Tracking
Velocimetry" on page?848 can cope with about twice the seeding density.
The 3D PTV consists of two main parts, in a first step the particles are triangulated to get a 3D
locations, one the 3D locations of the particles are known, the particle tracking algorithms
matches the particles in time. For more information on the algorithm please see Analysis Pro-
cess section.
16.4.1 Applying the 3D Particle Tracking Velocimetry
For all Volumetric Velocimetry methods it is important that the image quality is of a good level,
and that the images are pre-processed and cleaned to a degree where only the particles in
the images have a non-zero gray-value.
The 3D PTV needs input form the acquired images and the camera calibrations. All of them
need to be marked with a space-bar, as shown in the image below
.
Note: with the new release of DynamicStudio, the only calibration model available for this
method is a Pinhole calibration model. Support for a 3rd order Model will be introduced at a
later stage. For more information on the calibration model please and the Multi Camera Cal-
ibration please take a look to this section: "Multi Camera Calibration" on page?585
16.4.2 Interface and settings
The user interface for 3D PTV is split into 4 section: The display which is a preview window with
one of the images and a bounding box around it showing the to be reconstructed volume. The
volume settings which are defined in the 2nd part the "Tracking Volume", here the dimen-
sions and the center of the volume with respect to the calibrated volume is entered. In the
third section, the tracking and triangulation parameters are defined. While in the fours and
last section, basic image processing can be applied to the raw images to enhance the tracking
results.
Tracking Volume
In the Tracking Volume section, the volume is defined in which the actual PTV is perfromed.
Only particles within this volume will be considered. The example above shows the volume has
a size in X,Y,Z of 80 x 46 x 20 mm. the center of this volume (40 mm from the left, 23 mm
from the top and 10 mm form the back), is the point that references to the target location. In
the given case the center of the volume happens to be to the center of the calibrated area.
Thus if the volume would not be centered around 0|0|0 coordinate from the calibrate, here
the offset is defined.
Tracking Parameters
Tracking parameters define the particle triangulation and particle tracking properties.
l  Max deviation from epipolar line in px:Defines the distance a particle is allowed to be away
from the epipolar line in the other camera.
l  Projected point search radius in px: Defines the distance a particle candidate is allowed to
be away from the projected particle location in order to make a match.
l  Max movement / frame mm: Defines the search radius for the nearest neighbor criteria in
mm for the x,y,z direction this is how much a particle can move from one frame to the
next.
PAGE | 430
PAGE | 431
l  Min. track length: Defines the minimum duration of a particle track. Here one should set it
to at least 4 or 5 to avoid ghost or false tracks.
Image Processingg Parameters
Tracking parameters define the particle triangulation and particle tracking properties.
l  Display image:Defines which image time step is previewed.
l  Image threshold: Defines a threshold value below which every gray value is clamped to 0.
l  Image blur: Defines if the raw images should be smoothed or not and what the kernel size
for the blur should be.
16.4.3 Analysis process
The analysis section is divided into two main sections.
1.   The particle triangulation, where the 3D positions of the particles are determined
2.   The particle tracking, where the 3D positions are connected in time to get the trajectories
During the triangulation phase first all 2D particles centroids in the images are located. before
these 2D locations are projected along their epipolar lines into the other cameras. On the
next cameras potential particle candidates are found, these are potential 3D locations which
are close to the epipolar line of a given particle. This distance in the 3D PTV menu is referred
to as the "Max. deviation from epipolar line". This corresponds to the closest distance from a
particle centroid to the epipolar line.
Using additional cameras the 3D candidates are verified using projection into the remaining
cameras where the projected particle position needs to have a match within the specified
search radius. This entire procedure is shown in the sketch below:
For the the tracking phase the 3D particle locations are combined to 3D trajectories. Here, it
is important that always combine the same particle for the tracking process in time. A so-called
Four-Frame appraoch is used in order to optimize the tracking. For time-steps T1 until T2 from
the given example below, no known velocity information exists. Hence, the trajectory needs
to be initialized by the nearest neighbor criteria. Which means that the closest 3D locations of
a particle within the predefined search radius in x,y, and z-direction is the selected match for
the next time-step. In the menu this is referred to as "Max movement / frame". The following
sketch shows the search area in the red circle.
.
PAGE | 432
PAGE | 433
After a first velocity is known, the search radius in the next time step can be narrowed down
since the velocity is known. Ones we have found a particle at T3 the acceleration can also be
used, to narrow down the search radius again for the fourth time step T4.
With many particle candidates for the track present a criteria for ambiguities for established
tracks is needed. This avoids that particles which earlier were connected to another track, that
might fit better to another track can be reassigned to this one. Therefore a fitness function is
used. In case that multiple particles are within a search radius the fitness function always picks
the one closest to the projected centroid location. In case this particle already is part of
another track this track needs to be re-evaluated afterwards.
16.5 Adaptive  PIV
The Adaptive PIV method is an automatic and adaptive method for calculating velocity vectors
based on particle image pairs. The method can iteratively adjust the size, shape and location of
the individual interrogation areas (IA) in order to adapt to local seeding densities and flow velo-
cities and gradients.
The method also includes options to apply window functions, frequency filtering as well as val-
idation in the form of Universal Outlier Detection.
The picture below show the recipe dialog for the Adaptive PIV method.
The dialog has two main parts, the image view and the recipe settings seen in upper section of
the dialog. In the following the available settings will be described.
16.5.1 Interrogation  areas
On the ‘Interrogation areas’ tab it is possible to adjust the layout of the IA sampling grid.
The number of interrogation areas (IAs) and the spacing between their center positions is
determined by the parameter ‘Grid Step Size’. The grid step is specified as number of pixels
from one IA to its neighbor. Grid Step Size directly control the spacing between vectors so the
smaller it is the more vectors will be computed. If grid step is smaller than IA Size, the IA’s will
overlap. Grid step size is specified independently for both horizontal and vertical direction.
The Adaptive PIV method can automatically determine an appropriate IA size to use for each
individual IA, but specified minimum and/or maximum IA sizes limits the range. These are also
visualized by the green boxes in the center of the image view.
The first iteration will always use the largest IA size allowed, while subsequent iterations is
allowed to reduce IA?sizes where particle density is high enough to justify it.
Minimum IA?size is also used to determine the location of vectors; Both horizontally and ver-
tically there will be as many vectors as possible within the area covered (full image or ROI);
Grid Step Size determine the distance between neighbor vectors, while Minimum IA Size
determine how close to the borders a vector may be located. When centered around the vec-
tor position, the minimum sized IA?is guaranteed to be completely inside the image area pro-
cessed, while the maximum IA?Size centered around the same location may in fact extend
beyond the image (or ROI) borders.
By default Adaptive PIV?will analyze the entire input image, but with the checkbox 'Use only
selected region' you can chose a region to analyze by adjusting a rectangle inside the image
display.
With the checkbox 'Use only cross correlation' you can turn off all adaptivity and just perform
classic textbook cross-correlation with fixed interrogation areas and no offset or iterations. It
is useful for a quick test or perhaps as benchmark against which other approaches may be
compared, but it is not recommended for quality results. If this check box is selected a num-
ber of the features described below will be switched off.
16.5.2 Windowing and Filtering
The tab ‘Window/Filter’ is used to apply a spatial windowing and/or frequency filtering function
as part of the FFT based cross-correlation.
The conventional Window and Filter Functions are the same as for other correlation methods
("Correlation option Window/Filter" on page?944, but the checkmark 'Use Wall Windowing' is
unique to Adaptive PIV:
PAGE | 434
PAGE | 435
Wall Windowing can be used when a mask is available to identify the location of walls. The Mask
must be preselected before entering the Adaptive PIV?Recipe ("Selection (Input to Analysis)"
on page?419).You can use either a regular 'Mask' ("Define Mask" on page?474) or an ordinary
image (typically derived from acquired images through Image Processing). If you wish to use
an ordinary image you need to enable it's Custom Property 'Use as Mask' in order for it to be
recognized/accepted as a mask ("Custom Properties" on page?419). Zero-valued pixels in the
Mask image will be treated as 'Wall', while nonzero pixels will be interpreted as 'Fluid'.
The purpose of Wall windowing is to mitigate wall bias; Correlation measures the average dis-
placement/velocity of particles within the interrogation area (IA). There are (normally) no
particles inside walls, so when an IA extends into a wall resulting displacements/velocities may
be biased by particles far from the wall, that generally move faster than particles close to the
wall. Wall windowing attempts to mitigate this effect by masking also the particles far from the
wall, so remaining particles are symmetrically distributed around the center of the IA. Figures
below show an Interrogation Area (frame 1 only) and resulting Correlation map with and
without wall windowing applied:
Top left shows an Interrogation Area where the lower left extend into a wall (shown in blue).
Top right shows the result from correlating two such IA's.
Bottom left shows the same IA with Wall Windowing applied; The zero values in the lower left
have been rotated 180 degrees around the IA?center to mask out the particles in the upper
right as well; Remaining pixels are now symmetrically distributed around the IA center and
bottom right shows the resulting correlation map.
With Wall Windowing applied there is a risk that no particles remain from which to compute a
correlation in which case there will be no velocity estimate in this location, but if a vector is
found it will be unbiased.
16.5.3 Validation
The validation is used to prevent outliers from disturbing the iterations and thus the velocity
measurements. The validation is done by first applying peak validation on the image cross-cor-
relation and secondly by comparing each vector to its neighbors using the Universal Outlier
Detection algorithm.
Three peak validation schemes are proposed in order to invalidate vectors based on the
image correlation peaks:
o  Peak Height
if the Peak Height validation is enabled, then only the correlation peaks above the spe-
cified value will be retained as valid.
o  Peak Height Ratio
if the Peak Height validation is enabled, then the ratio between the two highest cor-
relation peaks is calculated. This ratio must be higher than the specified value in order to
validate the calculated displacement. Typical value for Peak Height ratio is 1.2.
o  S/N-Ratio
if S/N ratio is enabled, first the noise level in the correlation plane is evaluated by the Root
Mean Square of the negative correlation values. If the ratio between the correlation Peak
Height and the noise level is above the specified value, then the calculated displacement is
considered valid.
We recommend using Peak Height or S/N-Ratio validation criterion. Indeed, if the Inter-
rogation Area only contains noise the ratio between the two highest peak may still be quite
high.
If either Peak validation fails the corresponding vector will be Rejected. Later when the
"Universal Outlier Detection" on page?873 is performed, and substitution is enabled, the rejec-
ted vector may be replaced with the median of valid neighbor vectors.
After the first and intermediate iterations validation and substitution is mandatory, but after
the last iteration the user may choose not to validate at all, to validate, but not substitute rejec-
ted vectors or to both validate and substitute.
16.5.4 Adaptivity
The ‘Adaptivity’ tab contains settings that will affect the adaptive adjustment that is iteratively
applied to each IA.
PAGE | 436
PAGE | 437
Adapt IA Size to Particle Density
It is possible to enable/disable adaptivity of the size of the interrogation area based on
Particle Density.
If adaptivity to particle density is switched off the first iteration will use the maximum IA size
allowed (as normal), while in each of the following iterations the IA size is divided by two until
the specified minimum IA?size is reached. If adaptivity to particle density is switched on the ini-
tial correlation will still use the maximum IA?size allowed, while in each of the following iter-
ations the IA size is determined from an estimated particle density.
Two parameters adjust how the particle density adaptivity works:
Particle detection limit:
Determines how a particle is detected. A gray scale peak must rise this many times above the
noise floor to be counted as a particle.
Desired # of particles/IA:
Will affect the size of the interrogation areas by specifying how many particles an IA should
nominally contain. Regardless of particle density IA?Size will always be in the Minimum - Max-
imum range specified on the 'Interrogation areas' Tab.
Adapt IA Shape to Velocity  Gradients
It is possible to enable/disable adaptivity of the interrogation area shape to velocity gradients.
To be sure that the shape of the interrogation area is not changed to something way out the
user can set two different limits
First the absolute magnitude of each of the four gradients can be limited.
Second the combined effect of all four gradients can be limited as well.
Iteration control
Convergence limit (pixel):
A stop criteria for the adaptive iteration. When the translational part of the IA shape cor-
rection is less than the specified convergence limit, the iteration is stopped for the given IA. It
may continue for other interrogation areas.
Max # of iterations :
Specifies the maximum number of iteration to perform. The analysis will stop after the spe-
cified number of iteration, no matter if the analysis of the IA has converged or not.
16.5.5 Diagnostic
The top right part of the recipe contain a 'Diagnostic' tool, allowing the user to single step
through the iterative procedure, inspecting the interrogation areas, correlations and vectors
after each iteration in the adaptive process.
By pressing the ‘Step’ button, a single step in the iterative process will be calculated, and the
result will be shown.
Below we zoom in on a 5x5 neighborhood of interrogation areas and corresponding vectors:
The left part of the figure above show results after the first iteration and the right part show
the same area after the second iteration.
The blue rectangles illustrate the interrogation areas, possibly scaled down to prevent them
from overlapping in the display. Comparing the left and right image, the IA size was clearly
reduced from the first to the second iteration. When an iteration has converged the blue rect-
angle will turn green. The actual size, shape and offset of the interrogation areas is shown
when the mouse hovers over a specific vector. In the left image above the red rectangle
shows the actual IA?size and in the right image the IA translation and distortion can be seen;
The IA on frame 1 is shown red and the IA on frame 2 is shown yellow.
PAGE | 438
PAGE | 439
Right clicking an IA will open a context menu, from where the correlation image or the trans-
formed IA can be viewed by selecting the menu items ‘Show correlation image’ or ‘Show inter-
rogation areas’ respectively.
By default the particle image will be shown behind the interrogation areas and vectors, but
the Interrogation Areas can be hidden by removing the checkmark in 'Show Int Areas', and
the particle image can be replaced by the Mask (if selected), by a map of estimated particle
density or removed completely by changing 'Image to show'.
The image above shows the estimated particle density as an 8-bit image, where each pixel
value can be interpreted as the average number of particles within a 64x64 interrogation
area centered around the current location.
To detect particles a background estimate is first made and subtracted. From this the local
noise level is estimated and the image is divided by this noise to get an SNR-image. The SNR-
image is binarized with a user defined threshold (The parameter 'Particle detection limit' in
the 'Adaptivity' tab). Peaks in the resulting image are interpreted as particles and they are
finally counted within local neighborhoods and spatial smoothing applied to get a density map
such as the one above.
The parameter 'Particle detection limit' in the 'Adaptivity' tab is the only recipe parameter that
affects the density map.
Since a grayscale peak is by definition brighter than all neighbors in a 3x3 neighborhood at
most 25% of all pixels can be peaks(/particles). The max-value of 255 could thus theoretically
be found within a 32x32 neighborhood, but it is very unlikely in practice. For low particle dens-
ities this resolution is rather coarse, so we scale up by a factor of 4 and estimate particle count
in 64x64 neighborhoods even if this may theoretically lead to saturation (particle counts
above 255 are truncated).
A change of any of the recipe settings will make the iteration start over when 'Step' is next
pressed.
16.5.6 Reference Vector Map
If/when approximate or expected particle displacements are known, Interrogation Areas on
frame 1 &?2 are offset relative to one another. From the 2nd iteration and onward these off-
sets are naturally based on results from the previous iteration, but in the first iteration
particle displacements are typically not known and no offset can be applied. It is however pos-
sible to include a displacement predictor in the form of a reference vector map, from which
offsets in the first iteration can be determined. Such a reference vector map could e.g. be the
result of an ensemble correlation on the same image ensemble, providing a (temporal) aver-
age flow field. The reference vector map must be preselected before entering the Adaptive
PIV?recipe ("Selection (Input to Analysis)" on page?419). The reference vector map must have
the same number of vectors in the same locations as the ones generated by Adaptive PIV
itself, otherwise it will be ignored.
16.6 Calibrate  Analog  inputs
This method is used to calibrate the analog input stamps recorded with raw images (i.e.
images labeled with the       and      icons), thereby facilitating automated LIF processing. The
analog stamp can be the signal received from the Energy Pulse Monitor (e.g. for shot-to-shot
laser energy fluctuation compensation) or any other transducers such as a pressure or tem-
perature sensors or a CTA signal with asynchronous recordings.
16.6.1 How  to set calibration  values?
To set calibration values to a single raw image, right-click with the mouse on the image of
interest and select the 'Set properties' function. Complete the dialog window with the text A1
= x.xx; A2 = x.xx; etc. (with the symbol ; between each input), where x.xx are a floating point
values (e.g. average energy from the laser, temperature 1, temperature 2 and pressure) cor-
responding to the analog channels A1, A2, etc. Save the new settings.
To set multiple input images with the same (A1, ... A4) values at once, multi-select the images,
right-click with the mouse on the first image and select the menu 'Set the Log Text (Prop-
erties) for selected Records?'. Complete the dialog window with the text A1 = x.xx; etc as
shown in the example below.
PAGE | 440
PAGE | 441
Calibrated analog input values are set in the Log Property of raw image.
16.6.2 Calibration
Calibration is done according to a user-defined polynomial function, which order can vary
between 1 and 5 thereby offering extended flexibility for complex fitting calibration curves
(For instance, linear- and a log-functions would be approximated using a 1st and 5-order poly-
nomial function, respectively).
To calibrate analog inputs, select the raw images of interest (which Log Properties are prop-
erly set). Call the 'Analog calibration' method and complete the dialog box obtained according
to requirements; i.e.
l  Analog channel to consider...
l  and corresponding variable A1.. A4 to consider
l  Polynomial order
Press the 'Apply' / 'Display' buttons to preview the results, modify the polynomial order if
necessary and then press the 'OK' button to accept the final results. A new file labeled with
the icon      is then created in the database. This file contains all the calibration information
necessary to re-sample analog inputs before automated LIF processing (see related help file).
Analog inputs are calibrated according to user-defined polynomial functions.
Analog calibration data are labeled with dedicated icon for quick identification in the database.
16.7 Auto-Mask?Generator
The Auto-Mask Generator will attempt to create a static mask from an ensemble of particle
images, based on a normality test of gray-value time histories for each pixel.
16.7.1 Theory
The fundamental assumption is that gray-values of pixels inside a wall or other static 'Object'
will follow a normal distribution, while pixels in 'Flow' regions will not due to the occasional pas-
sage of seeding particles much brighter than the background. The Auto-Mask Generator is
based on the paper:
Andrea Masullo & Raf Theunissen:
Automated mask generation for PIV image analysis based on pixel intensity statistics
Exp Fluids (2017) 58:70
For each pixel in the image a normality test is applied to estimate the likelihood p that gray-
values follow a normal distribution. There are several such normality tests, the one used here
is the so-called Jarque-Bera test, based on skewness s and kurtosis k of the data:
-where N is the number of images, g is the grayscale values and ...
Since μ is itself estimated from the data the estimated σ is biased and so are s and k. For reas-
onably large N-values (number of images) the error is however small and in practice gray-
values rarely follow a normal distribution exactly, so there's no need for more advanced
formulas.
For normally distributed data the skewness should be 0 and the kurtosis 3. The Jarque-Bera
score JB quantifies the deviation from these ideal values:
Obviously s=0 and k=3 will lead to JB=0, corresponding to a perfect match with a normal dis-
tribution, while any deviation from the normal distribution will lead to JB>0.
Under the null hypothesis that g is normally distributed, JB should follow a Chi-Square dis-
tribution with two degrees of freedom. Consequently the p-value describing the likelihood
that g-values are normally distributed is:
With ν=2 degrees of freedom the Chi-Square distribution simplifies to:
If the pixel under investigation follows a normal distribution exactly we should get JB=0 and
thus p=1. If the pixel under investigation does not follow a normal distribution we will get JB>0
and 0<p<1.
Jarque-Bera propose a threshold of p=0.05 to distinguish normal distributions from non-
PAGE | 442
PAGE | 443
normal distributions, but for typical PIV?images this will work poorly since PIV?image back-
grounds may be quite far from normal. Areas where particles pass by are however even fur-
ther from a normal distribution so it's just a question of finding a suitable threshold value for p
to classify a pixel as belonging to either Flow or Object.
16.7.2 Automask Generator implemented in  DynamicStudio
The Auto-Mask Generator is found in the Masking Category of 'Select Analysis Method' and
accessible when an image ensemble is the parent:
The recipe contain settings in the left hand side, progress information at the bottom, the
usual 'OK', 'Cancel' & 'Apply' in the right hand side and a preview in the center showing what
the current mask looks like overlaid on the first image in the ensemble:
When you enter the recipe it will immediately start loading images from the parent ensemble
to build up the statistics. A resulting preliminary mask is shown in the central preview area,
changing as more and more images are included in the statistics. With the button in the lower
right corner you may 'Pause' and 'Resume' the process, possibly stopping early if/when the
mask no longer changes and is considered OK. If you stop early you can later 'Show Recipe'
and 'Resume' from where you stopped, but once an image has been included in the statistic
there is no way to remove it. As with most other statistical approaches 'more is better' so you
may have 'too few' or 'enough' images, but you cannot have 'too many'.
Thresholding
Thresholding rely on the spatial distribution of p-values found for each pixel and assumes impli-
citly that parts of the image are 'Object' while other parts are 'Flow'. Whether or not the
images do in fact contain any walls or objects the thresholding will typically find something to
classify as 'Object'.
The distribution of p-values is usually strongly nonlinear so thresholding is in fact based on ln
(p) = -JB/2.
The recipe offers three methods for finding the threshold: Otsu, K-means or Manual (See
recipe above).
l  Otsu is a deterministic method to determine a threshold from a histogram of p-values.
l  K-means clustering is a stochastic method to determine threshold iterating from a ran-
dom start guess.
Due to the random start guess resulting mask may change if you repeat the analysis.
l  Manual will allow you to set the threshold manually by dragging a slider left and right.
Post-processing
No matter how many images you include there is no guarantee that all pixels in the 'Flow'
region will at some point 'see' a particle and they may thus be falsely interpreted as 'Object'.
Likewise occasional noise may be just enough to falsely identify a pixel in a wall as 'Flow', where
it should of course be 'Object'. These false positive and false negative errors are typically small
affecting just a single pixel or small, isolated pixel clusters. True 'Flow' and 'Object' regions will
on the other hand typically cover much larger coherent pixel neighborhoods and this can be
exploited to correct false positive/negative errors using a combination of median filtering and
morphological Opening and Closing operations. For more details about Morphological oper-
ations in general see "Image Processing Library (IPL)" on page?525. Images below show the
same mask subject to increasing post-processing kernel sizes. It's the same example as above,
but zoomed in on a smaller area and with background particle image inverted so the mask in
red can be seen better. In the first one Post-Processing is switched off and both false pos-
itives and false negatives are apparent:
PAGE | 444
PAGE | 445
False 'Flow' are usually isolated pixels, that the 3x3 kernel can deal with, while in this example
false 'Object' regions in the 'Flow' area are larger and require at least 5x5. Still larger kernels
has a low-pass effect that tend to smooth edges. Since walls and objects are ususally much lar-
ger than particle images the 15x15 kernel is the default even if this is also the most time-con-
suming option.
Double-Frames
For Double-Frame images frame 1 & 2 are processed separately, generating two separate
masks that are merged into one just before returning.
16.7.3 Pitfalls,  Tips &?Tricks
Please note that the Auto-Mask Generator will assume that 'Walls' or 'Objects' are present in
the images whether or not they actually are. It will generate a mask no matter what input it
receives.
The Auto-Mask Generator is designed for and works best with PIV?images, where particles are
bright and distinct on a dark background. Putting it simply 'Object' pixels are expected to have
more or less constant grayscale intensities, while 'Flow' pixels are expected to have varying
grayscale intensities. For a pixel to be classified as 'Flow' it is thus important that it sees a
particle in some of the images. It is however equally important that it also sees the back-
ground and that this background is significantly darker than the particles. For images with
very high particle density or where the background is simply too bright, the particles may not
differ enough from the background to register as particles in the statistics. Consequently Flow
regions may be falsely marked as Object and thus masked out. In some of these cases pre-pro-
cessing of the images may help; Consider high-pass filtering the images before generating
the mask. For small particle images this can be done using a morphological Tophat filter and
otherwise a custom filter can be used for spatial High-Pass filtering of the images. Both the
Tophat and the Custom Filter are part of the "Image Processing Library (IPL)" on page?525.
The example below shows a mask generated with similar recipe settings on the basis of raw
images (left) and high-pass filtered images (right). Laser reflections from the tunnel floor gen-
erates a region of higher background intensities that is falsely classified as 'Object' (Wall). High-
pass filtering as preprocessing can reduce the problem:
The example above illustrates another problem that we can do little about: The experiment is
a wake flow behind a cylindrical obstacle and particles reflect in the surface of the cylinder.
This causes part of the cylinder to be interpreted as 'Flow' where it should of course have
been 'Object'. The best way to avoid this is to avoid reflecting surfaces which, for laser safety
reasons, is recommended anyway.
The Auto-Mask Generator creates a static mask based on the assumption that Walls or Objects
in the images remain stationary. If they do in fact move (possibly due to camera vibration), the
location and boundary of Walls and/or Objects will be fuzzy at best. Please consider stabilizing
the images first using "Rigid Object Tracking and Stabilization" on page?776.
'Flow' regions on the other hand are expected to contain particles that actually move. If a
particle is stationary the pixel intensities in and around it will not change much over time and
the region may be falsely classified as 'Object'. This can be a challenge for very slow flows. As a
rule of thumb particles should move at least twice the particle image diameter from start to
end of the input image ensemble.
Apart from objects being stationary the Auto-Mask Generator also assume overall light intens-
ities to be reasonably constant in the images of the parent ensemble. If particle illumination
and/or background light intensity increase or decrease over time it may skew the statistics
leading to Walls or Objects being falsely interpreted as 'Flow' regions. High-Pass filtering as
described above may also reduce this problem.
If visible in the images object shadows or the edge of a light sheet may be interpreted as a
'Wall' since outside of the light sheet the camera never saw any particles. Not because they
weren't there, but because they weren't illuminated. A light sheet rarely has a sharp and well
defined edge, but instead has a gradual decrease of light intensity, making it unclear exactly
where such a 'Wall' is located. Even if light travels in straight lines such a 'Wall' will typically be
wavy in response to large particles that occasionally pass by scattering just enough of the faint
light to be detected.
PAGE | 446
PAGE | 447
16.7.4 References
Andrea Masullo & Raf Theunissen (2017)
Automated mask generation for PIV image analysis based on pixel intensity statistics
Exp Fluids (2017) 58:70
Jarque, CM & Bera, AK (1987)
A test for normality of observations and regression residuals.
Int Stat Rev/Rev Int Stat 55:163?172.
doi:10.2307/1403192
16.8 When  to use  Average  Correlation?
When there are only few particles in the flow one can either move to PTV (Particle tracking) or
one can compute the average flow field PIV through Average correlation. In Average cor-
relation, the correlation function of each interrogation areas is averaged at each location for
all the images.
It increases the signal to noise ratio significantly and generates a clear correlation peak. Just
think of 20 PIV pictures. Let's assume in the first 15, there is no particles in a certain inter-
rogation region. The average correlation is nearly null, because these interrogation regions
only contain random noise. In the last there is a few particles in the region, these form a cor-
relation, and when added together they form a nice correlation peak.
See more about Average Correlation and schematic of how to obtain average velocities.
The evaluation of the peak is done normally, and it represents the average velocity within the
interrogation region and within the number of images recorded. This entails that you must
record sufficient number of pictures, so the total sum of particles in each region is reas-
onable. When you have a reasonable amount of images, the result is a nice vector map
without spurious vectors.
The applications are typical in micro-fluidics, where it often can be difficult to apply sufficient
amount of particles. But any application where it is difficult to obtain a result, for one reason
or the other it can be useful.
16.8.1 Using Average Correlation  to look at average PIV signal conditions
The average correlation can also be useful to use if one want to plot the average proportions
of the correlation peaks, signal height or peak widths.
From left: Recorded instantaneous image of bubbles and seeding, scalar map of the peak
widths from average correlation, and histogram of the peak widths.
The distribution of the bubbles and seeding is clearly seen in both the map of the Peak widths
and the Histogram
This feature, was in combination with the Peak validation used to separate the two flow
phases. (For further info on this feature and the example, read the help file on Peak Val-
idation)
16.8.2 Using an  offset vector map to improve the results
As default, the Average Correlation can not operate with any window shifting, which may pose
problem, when for example the velocity is high relative to the interrogation region, say for
example 20 pixels to a 32 pixel interrogation region.
In order to improve the result, one should make on run with a smaller time between pulses,
corresponding to say 5 pixels maximum displacement. Maybe fewer image recordings can be
used. Validate the result and filter strongly. Run the image recording again with the 16 pixels
displacement.
For processing use the 5 pixel recording as the "Vector map for offset of IA". This shifts the
interrogation spots corresponding to the 16 pixels, and ensures a correct result. This pro-
cedure also minimizes the so-called velocity bias, which is automatically dealt with in Adaptive
Correlation.
The " Vector map for offset of IA" can be selected (y double clicking) before calling the Aver-
age Correlation.
In order to ensure the most correct result, the Average Correlation function should actually
always be run two times.
PAGE | 448
PAGE | 449
16.8.3 Using preconditioning to minimize influence from  out of focus
particles
In particular in micro-fluidics there are particles out of focus which can contribute dominantly
to the correlation and hence result in the wrong velocity to be measured, assuming the out of
focus particles has an other velocity. This effect is particular strong using back illumination in
micro-fluidics. Other effects in ordinary PIV can be shadows from object in the background.
Objects or particles out of focus often have a significantly different level of gray values, so a
simple way reject these is to apply a simple gray value rejection.
Under the tab "Precondition" one can select which gray values to conserve, i.e. the particle
images are predominately in the range from 120 to 233. The values outside is set to 0, or an
other user defined value.
Use the display LUT to find the values you would like to use, check color-coding.
Please be aware there are many other methods which can be very useful to remove
unwanted background. I.e. subtract an average image before correlation or use some of the
many filter functions in the IPL - library.
16.8.4 Schematic of Average Correlation
When there are only few particles in the flow, adding the correlation functions together builds
up the correlation signal peak step by step.
The process can schematically be shown:
Interrogations areas from image frame A (pulse 1) and A (pulse 2) are correlated. The cor-
relation functions are added up and the peaks are finally evaluated.
PAGE | 450
PAGE | 451
Two other ways  of obtaining the average velocity
It is also possible to take the average of all the images. DynamicStudio does this, simply by
Select Similar of the images and then choose Mean Pixel Value from the Statistics group. This
image can be cross correlated as well as individual images. The process can schematically be
shown:
It works well if there is little or no background noise, but conceptual it is not a very good way
to obtain the results.
The classical way is shown for the complete overview. DynamicStudio does this, simply by
Select Similar of the images and then choose Cross-Correlate or Adaptive Correlations. Then
Select similar of the Vector Maps and choose Vector Statistics from the Statistics group.
The process can schematically be shown:
< back
16.9 Average  Filter
This method is used to filter vector maps by averaging neighbor vectors. The size of the (MxN)
averaging neighborhood is defined by the user with no maximum values. (Both M and N must
be odd, but can be set independently).
PAGE | 452
PAGE | 453
Tip: Averaging neighborhood sizes are typically square (e.g. 3x3 or 5x5), but can be rect-
angular also and this works very well for e.g. channel flows.
Example of results: (Top) Instant velocity vectors calculated by Adaptive PIV, and (Bottom)
Averaged vectors (5x5 neighborhood).
16.10 Calibration  refinement
Calibration Refinement improves the accuracy of an existing stereo calibration by using
particle images acquired simultaneously from both cameras.
Each of the original Imaging Model Fits (IMF's) refer to a coordinate system defined by the cal-
ibration target used. When using the imaging models for later analyses, it is generally
assumed that the X/Y-plane where Z=0 corresponds to the center of the lightsheet, but in
PAGE | 454
PAGE | 455
practice this assumption may not hold since it can be very difficult to properly align the cal-
ibration target with the light sheet.
Provided the calibration target was reasonably aligned with the light sheet it is however pos-
sible to adjust the imaging model fits by analyzing a series of particle images acquired sim-
ultaneously by each of the two cameras. This adjustment is referred to as Calibration
Refinement and changes the coordinate system used so Z=0 does indeed correspond to the
center of the lightsheet as assumed by subsequent analyses using the camera calibrations
(IMF's).
16.10.1 Required input
To perform Calibration refinement you need 4 inputs:
l  Two camera calibrations, one for each camera.
l  Two ensembles with multiple particle images acquired simultaneously by each of the two
cameras.
The particle images can be from actual measurements, they need not be acquired specifically
for the purpose of calibration refinement.
You may benefit from preprocessing the particle images f.ex. to remove the background, in
which case the processed images can be chosen as input for the calibration refinement. Like-
wise preprocessing of the calibration images may be applied to get the best possible initial
camera calibrations.
To initiate Calibration Refinement select the required inputs as shown above, right-click either
of the original IMF's and select 'Calibrate...'. Then pick 'Calibration Refinement' in the list of
possible calibrations:
You will get a recipe similar to the one below (you may have to right-click the display area and
select 'Fit to image and graphics zoom' in order to see something there):
The red and blue polygons illustrate the part of the lightsheet visible from each of the cam-
eras according to the present calibrations. Analysis is only possible in the overlap area visible
from both cameras, so in the example above the cameras fields of view could have been
aligned better.
16.10.2 Refinement Area
The green rectangle illustrates the area in which you wish to perform the calibration refine-
ment. At the top of the recipe you can select between different refinement areas:
PAGE | 456
PAGE | 457
Full Field of View                 Encloses the full field of view of both cameras including
areas with data from only one of them.
Common Field of View       The largest possible rectangle within the area of overlap
between the two cameras fields of view.
Common Calibration
area
The area where both cameras found calibration markers
in the calibration images.
Manual adjust                      User defined by dragging the rectangle corners or typing
values in a property dialog accessible from the context
menu.
Using a very large refinement area is likely to include regions where data is not available from
both cameras, but avoiding this by means of a smaller refinement area will probably exclude
parts of the overlap area, where data from both cameras is in fact available.
The smallest of the predefined refinement areas, 'Common Calibration area', is mainly inten-
ded for use with polynomial imaging models which may behave strangely if/when you extra-
polate beyond the area actually covered by the calibration.
16.10.3 Frame
Calibration refinement aims at changing the coordinate system so the X/Y-plane at Z=0 cor-
responds to the center of the light sheet. Most PIV-systems use a double-cavity laser in order
to facilitate a freely selectable delay between two consecutive particle images. This means
however that it is misleading to talk about "the lightsheet", since there are in fact two, one
from each of the laser cavities. Careful alignment of the two laser beams aim at making the
two light sheets coincide, but in practice you have to choose which of the two lasers/light
sheets you wish the refined calibrations to refer to. Provided the input particle images are
double-frames this is accomplished simply by choosing whether to refine on Frame 1 or
Frame 2 of the images. If input particle images are single-frame , you have no choice but to
refine on the basis of the laser cavity used when acquiring the images.
16.10.4 Interrogation  Area  Size
When a refinement area and a frame (light sheet) has been chosen, particle images are
dewarped (i.e. projected back to the light sheet using current calibrations and assuming that
everything the cameras can see is in Z=0). If the light sheet is infinitely thin and the current cal-
ibrations are correct (i.e. Z=0 does indeed correspond to the light sheet), the dewarped
images from camera 1 & 2 should ideally be identical since they are acquired simultaneously
and thus contain the exact same particles in the exact same physical positions in space. In prac-
tice the images are of course not identical, but for a good set of camera calibrations the devi-
ations should be small and random. If the calibration target was not properly aligned with the
lightsheet the assumption Z=0 does not hold and there will be systematic deviations between
dewarped particle images from camera 1 &?2. Particles will appear to shift and that can be
detected by using a normal cross-correlation between the dewarped images. In practice we
use average correlation over a series of image pairs, since cross-correlation between a single
pair of dewarped images normally has quite poor S/N-ratio.
For the average correlation you can choose interrogation area sizes of 64x64, 128x128 or
256x256, we use a fixed overlap of 50%. In general the largest interrogation areas produce
the best S/N-ratio and also has the best chance of recovering large alignment errors. Large
interrogation areas are however also more sensitive to gradients and with angular mis-
alignment between light sheet and calibration target the distance between target and light
sheet varies across the cameras field of view, so gradients will be present.
16.10.5 Disparity Map
The resulting vector map represents the misalignment between the light sheet and the cal-
ibration target and is referred to as a 'Disparity Map'.
To see the Disparity Map, press 'Apply' and wait for the system to dewarp and correlate the
series of particle images:
No matter the size of you refinement area, disparity vectors will only be calculated where data
is available from both cameras.
Furthermore each vector must be distinct in the sense that the correlation peak must rise sig-
nificantly above the noise floor. Vectors that do not fulfill this SNR criteria will be shown in red
and excluded from further calculations. If less than 10 valid disparity vectors are found you
will receive an error message saying that "Too few valid disparity vectors are found." and no
further analysis will take place (i.e. Calibration refinement is given up).
16.10.6 Dewarped Particle Images
You can switch on a display of the dewarped particle images (the first image in the ensemble is
shown):
PAGE | 458
PAGE | 459
Using the context menu or keyboard shortcuts A, B?&?T you can toggle between the
dewarped particle images from camera A and B respectively.
16.10.7 Average Correlation  Map
You can also switch on a display of the Average Correlation Map, which will follow the mouse
around showing the correlation map behind each of the disparity vectors:
In the example above we've zoomed in on a particular interrogation area by using the mouse:
Either click-and-drag to draw a rectangle around the area of interest or use the scroll wheel
while holding the Ctrl-key on the keyboard (zooms in and out centered around current pos-
ition of the mouse cursor).
16.10.8 Interpreting correlation  maps
If your disparity vectors do not look as nice as in the example above inspection of the cor-
relation map can be very helpful in understanding what the problem is.
It is quite normal for the correlation peak to be elongated, it is a consequence of the light
sheet having a finite thickness and the cameras looking at it from different angles.
If the peak appears to be fragmented (multiple peaks along a common ridge) it is probably
because too few particle images went into the calculation. This can be simply because there
were not enough images or because you're at or near the edge of the light sheet where light
intensity is low and only a few of the images contain particles big/bright enough to be detec-
ted by both cameras.
If the correlation map contains no peaks at all it is possible that the misalignment between
light sheet and calibration target was simply too big for the calibration refinement to recover
the true position of the light sheet. In that case the peak searched for is will be outside the
interrogation area and you may try using a bigger one. Alternatively check if by mistake
you're working on the basis of particle images that are not acquired simultaneously in which
case they do of course not correlate at all.
Finally you may consider the possibility that the cameras and/or the light sheet moved during
acquisition; If for example mirrors are involved in bringing the light sheet to the intended
measuring area small mechanical vibrations can cause the light sheet to move several mm.
Similarly mechanical vibrations may cause the cameras to move, which will of course also have
serious impact on the calibration and subsequent analysis of images.
16.10.9 Interpreting Disparity Vectors
Each disparity vector can be interpreted as a measure of the distance from the nominal Z=0 to
where the lightsheet really is. The vectors will in general point in one direction if the light
sheet is closer to the cameras than assumed and in the opposite direction if it is further away.
Assuming the lightsheet is plane we can make a least squares fit of a plane through all of the
disparity vectors and use the fitted plane as an estimate of where the light sheet really is. We
can then define a transformation (rotation and translation) that will be able to move points
back and forth between the original (target)?coordinate system and a modified (lightsheet)
coordinate system where Z=0 corresponds to the center. Applying this transformation to the
calibration markers from the original calibration images we obtain a new list of corresponding
image and object coordinates which can then be fed into the normal camera calibration
routines to generate modified calibrations.
Press OK to store the refined camera calibrations in the database. The refined calibrations will
use the same imaging model as the original camera calibrations (Pinhole camera model in the
example used here).
To verify that the modified camera calibrations match the position of the light sheet better,
you can try to perform yet another Calibration Refinement, this time using the modified cal-
PAGE | 460
PAGE | 461
ibrations as input data along with the same particle images as before. Resulting disparity vec-
tors should be smaller than before:
A third set of camera calibrations will be generated and stored if you press OK again, and you
can of course continue with even more iterations if you wish.
Resulting camera calibrations will be stored in the database as a chain of imaging model fits
derived from one another:
To visually see how the iteration moves to fit the lightsheet you can also overlay each of the
imaging model fits on an image:
-and zoomed in on the upper left corner:
Red is the original calibration, yellow is the first iteration, green the second (a third iteration
was tried, but didn't make any visible difference in this example).
Note: Overlaying refined calibration grids on calibration images will not align very well with cal-
ibration markers and cannot be expected to. Think of it as an indication of where the cal-
ibration markers should have been if they had been aligned with the center of the light sheet.
16.10.10 Change of Coordinate System
Measurements will always take place where the light sheet is no matter where the calibration
target was when calibration images were acquired. Therefore calibration refinement by
design changes the coordinate system from one aligned with the calibration target to another
aligned with the light sheet. In order to change it as little as possible the coordinate system is
first rotated around (0,0,0) to make the X/Y-plane parallel to the light sheet (i.e. without
PAGE | 462
PAGE | 463
moving the origin). Then the coordinate system is translated along the (newly rotated)?Z-axis
to make Z=0 correspond to the center of the light sheet.
16.11 Coherence  Filter
Feature trackers and cross-correlation may occasionally yield completely wrong velocity vec-
tors. To enhance the result of measurement, coherence based post-processing is applied to
the 'raw' velocity field obtained. The coherence filter modifies a velocity vector if it is incon-
sistent with the dominant surrounding vectors. The solution we use is a modified version of
the vector median filter. The procedure operates as follows:
Given a feature point Pc with the velocity vector vc, consider all features Pi, i = 1,2,.., p, lying
within a distance S from Pc, including Pc itself. Let their velocities be vi. Due to coherent
motion, these vectors are assumed to form a cluster in the velocity space. Introduce the
mean cumulative difference between a vector vi and all other vectors vj,
The median vector is the vector that minimizes the cumulative difference. Its index is
, the mean cumulative difference of the median velocity, characterizes the spread of
the velocity cluster. The standard median filter substitutes vc by the median vmed. In our
implementation, vc is substituted by vmed only if the difference between vc and vmed is sig-
nificant:
The standard median filter tends to modify most of the measurements and introduce an addi-
tional error. The conditional median filter only modifies the vectors that are likely to be impre-
cise or erroneous measurements.
Radius: the distance from the vector of interest. All vectors within this radius are used for val-
idation. Effect of filter is show below:
A. No filtering
PAGE | 464
PAGE | 465
B. Coherence filter (radius = 30 pixels)
16.12 Combine  Tracks
When 3D Scanning PTV is carried out, the result will be one Particle tracking dataset per Light
sheet. This method is made for combing the different track records into one single record.
Five different slices of a scanned 3D TOMO PTV result. The different colors represent a dif-
ferent light sheet.
16.12.1 Inputs:
The Combine Track function can be applied on any time resolved tracking data that was recor-
ded in single frame. At least two neighboring 3D tracking datasets are needed. The different
input tracks need to be highlighted and selected by pressing the space bar on the keyboard ( 
Link to: Selection (Input to Analysis)) as shown on the image below.
PAGE | 466
PAGE | 467
Marked input for Combine tracks    Selection of combine tracks
l            3D PTV (3D Method) ["Volumetric Velocimetry (Legacy method)" on page?925]
l  3D Tomographic Particle Tracking Velocimetry (3D Method) ["Tomographic Particle Track-
ing Velocimetry" on page?848]
16.12.2 Outputs:
The output dataset is a PTV data set with combined tracks. That can be filtered or post
proessed as usual 3D PTV and 3D TOMO PTV data.
16.12.3 Settings and how  to:
Once the tracks are selected no settings in the menu can be applied.
16.13 Combustion  LIF  processing
This method is used to calculate density number maps of a given combustion species such as
OH, CH and others inside a user-defined region of interest of a raw single-frame LIF image.
The result is returned in molecules/m3 (on frame_1) with the instantaneous error levels (on
frame_2). Note that the error is relative to the global energy absorption; i.e. the map should
be multiplied by Tr to obtain the relative error in % of [Nx] (See the application manual for the
notation used.)
Content:
l  Image analysis using 1 energy pulse monitor (Processing based on emitted energy level)
or 2 energy pulse monitors (Processing based on transmitted energy level)
l  Interpretation of the results
16.13.1   Image analysis
When calling the 'LIF Processing' method, the dialog window shown below pups-up:
PAGE | 468
PAGE | 469
Combustion LIF processing dialog window.
1.   First, in the 'Camera/ analog setup' tab the operator must specify whether 1 or 2 energy
pulse monitors are used during the experiments. Select the 'Emission' channel and the
'Transmission' channel, which is set to "N/A" when only 1 energy pulse monitor is used. Set
the parameters corresponding to the imaging characteristics (i.e. intensifier QE, gain, cam-
era filter transmission, etc.): This will be used to define the so-called ' light collection effi-
ciency' coefficient (Refer to the Combustion_LIF manual for transmission curves, etc.)
When using 2 energy pulse monitors, note that these parameters are not required for
data processing, as analysis is based on the transmitted light; i.e. Energy (Before flame) -
Energy (After the flame), i.e. reference is made to the total light budget and total signal
budget on the light propagation path (on which energy absorption is estimated).
2.   Make sure the light propagation is not done at a 0-angle and used the 'Rotate image'
option available in the Image Processing Library to correct the view if necessary (or simply
make sure the camera is placed correctly before the recording of LIF data). Also, make
sure that the analogue input(s) is/are re-scaled to the proper dimension and are not in
Volts. (Use the 'Calibration analog input' and 'Rescale analog input' method to this effect.)
3.   Define the light sheet characteristics by setting the height (in mm), the center position (Yc
in pixel), light sheet type for local energy correction, etc. Using the mouse, set the region
of interest (right-click, drag and un-click) inside which processing should take place. If
needed, manually refine the area by entering the (X, Y) pixel positions of the upper/left
and lower/right corners of this area. Note that if this area is larger than the light sheet
height, the algorithm will automatically set 0-values in the outer parts of the illumination.
4.   Select the combustion radical imaged (This will set values to several constants such as the
emission line) and complete the absorption cross-section box with the dimension x10-20
cm-2.
Note: With the version 1.00 of the Combustion-LIF software, only OH is available. The
option 'User-defined' is enabled but a couple of spectroscopic constants will not be set
properly. Although energy profile correction, energy budget, etc will be implemented
the absolute values will not be correct.
Once ready, press the 'Apply' and 'Display' buttons to preview the results and then 'OK' to
accept it. The resulting image is then labeled with the      icon for quick identification inside the
database.
16.13.2   Interpretation  of the results
The result is provided as a double-frame map with density number [Nx] (in molecules/cm3) on
frame_1 and an error map on frame_2 (only when 2 energy pulse monitors are used) to help
assess the quality of the instant LIF result. This error map is based on energy absorption (Tr)
on a portion of the light sheet and therefore features a gradient in amplitude with the light
propagation direction (Ideally, the value of Tr is between 0 and 5 but sometimes it can be up
to 15). To estimate the effective error on the instant density number map, the user should
multiply frame_2 by the absolute error done on Tr (which value can be derived when cal-
ibrating the analogue input channels for emission and transmission light, respectively).
Example number density [Nx] map with rescaled analog inputs 1 and 2 to mJ/pulse (Processing
using 2 energy pulse monitors).
16.14 Cross-Correlation  (Legacy method)
This analysis method is obsolete and included for backward compatibility only.
The recipe is read only and no further development or maintenance will be made.
It will be removed completely in a future release of DynamicStudio.
We recommend using "Adaptive PIV" on page?433 instead.
PAGE | 470
PAGE | 471
It is possible to use both double images and single images as input.
16.14.1 Cross-correlating single images
It is possible to correlate single frame images in many different ways. The order of selection is
also important.
Please be aware that DynamicStudio uses the Time between Pulses stored with the Setup as a
reference when computing the velocity in m/s, so if the Time between Pulse does not cor-
respond to the actual time between two image recordings, the displayed velocity is scaled
incorrectly.
16.15 Curve  fit processing
This method is used to fit experimental data such as       XY-plots and      probability (dis-
tribution) plots to non-linear functions. To use this method, select the file(s) of interest and call
the method "Curve fit processing" located in the 'LIF Signal" category.
Content:
l  Curve fitting procedure
l  Open data fit as numeric
l  Overview on non-linear fit models
16.15.1   Curve fitting procedure
To fit a set of (X, Y)-data to a given function:
1.   Select the X- and Y-data columns to used
2.   Specify the model to use and if not in the list of "most-commonly used functions", press
the 'User-defined fit' checkbox and write down the equation to fit as shown in the
example below.
3.   (When necessary, set initial values to the tolerance and max. iteration coefficients)
4.   Press the 'Apply' button to check the output and modify the fit function type if necessary.
Dialog window for data fit.
To help assess the goodness of fit, press the 'Display' button to edit parameters of the res-
ulting figure and select/unselect the 'Raw data', 'Fitted data' and 'Error (%)' (e.g. weighted
residuals) checkboxes for quantitative assessment.
PAGE | 472
PAGE | 473
Dialog window for control over quality of data fit and figure display.
Example of curve fit output including raw and calculated data.
Click on 'OK' to accept the final results. The data are then stored in the database and labeled
with the icon      to facilitate its location among other type of calculations.
< Back to the top
16.15.2   Open  data  fit as numeric
Once the fit data are stored in the database, direct access to the coefficients, errors (%) etc. is
made using the shortcut     . Selecting the 'Copy to clipboard' or 'Export All as file…' options
(click on the right-button of the mouse), data are easily retrieved.
Typical output of a 'curve fit' file when opened as numeric.
< Back to the top
16.15.3   Overview  on  non-linear fit models
Polynomial:
Gauss:
Exponential:
Exponential Expansion:
Beta:
16.16 Cylindrical  coordinate  transform
Cylindrical coordinate transformation converts position to polar coordinates and velocities to
radial/tangential components as follows:
Position:
l  r=√(x^2+y^2 )
l  θ=tan^(-1)?(y/x)
l  z=z
Velocity:
l  V_r=U cos?θ+V sin?θ
l  V_t=-U sin?θ+V cos?θ
l  V_a=W
-where (r, θ, z) are polar/cylindrical position coordinates and (Vr, Vt, Va) are radial, tangential
and axial velocity components respectively.
(Axis definition is used)
16.17 Define  Mask
This method is used to define a mask that can subsequently be used by "Image Masking" on
page?508 or "Vector Masking" on page?885, to mark regions of specific interest.
To define a mask, select an image ensemble and bring up the analysis selection dialog as
shown below.
PAGE | 474
PAGE | 475
Selecting the Define Mask option will display the dialog below:
The view and appearance of the image can be adjusted by using the color map (for details see
"Using the display from within an analysis method" on page?982).
16.17.1 Adding shapes to the mask
The mask can be composed of three types of shapes (rectangles     , polygons     and ellipses
). To add a shape to the mask, click on the desired mask shape in the toolbar, and select the
appropriate mask type (reject, outside, disable or transparent).
After the shape has been selected, click on the image to start specifying the shape location
and size.
Once the shape has been created its appearance can be further adjusted as described in
"Adjusting the rectangle" on page?987, "Adjusting the polygon" on page?985 or "Adjusting the
ellipse" on page?984.
To change the mask type of an existing shape (or a group of shapes) select the shape and spe-
cify a new type in the selection box.
PAGE | 476
PAGE | 477
16.17.2 Deleting shapes
To delete a shape, simply select the shape(s) and press the <Delete> key.
16.17.3 Selecting multiple shapes
Multiple shapes can be selected by holding down the <Shift> key while dragging a rectangle
that contains the shapes to be selected, or by selecting the shapes individually while holding
down the <Ctrl> key.
16.17.4 Ordering shapes
The final mask will be composed of all the specified shapes, if shapes are overlapping the top-
most shape will determine which mask type will be used.
The ordering of shapes can be adjusted by right-clicking the individual shapes and selecting
"Bring to Front"/"Send to Back" from the context menu.
16.17.5 Preview  the final mask
If the "Preview Mask" checkbox, located in the tool bar, is selected, a preview of the final mask
will be superimposed onto the image. From this preview it is easily verified that the correct
ordering of shapes is met.
16.17.6 Vector map Overlay
If a vector map is selected in the database, prior to entering the define mask dialog, the vec-
tor map will be displayed as an overlay to the image and the mask. The vector map display can
be toggled on/off by the check box "Show Vectors", and the scaling of the vectors can be adjus-
ted by the slider "Vector Scaling".
16.18 Diameter  Statistics
The histogram display refers directly to the underlying histogram dataset, which is a subset of
the IPI dataset or Shadow dataset.
When selecting Shadow Histogram, the following window appears.
Histogram setup
Type the minimum, maximum diameter and number of bins to be used.
Process
In double frame mode, select which Image to process: A for the first frame, B for the second
frame or both.
Region
Check "use entire area" to process the entire image or type the coordinate (in pixel) to the
region of interest
Click on Apply or OK to execute processing of the histogram. The software will run through all
the selected datasets and then display the final result in a plot window
PAGE | 478
PAGE | 479
The histogram display refers directly to the underlying histogram dataset, which is a subset of
the first dataset in the IPI dataset series. The data can be displayed in either tabulated or
graphical form.
The graphical display of the histogram shows one of three possible histograms:
l  Diameter histogram
l  Area histogram
l  Volume histogram
For a description on how to change the graphical setup of the display, please refer to "XY Dis-
play" (on page?1040).
Below the graph (in the info box), are shown the diameter statistics:
l  D10: diameter mean.
l  Counts: the number of valid particles used in the histogram.
l  D StdDev: standard deviation of diameters.
l  D20: area mean
l  D30: volume mean
l  D32: Sauter mean.
l  D43: De Broukere mean
The Dpq diameter statistics mentioned above are defined as follows:
16.19 Extract
The Extract analysis method (formerly known as Extract to XY line plot) makes an extract of
multiple datasets in selected positions (i, j). This is particularly useful in connection with time
resolved data, but can also be used in other connections.
The result from an extract can either be opened as Numeric (exported or copied via the clip-
board to i.e. Excel) or it can be view in DynamicStudio with the Open as XY plot (Short cut
Ctrl+X).
l  Example: making a time series and phase plot
l  Extracting gray values from a series of images
16.19.1 Examples
Making a Time Series  and a Phase Plot
Time resolved data are extracted from 250 vector maps in 5 points. The points are indicated
in the flow with circles. The flow is a flow over a cylinder generated by a slightly larger jet
below the cylinder.
PAGE | 480
PAGE | 481
The points are selected by the position tab with index numbers and the tools given here.
In this example, we have extracted the V-velocity component as indicated in the Quantity win-
dow. This gives us a plot of the five points as a function of the vector map index.
We have additionally indicated that we would like to use the elapsed time (index multiplied
with recording time), by checking this in the X candidate.
Finally, to generate a phase plot of the V-velocities, we press "Add" and select V. This will give
us the V component, which can then be used as an X axis candidate.
The time series plot, showing 3 of the points (right click on the plot to select other point, un-
check, change the plot format)
By right clicking on the plot, it is also possible to select one of the V-components along the X-
axis. Here the V from (38, 22) is plotted along the X-axis and four of the points are plotted
along the X-axis. The one from (38, 22) obviously providing a straight line.
PAGE | 482
PAGE | 483
Local Grayscale Variations
Use this Extract analysis method to investigate local variations in the grayscale values in
images in multiple positions. If the image is a double image grayscale values are extracted
from both frames. (formerly a resampling was necessary for extracting grayscale values from
image maps, this is not required anymore).
Linearity  of LIF Measurements
If you are making LIF measurements using the analog input to sample the laser energy, an
extract of the LIF signal from the images plotted as a function of the analog input, could show
the linearity of the two signals.
16.20 Feature  Tracking
Feature Tracking is an optical flow analysis tool, where flow structures are tracked from
frame-to-frame. Feature tracking is analogous to correlation techniques in that the user spe-
cifies areas of interest and iteratively solves for the result. However, tracking techniques do
not search the same place on each image, rather, the structure found on a previous image is
tracked and if found its new position is recorded. In this way a structure can be followed for
many frames, perhaps the entire length of the recording sequence.
The KLT tracker works as follows: good features are located by examining the minimum eigen-
value of each 2 by 2 gradient matrix, and features are tracked using a Newton-Raphson
method of minimizing the difference between the two windows. Multi-resolution tracking
allows for relatively large displacements between images. Each feature being tracked is mon-
itored to determine if its current appearance is still similar, under affine distortion, to the ini-
tial appearance observed in the first frame. When the dissimilarity exceeds a predefined
threshold, the feature is considered to have been lost and will no longer be tracked. At least
two frames are needed for the operation of the algorithm.
Required input: an ensemble of single frame images.
Number of features  to find: the maximum number of features to find per image.
Test: this will determine the maximum number of features to find by analyzing several
images in the input.
Window size: the size of the feature window.
Maximum iterations: the number iterations that the tracking algorithm will utilize to find a
feature. A feature is considered lost if it is not found within this criteria.
Minimum eigenvalue: the minimum allowable eigenvalue for new features to be selected.
Maximum residue: the maximum residue, averaged per pixel, when tracking.
Minimum displacement: the minimum displacement, in pixels, necessary to stop the iter-
ative tracker and declare tracking successful.
Search range:
Minimum distance: the minimum acceptable distance between each feature.
Image smoothing: apply image smoothing.
Auto-contrast: Normalize images for gain and bias.
Border x,y: size of the border in pixels that is not analyzed. This is necessary due to the
nature of a Gaussian convolution (much of the image is unknown). Set to ?1 for automatic
setup.
Calculate: This will calculate a recommended border size on basis of the setup.
PAGE | 484
PAGE | 485
The data can be displayed in several ways:
A. Display  all tracks  for all frames:
B. Partial tracks:
Set an interval of features to display and tracks with histories equal to or longer than the spe-
cified range will be displayed.
The display options be accessed using the click menu on the display.
In this dialog it is possible to change the visualization parameters of the direction arrows, the
relevant time segment to visualize as well ad how the display colors the track: either by point
in time or speed
16.21 FeaturePIV
The Feature PIV module takes as input the feature table from the feature-tracking module
and uses the positions found to provide the points of interest for cross-correlation vector ana-
lysis. The advantage of exploiting feature tracking is that regions in an image that contain use-
ful information are used to seed the cross-correlation, rather than specifying a regular 2D
mesh.
There are three forms of output:
l  The direct displacements from the feature table are converted to vectors.
l  The positions specified in the table are used to provide interrogation positions for
cross-correlation analysis.
l  Same as above but using the adaptive cross-correlation.
PAGE | 486
PAGE | 487
A)  Direct feature processing of vectors:
Process  only  feature displacements: vectors are determined directly from the feature
translations frame-by-frame.
Apply  coherence filtering: additional filtering of vectors derived from feature dis-
placements.
Radius: coherence filtering radius. Coherence filtering takes a vector within a local neigh-
borhood that best represents the local flow.
B)  Cross-correlation processing of images. The positions  of features  are used to locate the
origin of interrogation areas  used for cross-correlation.
Select correlation: select the cross-correlation algorithm: cross- or adaptive.
Interrogation areas: the size of the interrogation areas.
Moving average validation: apply moving average validation of the resulting vectors using
the supplied Iterations and Factor.
Peak height validation: validation based on height ratio between the first two dominant
peaks of the cross-correlation.
Additional options:
Include non-tracked features: this will include new tracks that do not have a corresponding
particle in a previous image.
Since the positions are located as specified by the feature tracking and not in any regular
format, the FlexPIV engine is used to process the data. The advantage of converting the data
in this way is that the data can then be compared to other methods and processed by other
analysis modules found in Dynamic Studio. A vector result is shown below:
16.22 FlexPIV  processing
FlexPIV processing is a very flexible way of performing PIV analysis. Conventional PIV pro-
duces velocity vectors positioned in a rectangular grid, but FlexPIV will allow you to calculate
vectors in more or less arbitrary positions, and thus adapt to the flow-field at hand. Fur-
thermore conventional PIV normally apply the same analysis to all points, while FlexPIV will
allow you to use different analysis settings in different regions of the flow-field.
In practice FlexPIV comprises two elements:
l  FlexGrid is used to define grid points and determine analysis settings
l  FlexPIV Processing is used to perform the actual analysis.
To define a grid, select an image ensemble and bring up the analysis selection dialog as shown
below.
PAGE | 488
PAGE | 489
The definition of both vector grid and analysis methods is too complex for a simple recipe, so
FlexGrid is launched as a separate program. exchanging information with DynamicStudio.
From DynamicStudio v3.20 the grid object is now saved in the data base, but the traditional
way of handling grid object, via so-called grid-files with extension '.grd', is still supported.
Select the grid object saved in the database as input for the FlexPIV method. If you do so you
will not, as described bellow, have the possibility to edit the grid object from within the recipe,
but have to make changes to the grid object via the Define Flex grid method. This is done by
selecting Show recipe on the grid object saved in the database.
From the DynamicStudio recipe you have the option to either create a new grid file or to load
and/or edit an existing one:
Pressing 'Apply' or 'OK' you can of course also activate FlexPIV Processing to perform PIV ana-
lysis according to specifications in the grid file.
Please remember to save the grid file whenever you've made changes from inside the
FlexGrid software. If you fail to update (save) the grid file DynamicStudio will not be aware of
your changes.
You can store the grid file anywhere the PC can get access to it, but it is recommended to
store it together with the data you intend to process. If you plan to use the same grid file for
processing of data from different databases you might consider local copies of the grid file
instead of one global file. If a grid file has been used for processing, later changes to the file
will cause loss of traceability for the old PIV results. With local copies of the grid file this is less
likely to happen.
In the following you will find a brief introduction to the fundamental concepts in FlexPIV. For
detailed explanations please refer to the written manual supplied with your system. A full and
complete documentation of all features in FlexPIV is beyond the scope of this help-file.
16.22.1 Defining grid points
In FlexGrid points are defined on the basis of grid objects. There are three fundamental
objects available; Rectangles (       ), Ellipses (       ) and Polygons (       ).
Grid objects can be drawn directly on the screen by using the mouse; For rectangles and
ellipses you simply click and drag the mouse, for polygons you left-click once for every vertex
point and double-click on the last vertex point to finish it. Subsequently you can modify the
objects using the mouse or the property editor in the right-hand side of the FlexGrid working
screen.
Each object can be one of four possible types; Grid (       ), Hole (       ), False hole (       ) or Wall (
).
The default type for new objects is 'Grid', meaning that grid points will be generated inside
the object boundaries. Objects can however be changed easily to any of the other object
types. A 'Hole' is a region where you do not wish to calculate velocity vectors, whereas a 'False
PAGE | 490
PAGE | 491
hole' is an area where you wish to change local settings for vector distribution and calculation.
Holes and false holes have to be linked to a grid object in order to have any effect (the 'hole'
has to be in something). The link is created by selecting the grid and the hole object and then
clicking the 'Group'-button (      ).
Within grid objects grid points are automatically generated according to one of four prin-
ciples. The grid can be Cartesian (      ), Polar (      ), Elliptic (      ) or Delaunay triangulation (      ).
The last two are only available when you switch the user interface to 'Expert mode'. For each
of the grid types point spacing and similar are controlled from the property editor. By default
the grid origin is in the lower left corner of the object, but clicking      in the toolbar you can
move it to the center of the object. This is useful for the polar and elliptic grid types.
16.22.2 Defining vector analysis
Apart from defining where you want velocity vectors, you also need to specify how to cal-
culate them. All grid points within a grid object share a common set of processing parameters,
but using multiple grid objects you can have different processing options in different regions
of your flow-field. For vector processing you can choose either conventional cross-correlation
or adaptive correlation. For both of these you must choose interrogation area size (the IA will
be centered around each of the grid points), and you can also choose various other pro-
cessing parameters including validation and sub-pixel interpolation schemes:
16.23 Grid  Interpolation
Grid interpolation reconstructs an irregular or regular grid of scalar or vector data onto a
user specified regular grid. The method of Thin Plate Splines (TPS) is used to interpolate data.
TPS is an algorithm for interpolating and/or fitting 2D data. As the name implies, TPS essen-
tially takes as input 2D data and "bends" a flat plate until all the points pass through it. In the
event of too much noise and the possibility of singularities, the user can apply "relaxation".
Zero relaxation forces the plate (surface) to pass through all the input points, a large value
reduces the result to a least squares approximation.
Automatic scaling implies that the resulting scalar or vector map adopts the limits of its parent.
Alternatively the user can enter X- and Y- min/max limits.
Interpolation is controlled by the following two parameters:
1.   Radius: the distance in pixels about a point of interest when collecting data points for an
interpolation.
2.   Relaxation: the degree of relaxation, that is, requirement, that all points pass through the
resulting surface created during interpolation.
PAGE | 492
PAGE | 493
The irregular grid above (created by FlexPIV) can be interpolated onto a regular grid and pro-
duce the following result.
16.24 Histogram
The Histogram method is used to extract statistical information about an vector and scalar
datasets. The result from the method is a histogram showing number of counts of a selected
variable of the dataset. It is possible to define a Region of Interest and a temporal window
length on which the Histogram information is to be calculated.
The recipe is able to compute an histogram based on the floowing inputs:
? 2D-2C Vector Maps, calculated from any 2D PIV method
? Stereo-PIV 2D-3C Vector Maps
? Volumetric Volecimetry 3D-3C, obtained through the 3D LSM technique
? Scalar Maps, gradients or vorticity for example.
16.24.1 The Recipe dialog
Selecting component
The top area of the recipe dialog is for selecting what component. All available components
will be listed and it will then be possible to select a specific component. Please ensure that one
variable is selected prior to start of the computation.
Include vectors
It is possible to select if all, only valid, or only valid and non-substituted vector will be included
as an input for the calculation.
Histogram Settings
Minimum and Maximum values specifies the limits for the resulting histogram. If a value is out-
side the valid rage between Minimum and Maximum, this value is out of range. "Out of range"
values will be added to "Out of range bins", that can be included in the resulting histogram by
checking “Add out of range bin’s”
PAGE | 494
PAGE | 495
Number of bin’s specifies how many bins the range between Minimum and Maximum will be
divided in to.
ROI settings  (specified in grid index)
It is possible to have the result based on the full vector map (by checking “use full vector
map”), or to specify a Region of Interest (ROI) in the vector map. Enter a starting point (i,j) and
the widht and heigth of the desired ROI, specified in grid index. The grid indexes are shown at
the bottom left of the data display and corresponds to the location of the mouse (see the fol-
lowing figure). If a ROI is specified only vectors inside the ROI will be included in the cal-
culation.
The recipe dialog above shows how the recipe will look if the input vector map is a 2D vector
map. If the input vector map is a 3D vector map a input for ‘k’ and ‘Depth’ will be added to the
recipe.
"Temporal window length" determines how many datasets are included in the calculation.
"Include all" will set the contents of Temporal window length to “Auto”. This means that all data-
sets in the parent will be included in the calculation and only 1 histogram will be computed (N-1
output). If more than 1 is selected then the number of result will be equal to “Number of data-
set" in the parent ensemble, and each Histogram will be computed using a number of snap-
shots equl to the specified "Temporal window length" (N-N output).
Results  from Histogram
An example of a result from Histogram can be seen below:
As default the histogram displays each bin as a percentage of all counts. The user can choose
to have the result displayed with counts instead using the display options menu available by
right click -> "Display options". More general information on the display can be found here.
The statistical information below the histogram is based on the vectors include in the range
specified by the recipe entrees minimum and maximum value. The RMS value is computed on
the mean-substracted data.
16.25 Image  Arithmetic
As indicated by the name, the method enables arithmetic on pixel values. Operations can be
performed on any type of images (for example 8-, 10- or 12-bit images as well as floating
point images), and can be applied to both single- and double-frame images.
PAGE | 496
PAGE | 497
There are 4 types of arithmetic operations plus extrema:
l  Addition and subtraction (Parent and Operand images are added to or subtracted from
one another)
l  Multiplication and division (Parent and Operand images are multiplied or divided by one
another)
l  Minimum and Maximum (Return the smallest or largest grayvalues from Parent and Oper-
and)
There are two possible operand types available:
l  With an image as operand
l  With a constant value as operand
It is possible to combine the two operands so you can for example subtract another image
and then add a constant value as shown in the example below.
Finally you have the option to perform "Thresholding" on page?499 on the result before
returning it. This is useful to limit the output to a certain range.
16.25.1 Image arithmetic with  another image
The operand image has to be pre-selected, i.e. selected before you enter the Image Arith-
metic recipe. To learn more about selecting data for analysis, please refer to the section User
Selected Data in the help document 'Working with the Database'. Having selected the oper-
and image enter the Image Arithmetic recipe and the selected image will be listed as operand
image in the top section of the recipe. The parent and operand images will be combined pixel
by pixel using the chosen operation; Minimum Maximum, Add, Subtract, Multiply or Divide.
Click 'Apply' to pre-view the result or simply click 'OK' to calculate and accept the result.
Please note that the option 'Divide by' involves the risk of division by zero in case the denom-
inator image includes pixels with a value of zero. For these pixels the analysis will produce out-
put pixel values of zero.
16.25.2 Image arithmetic with  a  constant value
Here you specify a constant value with which all pixels in the parent image is combined to gen-
erate the desired output. Again you may choose either of the operations Minimum. Max-
imum, Add, Subtract, Multiply or Divide.
PAGE | 498
PAGE | 499
The option 'Not in use' allows you to use only an image operand or a constant operand. If you
perform both operations, the image operand will be applied first and the constant operator
second.
16.25.3 Thresholding
The calculated result can be thresholded to user specified limits if so desired. This is set up in
the lower section of the recipe, where upper and lower clamp limits and values can be
enabled and assigned individually. For upper clamping, pixel values that exceed the specified
clamp limit will be assigned the specified clamp value. Similarly for the lower clamping, pixels
with a value smaller than the specified limit will be assigned the specified lower clamp value.
Please note that whether or not you enable data clamping 8-bit images are always clamped to
the limits 0-255, 10-bit images to 0-1023, 12-bit images to 0-4095 and so on. In these cases
data clamping is relevant only if you wish to limit output values further.
16.26 Image  Balancing
The Image balancing module corrects light sheet non-uniformities that affect the outcome of
other analysis routines. The user selects as input an ensemble of image maps. The output is a
correction map the user can employ to adjust images. The program flow is as follows:
Image balancing is a two-step process. The first step is to create an image balance map that
consists of factors determined from an ensemble of input images. The map is then applied
onto individual image maps, correcting for any strong variations in laser intensity.
Step 1: Image balance map
Select an ensemble of image maps that you want to correct. There should be enough images
in the ensemble so that a mean image generated would show relatively soft variations in light
and limited noise activity. Select “Image Balance Map” from the list of analysis methods. The
image balance map analysis will then process this data and produce a correction map that you
can display.
Smooth cell size: the size of the smoothing matrix. Use larger values for sparse data.
Step 2: Apply  balance map
Select an image balance map as fixed input. Select the input dataset you wish to process and
select the analysis "Image Balance Processing".
Pair of unbalanced images (double image):
Correction maps (for frame 1 and frame 2):
PAGE | 500
PAGE | 501
Same images after correction (application of correction map):
16.27 Image  Dewarping
Images recorded with an off-axis camera will be distorted (warped) due to perspective.
With an imaging model fit (IMF) describing the distortion, images can be de-warped.
Image map dewarping is done by imposing a re-sampling grid in the object plane (i.e. the light-
sheet plane). Using the IMF each of the grid points in the re-sampling grid is mapped to a cor-
responding point in the image plane (i.e. the surface of the image sensor). From this
calculated pixel position a grayscale value is derived from the original image and assigned to a
pixel in the de-warped image.
l  Recipe dialog: Imaging Model Fit
l  Recipe dialog: Re-sampling grid
Note: De-warp using Polynomial IMF's
l  Recipe dialog: Re-sampling scheme
l  Recipe dialog: Fill color outside image
l  Recipe dialog: Z-coordinate
16.27.1 Dewarping image maps
Apart from dewarping of measurement images this method is also useful to validate and/or
verify the parameters of an IMF, since dewarping one or more of the calibration images
should produce a de-warped image where calibration markers are well aligned. (See below)
Original
(warped)
image with
per-
spective
distortion.
De-warped
image
without
per-
spective
distortion -
Use rulers
to verify
that (0 , 0)
is aligned
with the
zero
marker,
and that.
horizontal
and ver-
tical
marker
spacing is
correct.
Dewarping of other images is done in exactly the same manner, provided of course they are
recorded with the same camera and the same recording geometry (i.e. neither camera nor
lightsheet has been moved since calibration images were acquired and IMF calculated).
To de-warp an image, select the relevant IMF using the "select method". Move to the image
that you wish to de-warp and select 'Dewarping of Image Map' among the list of possible New
Datasets. You should get a recipe like the one shown below:
PAGE | 502
PAGE | 503
Selecting several calibrations and image ensembles will dewarp all images to the same world
space area. Use this feature when the images need to be dewarped to the same world space
coordinate system like for example 2D PIV.
Top
16.27.2 Recipe dialog:  Imaging Model Fit (camera  calibration)
The topmost entry in the recipe identifies the imaging model fit chosen. If you failed to
choose one before, or chose the wrong one by mistake, click the 'Select' button and identify
the IMF that you wish to use for dewarping. Please make sure that the IMF corresponds to
the camera and geometry actually used when acquiring the image(s) you want to de-warp.
Top
16.27.3 Recipe dialog:  Re-sampling grid
The re-sampling grid can be created automatically or defined by the user. When a user
defined grid is selected, clicking the button labeled 'Suggest' will calculate and show the values
used for the automatic grid.
The grid settings used for the automatic grid will include all of the original in the de-warped
image, and choose the grid spacing so the total number of grid points match roughly the num-
ber of pixels in the original image, not counting grid points that maps outside the original (See
example at the top of this page).
User defined grids can be useful to either zoom in on regions of specific interest (see below),
and/or if you wish to compare simultaneous results from two or more cameras, in which case
you should use the exact same grid settings to de-warp each of the images in question.
User defined Re-sampling grid with
-15 mm <= X <= +15 mm and -15 mm <= Y <= +15 mm
Grid spacing 0.15 mm/pix (Left) and Grid spacing 0.75 mm/pix (Right)
In the example above the effect of changing the grid spacing is also shown; The left hand
image is de-warped with the suggested grid spacing of 0.15 mm/pix, while the right hand
image is de-warped with a grid spacing of 0.75 mm/pix, -i.e. 5 times higher producing a de-
warped image with 5^2=25 times fewer pixels, and correspondingly coarser representation
of the original image (Images above are scaled to display at equal size).
Reducing the grid spacing below the value suggested will of course produce a de-warped
image with more pixels, but very small steps will in general not improve image quality, since
the original image does not contain any more information. As a rule of thumb grid spacing less
than half the value suggested is a waste of computer RAM, and only increases processing time
of any subsequent analysis without producing any additional information.
Please Note: Be careful when using polynomial imaging models for dewarping with an auto-
matic or suggested re-sampling grid; Imaging model fits rely on calibration images with cal-
ibration markers, which are not always found or even available near the edge of the images.
Strictly speaking the resulting IMF is thus only valid in the center of the image, and using it
near the edge requires extrapolation. With a linear model such as the DLT this is usually gives
correct values, but the polynomial model extrapolation may produce incorrect results.
Since the automatic or suggested re-sampling grid attempts to include all of the original
image, it will attempt to process the image edges and thus extrapolate from the IMF, which
may not always work as expected.
Top
16.27.4 Recipe dialog:  Re-sampling scheme
Each of the points in the re-sampling grid is mapped onto the surface of the image sensor
using the IMF chosen. The resulting pixel coordinates will usually NOT be integer, but posi-
tioned somewhere inside a 2x2 pixel area on the original image. Grayscale value for the res-
ulting pixel in the de-warped image is then determined either by bi-linear interpolation, or
simply by taking the nearest neighbor within the 2x2 pixel neighborhood.
Top
16.27.5 Recipe dialog:  Fill color outside image
Each of the points in the re-sampling grid is mapped onto the image plane, but some of these
may fall outside the surface of the image sensor. This will for example be the case when using
an automatic re-sampling grid, which attempts to include all of the original in the de-warped
image, but may of course also occur with a user defined re-sampling grid.
Since no information is available to interpolate between, some fixed grayscale value will have
PAGE | 504
PAGE | 505
to be assigned, and the user can choose Black (=0, -Default), White (=255 for 8-bit images, or
4095 for 12-bit images), or an average grayscale value calculated from the original image.
Different fill colors outside image; Black, White and Average gray.
Top
16.27.6 Recipe dialog:  Z-coordinate
When mapping points from the re-sampling grid onto the image sensor, X- and Y- coordinates
are generated based on min/max values and step sizes, while the Z-coordinate is normally
assumed to be zero. This corresponds to the center of the lightsheet with most normal IMF's,
but if this assumption does not hold the user may enter a non-zero Z-coordinate either in the
log entry of the image properties, or in the dewarping recipe.
There are two typical reasons for using non-zero Z-coordinates;
-The image being de-warped is actually recorded in front of or behind the lightsheet.
-The calibration target was poorly aligned with the lightsheet, so Z=0 does NOT correspond to
the center of the lightsheet.
Please Note: Non-zero Z-coordinates will affect the resulting de-warped image only if the IMF
used is a 3D-model, i.e. generated from an image of a multilevel calibration target, or from a
series of images, where a plane target has been traversed through several Z-positions. Using
a 2D-IMF, the Z-coordinate does not affect the result.
Top
16.28 Image  Histogram
The Image Histogram method is used to extract statistical information about an image. The
result from the method is a histogram showing number of counts of each pixel value. It is pos-
sible to define a ROI on which the Histogram information is to be calculated. Instead of using
the ROI it is possible to specify a mask as input to the method.
You can zoom in, change the color map and other things on the Image. Please "Using the dis-
play from within an analysis method" on page?982
16.28.1 Using ROI
Below is an example of the recipe for the method using the ROI :
The ROI rectangle can be manipulated either by mouse interaction or by text input in its prop-
erty dialog. In the following paragraphs the two different ways of manipulating the ROI are
described.
16.28.2 Manipulating the ROI rectangle using the mouse.
The position and size of the ROI rectangle can be changed by mouse interaction. The entire
ROI is moved by using the left mouse button to drag the ROI to its new location. When the ROI
is selected, or while hovering the mouse over the ROI rectangle, its corner manipulators are
displayed as illustrated in the image below. By using the mouse to drag these corner manip-
ulators to a new location it is possible to change the ROI rectangle. If the shift button is
PAGE | 506
PAGE | 507
pressed while dragging a corner manipulator the center of the ROI is kept at its current loc-
ation.
16.28.3 Setting the ROI rectangle using the property dialog.
When selecting Properties from the context menu(right clicking the ROI) the dialog to the left
(above) is shown.
Entering the desired values into the properties dialog will adjust the position and size of the
ROI rectangle.
16.28.4
Selecting "Fit to full image" will fit the ROI to the full image.
16.28.5 Input Image types
For 8-, 10- or 12-bit input images the output histogram will contain 256, 1024 or 4096 bins,
each specifying the number of pixels with the specific gray-value 0,1,2,...
For floating point images there are millions of possible gray-values and each and every pixel
can in principle have it's own unique gray-value. In this particular case the histogram will thus
create 256 equally wide bins between the minimum and maximum gray-value present in the
image and return the number of pixels that fall in each bin.
Numerical display of results will show actual (integer) gray-values for 8-,10- or 12-bit input,
while for floating point images bin values shown in the numerical display will be the lower limit
of each bin, i.e. bin edge rather than bin center.
16.28.6 Using a  Mask
Selecting a mask as input to the analysis method will disable the ROI. The mask will define
which areas are to be used during the analysis.
Using Image ensemble as  mask input
Instead of using a Defined Mask as input, it is possible to select an image ensemble as input.
The Image ensemble most hold the same amount of images as there are datasets in the par-
ent ensemble.
In order to let use an Image ensemble as mask input, the ensemble custom property must
include the Mask variable. to do this, Right click the ensemble and select "Custom Prop-
erties...". In the "Custom Properties for <Ensemble Name>" dialog, Check the "Mask" check-
box and click OK.
The pixel values of the images in the ensemble indicates if the pixel is masked, here a value of
0 indicates that the pixel is masked. Any other value will indicate not masked.
Using methods like Image Math, Image Arithmetic and Image process Library you can create
any series of mask images that can be used as mask input for the method.
16.29 Image  Masking
This method is used to mask images by assigning specific gray-values in regions defined by the
user as being of no interest.
To apply masking you must first define a Mask, using either the analysis method "Define
Mask" (on page?474) or a regular image with the Custom Property 'Mask' enabled (See "Cus-
tom Properties" (on page?419)).
The mask ensemble must contain either one static mask or N dynamic masks, where N equals
the number of images. Dynamic masks are typically derived from the parent images them-
selves e.g. using the "Image Processing Library (IPL)" (on page?525). If you use regular
images for masking, nonzero pixels in the Mask image will identify pixels in the parent image
that are to be left untouched, while Zero-valued pixels in the Mask image identify pixels in the
parent image that will be modified.
To mask single-/double-frame images, you must first pre-select the mask as 'User Selection'
by selecting it and pressing the 'Space bar' "Selection (Input to Analysis)" (on page?419). Then
you can select the ensemble with images to be masked and select the analysis method Image
Masking. No matter what kind of mask is used, the image masking recipe is the same:
PAGE | 508
PAGE | 509
In the recipe you can specify the gray-value to be assigned to masked pixels in the parent
image:
l  Black-out areas
l  White-out areas
l  Mean pixel value (calculated by the software)
l  User-defined (fixed) value
The result image is labeled with the icon      , clearly showing that a mask was applied to the
parent image (      ).
The following examples are based on a top-down view into a square water tank with a mag-
netic stirrer at the bottom. The light sheet is horizontal and just above the spinner. In each
image the (static) tank walls can be seen as well as the (moving) spinner at the bottom of the
tank:
Using "Define Mask" (on page?474) we can create a mask to remove the walls and anything
beyond them:
...please note there is only one mask, which is applied to each of the parent images suc-
cessively.
Using the "Image Processing Library (IPL)" (on page?525) we can create a series of masks to
remove the spinner:
PAGE | 510
PAGE | 511
...please note that a separate mask is created and applied to each of the parent images.
We can apply the static mask to the dynamic ones to create a series of hybrid masks with which
both walls and spinner can be masked out:
... use option 'Black-out areas' in the masking recipe to merge the two masks.
With this 'Hybrid' mask we can remove both walls and spinner:
When creating dynamic masks please note that mask images derived from a double-frame par-
ent will also become double-frame, while masks derived from a single-frame parent will
become single-frame.
A?single-frame mask can be applied to both single- and double-frame images and even used
for "Vector Masking" (on page?885), but a double-frame mask can be applied to double-frame
images only.
If you've made a double-frame mask, and wish to apply it to a vector map or single-frame
image, you must first extract either frame 1 or 2 using "Make Single Frame" (on page?663),
check custom property 'Mask' again and then use the resulting single-frame mask.
16.30 Image  &  Volume  Math
(In the following "pixel" may also mean "voxel")
PAGE | 512
PAGE | 513
This analysis method enables mathematical operations on pixel values. It is slower than
"Image Arithmetic" (on page?496) and "Image Processing Library (IPL)" (on page?525), but it is
more flexible. Operations can be performed on any type of images (for example 8-, 10- or 12-
bit images, as well as floating point images), and can be applied to both single- and double-
frame images.
Image Math allows you to manipulate grayscale values in an image by specifying mathematical
formulas to be applied:
The Image Processing to apply is defined in the Output tab.
16.30.1 Inputs
The available input images are referred to by designated names, ParentImage, Image1,
Image2, and so on.
You may type them in or pick from the drop down list of available 'Inputs':
The ParentImage will always be available, whereas other images will be available only if
marked as User Selection before entering the Image Math Recipe ("Working with the Data-
base" on page?93).
To aid the choice, the database ensemble name is shown on the right after a vertical bar sep-
arating the name and description. The list may also contain intermediate results named Res-
ult1, Result2, etc, if such are computed within the Image Math Recipe (see below).
16.30.2 Scalars
'Inputs' are images with varying gray values across the field of view, but there are also scalars
available:
For each of the available input images, you have access to the minimum, mean, and maximum
gray scale value as well as the rms of grayscale values within the image. Scalar Value 'i'
describes the frame number (1 or 2 in a double-frame image), whereas 'x' and 'y' contain the
coordinate of each pixel ((x,y)=(0,0) is the lower left corner regardless of offset and scale
factor that may have been defined for the camera in question).
Independent of the input image, natural constants 'Pi' and 'e' are available, and you may
define and use scalar variables named V0, V1, V2, etc. You can assign a value to each of these
either by specifying that value directly or by assigning the result of some calculation to the
variable. Afterward the variable can be used as part of the expression defining the output.
16.30.3 Functions
Predefined functions can be applied to all pixels (or numbers derived from them):
PAGE | 514
PAGE | 515
The available functions are:
Function Name                      Arguments.   Explanation                                          Example
sin(a), cos(a), tan(a)                         1            sine, cosine & tangent                  sin(pi/3)=sqrt(3/4)
asin(a), acos(a), atan(a)                   1            inverse sine, cosine & tangent
sinh(a), cosh(a), tanh(a)                  1            hyperbolic sine, cosine
&?tangent
asinh(a), acosh(a), atanh
(a)
1            inverse hyperbolic
sine, cosine &?tangent
log2(a)                                              1            binary (base 2) logarithm
log10(a), log(a)                                1            common (base 10) logarithm
ln(a)                                                   1            natural (base e) logarithm
exp(a)                                                1            exponential function, exp(x)=ex  exp(1)=e
sqrt(a)                                               1            square root
abs(a)                                                1            absolute value
rint(a)                                                1            round to nearest integer
sign(a)                                               1            = 1 if x>0
= 0 if x=0
=-1 if x<0
pow(a;b)                                           2            raise a to the power of b; ab
rnd(a;b)                                            2            random number between a & b
min(…)                                             var           min of all arguments                     min(a;b;c;d;e)
max(…)                                            var           max of all arguments                    max(a;b;c;d)
sum(…)                                            var           sum of all arguments                    sum(a;b;c)
avg(…)                                             var           mean of all arguments                  avg(a;b;c;d;e;f)
ParentImage_
GetPixelValue
4            gray value of spatio-temporal
neighbor pixel
(boolExpr)? valueIfTrue :
valueIfFalse
3            if (Boolean Expression)
then (valueIfTrue)
else (valueIfFalse)
The regular and hyperbolic trigonometric functions are self explaining as is the exponential,
the logarithms and other functions with a single argument.
When there is more than one argument in a function call they must be separated by semi-
colons ';'.
Power function and random number generator both take two arguments, but require no fur-
ther explanation, and min, max, avg, and sum are also obvious.
Parent_GetPixelValue extracts pixel values from the parent image (available also for other
input images in the form Image#_GetPixelValue, #=1,2,3,…).
GetPixelValue takes 4 arguments (T, Frame#, x, y):…
l  T is the image sequence number and relative to the current image, so T=0 refers to the
current image, T=+1 to the next image in the ensemble and T=-1 to the previous image.
l  Frame# specifies whether to extract the grayscale value from frame 1 or 2, use 'i' or the
value '0' to access the current frame, or use '-1' to indicate that the other frame in the
double frame is to be used.
l  (x;y) specifies the location of the pixel, you can simply use (x;y) to access the current or con-
structs such as (x-1;y+1) to access neighbor pixels.
Conditional execution can be performed with the 'if … then … else' construct in the form
(boolExpr)? valIfTrue : valIfFalse.
For example, thresholding could be done with the expression…
(ParentImage<1)? 1: ParentImage … corresponding to … if ParentImage <1 then 1 else Par-
entImage
(…using max(1;ParentImage) would give exactly the same)
16.30.4 Operators
Operators are applied between two variables, the following are supported:…
PAGE | 516
PAGE | 517
Operator  Meaning          Example    Input            Output
+                 addition           a+b             Numerical  Numerical
-                  subtraction     a-b              Numerical  Numerical
*                  multiplication  a*b             Numerical  Numerical
/                  division            a/b              Numerical  Numerical
^                 raise a to the
power of b
a^b             Numerical  Numerical
<=               less than or
equal
a <= b         Numerical  Boolean
>=               greater than
or equal
a >= b         Numerical  Boolean
!=                not equal         a != b          Numerical  Boolean
==               equal                a == b         Numerical  Boolean
>                 greater than   a> b            Numerical  Boolean
<                 less than          a<b             Numerical  Boolean
&&              logical AND      a && b       Boolean     Boolean
||                logical OR         a || b         Boolean     Boolean
'Numerical' input and output could be pixel grayscale values (integer or floating point),
whereas 'Boolean' input will interpret 'zero' as 'false' and nonzero as 'true'. Boolean output
will use '0' for false and '1' for true and can of course be used as an argument in e.g. 'if … then
… else …' constructs.
16.30.5 Error description
If the expression in the formula entry field cannot be evaluated an error description will be
shown.
16.30.6 Output Selection
(Voxel volumes are always floating point volumes, therefore it is not possible in Volume Math
to select Output type)
Image Math takes image(s) as input and generates image(s) as output. The analysis is per-
formed in floating point, but the final output can be chosen freely. You will typically use 'Same
as parent', but may specify Floating point images or choose a specific bit depth:
16.30.7 Example
Here's an example of nonlinear scaling of grayscale values that can be accomplished using
Image Math:
Each pixel of a 12-bit image is replaced by the square root of itself multiplied by 4095 (=ma-
aximum gray value for a 12-bit image):
PAGE | 518
PAGE | 519
A?similar visual effect can be accomplished by using a nonlinear lut (lookup table), such as
'Hyperbolic' or 'Gamma' in the display options of the image. This does however not change
the numerical contents of the image, but only affects the on-screen display of the image.
Using Image Math as shown here will change the grayscale values of the image and thus
affect subsequent analysis also.
16.30.8 Advanced processing
The default recipe for Image Math contains tabs, named 'Output' and 'New'. The text above
describes the content of 'Output'.
It is possible to create and store intermediate result images from within the Image Math
recipe. This is done using a tab for each of the intermediate image results that you need.
Clicking 'New' will create a new tab named 'result1', clicking 'New' again will create yet another
tab named 'result2' and so on:
Each of these tabs looks and works exactly the same way as the 'Output' tab described above,
except that 'Output' is replaced by an intermediate image named 'result1', 'result2', etc.
The tabs are processed from left to right, and each tab has access to the results from all pre-
vious tabs. With two intermediate results as shown above, 'result2' will have access to 'Par-
entImage' and 'Image1', 'Image2' etc (if any other images are 'User Selected'). In the tab
'result1' you will have access to the same data PLUS 'result2' and in the final 'Ouput' tab you
can access the usual data as described above, PLUS 'result2' & 'result1'.
16.31 Image  Mean
Calculates the average intensity of corresponding pixels in all the selected images. A minimum
selection of two images is required. The resulting output image is placed under the high-
lighted dataset (the one selected last).
Notes
l  "Corresponding pixels" means "pixels with identical x- and y-coordinates" starting with
pixel(1,1) in the lower left corner
l  Make sure that the input images are compatible in terms of image dimensions and gray-
scale resolution. When trying to average images of different dimensions a warning will be
issued. The output image inherits the dimensions from the parent dataset; input of dif-
ferent dimensions are cropped or zero-padded
l  The analogue input values displayed in the "Mean Pixel Values" image are the average
over the analogue input values of the corresponding channels in the individual images.
16.31.1 Application  example
In PIV "Mean Pixel Values" is very useful for generating an image of the common background
in independntly acquired images.
PAGE | 520
PAGE | 521
(A) Single image as acquired.                                   (B) The average of 50 images using Mean
Pixel Values. Note the apparent "hot
pixel" CCD-sensor damage in the lower
right part of the image.
(C) Single image (A) with the common back-
ground (B) removed using Image Arith-
metic's
16.32 Image  Min/Mean/Max
The 'Image Min/Mean/Max' method is located in the "Image Processing" category. It is used
to compute power mean greyscale values from a series of images.
The Power Mean (or generalized mean) Mp with exponent 'p' of the positive real numbers x1,
..., xn is defined as:
-which for p approaching minus infinity will return the minimum of all x-values and for p
approaching plus infinity will return the maximum.
M1 (p=1) is the conventional (arithmetic) mean, and in the limit of p approaching 0 we get the
geometric mean:
The recipe supports predefined p-values of:
p = +∞ (Maximum)
p = 2 (Quadratic Mean)
p = 1 (Arithmetic Mean)
p = 0 (Geometric Mean)
p = -1 (Harmonic Mean)
p = -∞ (Minimum)
The Power means for a given series of values can be ordered as follows:
Maximum >= Quadratic >= Arithmetic >= Geometric >= Harmonic >= Minimum
The formula for power mean is defined with positive x-values in mind, but Maximum, Quad-
ratic, Arithmetic and Minimum can be computed for negative values as well.
The Geometric and Harmonic mean has been designed to return zero if just a single non-pos-
itive greyvalue is found among the input greyvalues.
PAGE | 522
PAGE | 523
Example of input image of a flame
Power means of a series of flame images as the on shown above.
From top left to bottom right:
Maximum, Quadratic, Arithmetic,
Geometric, Harmonic, Minimum
Histograms of the power mean images above (with logarithmic y-axis)
PAGE | 524
PAGE | 525
16.33 Image  Processing  Library (IPL)
As the name implies the Image Processing Library contains a library of tools for Image Pro-
cessing. Input must be an (ensemble of) image(s) and output will be images also (as opposed
to 'Image Analysis', which generally extract some sort of numerical data from the input
images).
In the left hand side of the recipe the available tools are listed in categories and in the right
hand side the ones chosen are listed in the order they will be executed:
The drop down 'Category' offers various types of image processing tools, each listed in 'Avail-
able'. Having selected a method in the list you can double-click it or press the '>>' button to
have it moved to the list of 'Selected' methods. It will be inserted just below the one currently
highlighted in the list (typically the last). Already chosen methods can be removed by high-
lighting them and then pressing 'Delete' or the '<<' button. Some Image Processing methods
have parameters or properties, which you can access by double-clicking the method in the
'Selected' list or by pressing 'Properties...' while the method is highlighted. The methods
chosen will be applied one after another starting from the top of the list and returning the
final result after the last methods has completed. You can change the order by highlighting a
method and then press the 'Up' or 'Down' buttons to apply that method sooner or later. In
the example above 'Pixel Normalization' is first applied to the input image. On the result of
that 'Peak Search' is applied and the result of that is then input to 'Custom Filter' and so on
until the final 'Remove Outliers', the result of which will be returned and stored in the Dynam-
icStudio Database.
You cannot see the intermediate results, so analysis methods are often added one by one,
pressing 'Apply' each time and perhaps tuning various 'Properties', before the next step in
the analysis is added.
The filters featured in the IPL module can be used to smooth images (Low-pass), detect edges
(High-pass), enhance image contrast (Low-pass & Morphology) as well as for non-linear cal-
culations (Signal processing). It also includes various image-processing tools (Utility and
Threshold). Finally a Custom Filter is available to allow filtering with user defined filter kernels.
16.33.1 Low-pass filters
Mean filters
The mean (NxN) filter is the simplest linear, local filter used to smooth images. This filter does
not take spatial gradients inside the kernel into consideration. Thus, for applications related to
fluid mechanics, kernel sizes of (3x3) or (5x5) are recommended. Larger kernel sizes may sig-
nificantly increase numerical diffusion.
The NxN mean filters use very simple convolution kernels:
3x3:          1               1               1
1               1               1              /9
1               1               1
5x5:       1         1         1         1         1
1         1         1         1         1
1         1         1         1         1        /25
1         1         1         1         1
1         1         1         1         1
7x7:     1      1      1      1      1      1      1
1      1      1      1      1      1      1
1      1      1      1      1      1      1
1      1      1      1      1      1      1     /49
1      1      1      1      1      1      1
1      1      1      1      1      1      1
1      1      1      1      1      1      1
9x9:    1    1    1    1    1    1    1    1    1
PAGE | 526
PAGE | 527
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1   /81
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1
1    1    1    1    1    1    1    1    1
Median filters
Median filters are non-linear filters that sorts the (NxN) elements by intensity (grayscale or
scalar value such as concentration or temperature) and replace the center pixel of the kernel
by the median value. The median filter thus eliminates high-frequency noise but preserves
edges. For applications related to fluid mechanics, the median filter is better suited than the
mean filter. On the other hand, for applications related to solid state physics (e.g. surface
deformation), mean filters are generally recommended
Minimum & Maximum filters
Maximum or minimum filters are another kind of nonlinear filters used to remove localized
high-frequency noise.
Gaussian filters
Gaussian-filter is another type of linear filter. With this operator, the new value at the center
pixel of the kernel is calculated as the two-dimensional Gaussian distribution. As opposed to
other linear low-pass filters, this filter weighs the grayscale value at the center of the kernel
higher than those near the edges.
?The NxN Gaussian filters use the following convolution kernels:
3x3:         1              2              1
2              4              2             /16
1              2              1
5x5:       2         7        12        7         2
7        31       52       31        7
12       52     127     52       12     /571
7        31       52       31        7
2         7        12        7         2
16.33.2 High-pass filters
All high-pass filters available in the module are used for edge identification. The result
obtained depends on the numerical recipe applied:
High-Pass filters
The NxN High-Pass filters use the following convolution kernels, to subtract the local mean
(albeit without the divisor used for the NxN kernels described above):
3x3:         -1             -1             -1
-1             8              -1
-1             -1             -1
PAGE | 528
PAGE | 529
5x5:     -1      -1      -1      -1       -1
-1      -1      -1      -1       -1
-1      -1      24      -1       -1
-1      -1      -1      -1       -1
-1      -1      -1      -1       -1
The lack of a divisor may lead to grayscale overflow and the 5x5 high-pass filter will in general
produce brighter images than the 3x3 high-pass as can be seen in the examples above.
Prewitt filter
The Prewitt filter is in fact a combination of two filters, each using a 3x3 convolution kernel to
estimate horizontal and vertical gradients respectively:
Horizontal, Gx:                                 Vertical, Gy:
-1      0      1
-1      0      1
-1      0      1
1      1      1
0      0      0
-1     -1     -1
Having estimated the (signed) gradients in both horizontal and vertical direction, the final res-
ult is calculated as the total magnitude of the local gradients by taking the square root of the
sum of the squared gradients:
Prewitt = Sqrt( Gx2 + Gy2 )
Roberts
The Roberts filter also calculates local grayscale gradients in two orthogonal directions and
returns the total magnitude of local grayscale gradients. The gradients are however determ-
ined in directions of ±45 degrees using 2x2 convolution kernels:
+45°, G+45:                                      -45°, G-45:
0           1
-1          0
-1          0
0           1
Again the final result is calculated as the total magnitude of the local gradients by taking the
square root of the sum of the squares:
Roberts = Sqrt( G+452 + G-452 )
Sobel
The Sobel filter also calculates local gradients, but returns to 3x3 convolution kernels to estim-
ate horizontal and vertical gradients respectively. The Sobel filter differs from the Prewitt fil-
ter by assigning higher weight to the center of the kernel than to the corners:
Horizontal, Gx:                                 Vertical, Gy:
-1      0      1
-2      0      2
-1      0      1
1      2      1
0      0      0
-1     -2     -1
Having estimated the (signed) gradients in both horizontal and vertical direction, the final res-
ult is calculated as the total magnitude of the local gradients by taking the square root of the
sum of the squared gradients:
Sobel = Sqrt( Gx2 + Gy2 )
Laplacian
The 3x3 Laplacian filter is in fact identical to the 3x3 High-Pass filter, while the 5x5 Laplacian
can be interpreted as a 3x3 Gaussian followed by a 3x3 Laplacian:
3x3:         -1             -1             -1
PAGE | 530
PAGE | 531
-1             8              -1
-1             -1             -1
5x5:     -1      -3      -4      -3       -1
-3       0        6        0       -3
-4       6      20      6       -4
-3       0        6        0       -3
-1      -3      -4      -3       -1
Again the lack of a divisor means that results from a 5x5 kernel are in general brighter than
those from a 3x3 kernel.
16.33.3 Morphology filters
Morphology filters is a class of nonlinear filters, which in their most basic form corresponds to
the minimum and maximum filters. Combining these in different ways can however produce
more advanced results.
Dilation & Erosion filters
A?Dilation filter will let bright pixels flood darker neighbors within a 3x3 neighborhood. This is
basically the same as the Maximum filter, but applying the filter repeatedly you can effectively
produce kernels significantly larger than supported by the Maximum filter.
An Erosion filter works the opposite way by letting dark pixels flood brighter neighbors within
a 3x3 neighborhood. This is basically the same as the Minimum filter, but applying the filter
repeatedly you can again produce much larger filtering kernels.
Neighborhood Radius            1            2            3            4           ...                  N
Resulting kernel size            3 x 3      5 x 5      7 x 7      9 x 9        ...        2N+1 x 2N+1
Opening & Closing filters
The Opening filter is a combination of N Erosions followed by N Dilations. Bright areas smaller
than the kernel will disappear and neighboring dark areas will tend to merge. (Kernel size
depend on N the same way as explained above for Dilation and Erosion filters). For images
with small bright particles on an otherwise dark background, the opening filter can be used
for background estimation, since a sufficiently large kernel will ensure that all (isolated)
particle images are removed.
The Closing filter does the opposite by starting with N Dilations followed by N Erosions. Dark
areas smaller than the kernel will disappear and neighboring bright areas will tend to merge.
This can also be used for background estimation if you're looking at dark objects on a bright
background.
Tophat & Blackhat filters
The Tophat and Blackhat filter computes the difference between the original image and its
Opening or Closing respectively:
o  Tophat(Image) = Image - Opening(Image) ... also known as "White Tophat" filter
o  Blackhat(Image) = Closing(Image) - Image ... also known as "Black Tophat" filter
As described above the Opening filter can be used for background estimation when applied to
images of bright particles on a darker background. The Tophat filter subtracts the opening
from the original image, so in effect it is used for background removal. This requires of course
that no particles remain after the opening, since otherwise they will disappear when sub-
tracting the estimated background from the original image.
The Blackhat filter does exactly the same, but looks for dark objects on a brighter back-
ground. As a side-effect the output is inverted compared to the original so it will show bright
objects on a black background.
Shock filter
The Morphological Shock filter is also known as morphological sharpening. In its most basic
form the filter compares each pixel to the minimum and the maximum greyscale value in its
neighborhood and returns whichever is closer. If the current pixel value is exactly midway
between the local minima and maxima the pixel keeps its current value. A "raw" shock filter is
very sensitive to noise and to reduce this sensitivity a median filter is applied first. The
min/max (erosion/dilation) filter is then applied to the median filtered image and the local min
PAGE | 532
PAGE | 533
or max returned as described above.
Gradient filter and Distance Transform
The morphological gradient filter is calculated as the difference between a 1-pass Dilation and
a 1-pass Erosion.
For all pixels in the image the Distance transform will measure the distance to the nearest
zero-valued neighbor pixel. Pixels that are already zero will thus remain zero, while nonzero
pixels will have their grayscale value replaced with the approximate distance to the nearest
zero-valued pixel.
16.33.4   Thresholding
Remove Outliers
The thresholding filter Remove Outliers will allow you to set upper and lower limits on the
accepted grayscale values in the image. Grayscale values outside the specified limits can be
set to the limit values or set to the minimum and maximum values supported by the image
(f.ex. 0 and 255 for an 8-bit image or 0 and 4095 for a 12-bit image as in the example below).
16.33.5 Utility filters
Image analysis often requires image manipulation like rotation, scaling and pixel shift among
many other operations. The IPL module contains many methods to help you manipulate
single- and double-frame images.
Rotate
The method 'Rotate' will rotate the parent image around it's own center as specified in the
properties for the analysis:
Positive angles rotate the image counterclockwise and negative angles rotate clockwise.
The grayscale values of the resulting image are computed by interpolating between neigh-
boring pixels in the parent image and you must choose interpolation method 'Nearest Neigh-
bor', 'Linear', 'Cubic' or 'Catmull-Rom'. As the name implies 'Nearest Neighbor' simply picks a
greyvalue from the nearest neighbor pixel in the parent image, while 'Linear ' interpolate
between the nearest 2x2 pixels in the parent image. 'Cubic' interpolates using a third order
polyomial fitted to the nearest 4x4 pixels in the parent image. 'Catmull-Rom' uses the same
4x4 pixel neighborhood, but fits a spline to them to perform the interpolation.
By design the image size is maintained, meaning that corners of the original image will often
be cut, while corners of the derived image will be black since they originate from outside the
parent image. Both of these effects can be seen in the example above.
Scale
The method 'Scale' will scale the image horizontally and/or vertically by user defined scaling
factors:
PAGE | 534
PAGE | 535
Horizontal and Vertical Scale Factor are set independently and values above one will increase
image size while values below one will reduce it.
As for 'Rotate' output pixel values are computed by interpolating between neighboring pixels
in the parent image and you must choose an interpolation method.
As opposed to 'Rotate' 'Scale' will in fact change the size of the image so the derived image will
typically be larger or smaller than its parent.
Shift
The method 'Shift' will move pixels in the parent image left/right and/or up/down according to
user specifications:
Horizontal and Vertical translations are set independently and may both be positive or neg-
ative, but must be integer pixel counts.
The image size is maintained and as shown above part of the parent image will be lost as it
moves outside the frame while parts of the child image will become black as it originates from
outside the parent.
Mirror
The method 'Mirror' will mirror the parent image Left/Right, Up/Down or both as specified in
the properties:
If you fail to check at least one of the options you will get an error message when you try to
apply the analysis.
The top left image shows the parent image, the rest are mirrored Left/Right, Up/Down or
Both.
So far the 'Utility Filters' have manipulated the position of pixels, but tried to change grayscale
values as little as possible.
The remaining methods in this group will leave pixels where they are, but change the gray-
scale values:...
Invert Pixel Values
The method 'Invert' will produce a negative image, where black pixels become white and vice
versa:
The method adapts to the grayscale depth of the parent image and has no user defined set-
tings.
For display purposes a similar effect can be accomplished via the "Color map and Histogram "
(on page?992), but without changing the grayscale values of the image.
Pixel Normalization
PAGE | 536
PAGE | 537
Normalizing grayscale values is a useful preprocessing step if for example you wish to per-
form thresholding, but cannot find a suitable threshold value to apply across the entire image.
Classic normalization is performed by subtracting the Mean and dividing by the Rms derived
from some meaningful neighborhood around each pixel. Traditional Mean and Rms cal-
culations are however quite sensitive to noise, so in practice the Mean is replaced by the
Median (MED) and the Rms replaced by Median Absolute Deviation (MAD), both of which are
much more robust statistical quantities:
where gIn and gOut are input and output grayscale values of the pixel in question and MEDΩ
and MADΩ are the Median and Median Absolute Deviation in the spatial neighborhood Ω
around this pixel. The minimum noise level ε is included to avoid division by (almost) zero in
areas with more or less constant grayscale values.
The output image will inherit the grayscale depth from its parent, floating point images are
not supported. Since the input is divided by ε or higher we scale up the result by ε to preserve
dynamic range as much as possible.
If the background follows a Gaussian distribution the standard deviation σ is related to the
MAD?as:
σ ? 1.4826 ? MAD
The classical interpretation of S/N-Ratio assumes implicitly that noise is Gaussian and that data
has been normalized by division with σ, meaning in this case that output pixels with grayvalue
1.4826 ?ε can be considered to have a S/N-Ratio of 1. For practical purposes you can multiply
by 1.5, so if for example ε=4, grayscale values of 1.5 ? 4 ? 6 = 36 are 6 times higher than typical
fluctuations in the neighborhood.
Depending on the experiment , grayvalues significantly above a certain threshold can either
be interpreted as shot noise or identify a signal (e.g. a particle) that rise significantly above the
noise floor.
The neighborhood over which the Median (and the MAD) is computed is chosen as 'Kernel
Size' in the Normalization properties:
A median filter with a square kernel may produce horizontal and/or vertical artifacts in the out-
put and to remove those the median filter is in fact applied twice. This means that we are in
fact computing the median of medians.
Kernel Size must be odd and can be chosen from 3x3 to 15x15 pixels. Statistically bigger is bet-
ter, but especially for 10- or 12-bit images median filtering with large kernels is time-con-
suming, and most often Kernel Sizes of 9x9 or 11x11 is sufficient. The smallest kernel size of
3x3 is not recommended except for sparsely seeded flows where particle images are on the
order of 1 pixel in diameter, and even there a 5x5 median filter will be much more reliable
and only marginally slower.
Please note that pixels darker than the median (background) will be truncated at zero. This is OK
if you're looking at bright particles on a dark background, but if you're looking at dark objects
(shadows) on a bright background you should invert the image before normalizing it.
The minimum noise level ε can be chosen as 'Auto', 2, 4, 8 or 16.
'Auto' will pick the value 4 for an 8-bit parent image and 8 for images with pixel depth larger
than 8 bits.
Peak Search
As implied by the name the method 'Peak Search', looks for greyscale peaks in the input
image. Technically 'Peaks &?Plateaus' would be more correct since the method identifies pixels
that are brighter than or equal to their neighbors in a defined neighborhood:
The neighborhood is octagonal with a diameter of 3, 5, 7, 9, 11, 13 or 15 pixels (O3 is in fact
3x3 square).
To avoid detecting bumps in the noise floor the grayscale value must also be greater than or
equal to the specified threshold.
A?threshold value of 0 means 'Automatic' and the code will choose 24 for an 8-bit parent
image and 48 for images with greater bit depths.
For floating point images there is no logical 'Automatic' choice, so the user must actively
choose a value and Threshold=0 actually means 0 in this situation.
For floating point input images the output will be 8-bit, otherwise the output image inherits
the grayscale depth from its parent.
Please note that output is binary containing only 0's and 1's, so the image will typically appear
all-black until the "Color map and Histogram " on page?992 is adjusted.
Note also that many of the 1's will be isolated pixels, that may not show if the image is dis-
played at less than 100%?zoom.
PAGE | 538
PAGE | 539
Above we've zoomed in on part of an image from a PIV experiment. Topmost is the input
image, inverted for display purposes showing gray particles on a white background. In the
middle output from the Peak Position analysis is shown and at the bottom the two images are
overlaid.
As shown clusters of 2 or more neighboring 1's may occur for example in regions where the
camera has been saturated, post-processing may be required if this is a problem.
In many cases preprocessing of the images will be required before a peak search is attemp-
ted. You could for example apply Pixel Normalization as described above. With PIV?images of
reasonable quality a simple (and much faster) high-pass filter might suffice.
16.33.6 Signal processing
Discrete Cosine Transform (DCT) are used to transform intensity levels into (Amplitude, fre-
quency) domain.
Inverse Discrete Cosine Transform performs the opposite transformation.
16.33.7 Custom  filter
Finally the image processing library offers you the possibility to apply a linear filter with a con-
volution kernel of your own design. Square, odd-sized kernels from 3x3 to 15x15 are sup-
ported and filter coefficients can be either floating point or integer values. In the former case
processed images will also be floating point, while the latter will produce images with the same
grayscale depth as the parent image. For integer image formats grayscale values will be trun-
cated at zero and at the upper limit (f.ex. 255, 1023 or 4095 for 8-, 10- or 12-bit images
respectively). To avoid or reduce overflow you can specify a filter divisor when you've chosen
integer filter values. The default is a 3x3 integer kernel with one in the center, zero else-
where and a divisor of 1. This default kernel does not do anything it is up to you to assign
meaningful filter coefficients.
The example below is of little practical use, but illustrates well how the custom filter is used. A
very simple filtering kernel with just two nonzero filter coefficients, 9 pixels apart will shift the
originial image 4 pixels left and right, add the two shifted images and finally divide by two as
specified by the Filter divisor:
A few examples are shown below, illustrating how some of the built-in filters are implemented
using linear filtering kernels...
The 3x3 Mean filter simply adds grayscale values in a 3x3 neighborhood and divides the sum
by 9:
PAGE | 540
PAGE | 541
The 3x3 Gaussian filter multiplies each grayscale value in a 3x3 neighborhood by 1, 2 or 4,
adds the resulting numbers and divides the sum by 16:
The classic 3x3 Gaussian above has a variance of 0.5 pixels, i.e. a standard deviation of sqrt
(0.5)=0.71.
Applying the Gaussian repeatedly the variance adds up, so using the 3x3 Gaussian twice
should correspond to a single Gaussian with a variance of 2x0.5=1.0, i.e. standard deviation of
1.0. You must however anticipate roundoff errors to accumulate when applying the same
(Gaussian)?kernel repeatedly and you can accomplish the same more accurately by using a
single convolution with the following 5x5 kernel (not equal to the predefined 5x5 Gaussian):
Applying this kernel repeatedly, the variance also add up, increasing by one each time. Again
roundoff errors will accumulate and using the 5x5 kernel above twice is the same as applying
this 9x9 kernel once:
You can in principle keep increasing the size of the kernel, but the largest kernel supported is
15x15 and already there it becomes impractical to type in each of the coefficients manually.
Please note that you can copy/paste them to/from Excel or similar.
As you can see in the examples above each cell is color coded based on its value: Positive val-
ues have a red background, zero values a white one and negative values a shade of blue.
Nonzero values are furthermore divided in 'close to zero', which are pale and 'far from zero',
which have a more saturated color. This is illustrated more clearly in the 'Mexican Hat' type of
filter kernel below:
PAGE | 542
PAGE | 543
The color coding is for visualization purposes only and has no influence on the results when
the kernel is applied.
In the bottom right hand side of the Custom Filter Properties a few key parameters of the cur-
rent kernel are listed: The sum of all coefficients, the sum of positive coefficients, the sum of
negative coefficients, the maximum value and the minimum value.
If one or more of the coefficients are wrong the color coding and/or the numerical values on
the right may help you discover and correct the mistake before applying the kernel.
If you resize an existing kernel the coefficients will be preserved as much as possible; If you
increase the filter size the present kernel will be padded with zeros to match the new size and
if you reduce the filter size the present kernel will be cropped, preserving the central values
that fit the new size. If you changed filter size in order to enter a completely new kernel from
scratch you may wish to 'Clear kernel' to remove the coefficients left over from the old ker-
nel; This will set divisor to one and all coefficients to zero except the central one, which will be
set to one. Such a filter will do nothing if applied, it is up to you to enter nonzero kernel coef-
ficients to create a filter of your own design.
Kernel Library
Last but not least you have the option to save custom filter kernels in a kernel library so you
can use them again later:
To save a kernel that you have typed in, press the button 'Save Kernel' in the right hand side
to enter the following dialog:
Please replace 'My kernel' with a name by which you can later recognize the kernel. Optionally
you may also add comments or further description in the 'Info text', which by default contain
just a time-stamp for when the kernel was saved. A?small thumbnail of the color-coding is also
created to provide visual clues beyond the name to help you find the kernel later. When you
press 'OK' you will switch to the kernel library and see the newly created kernel stored there
along with other custom kernels (In this case the kernel was renamed to 'O15 Mexican Hat',
no such kernel will be available to you unless you create it yourself):
This is also the dialog you reach if you press 'Load kernel' from the custom filter recipe. You
can pick any of the kernels in the library and press 'OK' to load it. You can also rename or
delete existing kernels (except a handful of kernels that are predefined).
Finally you have the option to 'Export' or 'Import' a kernel as a small file in order to e.g. share
filter kernels among colleagues.
Please Note: The kernel library is used exclusively to fill in coefficients and divisor of a custom
filter as if you typed them in manually. Once you press 'OK' or 'Apply' the divisor and coef-
ficients used will be stored along with the results of the image processing applied, but the
PAGE | 544
PAGE | 545
name of the kernel is not stored anywhere but in the library itself. This means that even if ker-
nels in the library are later renamed or deleted, images processed using those kernels will
maintain complete traceability.
16.34 Image  Resampling
16.34.1 Re-sampling window
Using re-sampling option, scalar images can be sampled according to a grid, which cell size is
defined by the user (Parameter 'Stepsize'). Select the 'Default' button to get min/max pixels
values with the camera used and refine the grid geometry with the option available; namely
the type of grid (square/rectangular), the size of the cells (in pixel) and the region of interest
(X and Y coordinates).
This re-sampling calculation is particularly interesting for the overlapping of two camera; e.g.
for cross-correlation calculations with PIV/LIF set-up (see the Reynolds flux method). Simply
select the vector map describing the view correction and always remember to correlate from
the PIV-camera to the LIF-camera so the transformation is performed in the proper direction
when re-sampling LIF images. For further details and "how to do" help on this topic please
refer to the LIF manual.
16.34.2 Re-sampled maps
Scaling on the resulting re-sampled scalar maps can be added: Right-click with the mouse on
the map and select the option 'Info box'. The coordinate system (Pixel or SI) can be added in a
similar way, selecting the appropriate 'Rulers' option.
16.35 Image  Resolution
This method is used to modify the pixel (bit) resolution of raw single- or double-frame images
(e.g. 10-bit to 8-bit). Utilization of the method is simple: Select or multi-select the image(s) of
interest and call the method 'Reduce pixel resolution' located in the category 'Image con-
version'. Set the new pixel bit resolution, press the 'Apply' and 'Display' buttons to pre-view
the result and then the 'Ok' button to accept it (and extend the re-sampling to every images
when using multiple selections).
PAGE | 546
PAGE | 547
Image bit resolution is changed easily: Set the new bit-resolution and press the 'Ok' button to
accept the re-sampling.
16.36 Image  RMS
Gives a measure for the statistical spread of the pixel values for independently acquired
images; the method calculates the standard deviation of corresponding pixels from their
mean value. The result is delivered as an integer, since the dataset is treated as a an image.
A minimum selection of two images is required. The resulting output image is placed under
the highlighted dataset (the one selected last).
Notes
l  "Corresponding pixels" means "pixels with identical x- and y-coordinates" starting with
pixel (1,1) in the lower left corner.
l  Make sure that the input images are compatible in terms of image dimensions and gray-
scale resolution. When trying to calculate the RMS pixel values for images of different
dimensions a warning will be issued. If you proceed, the output image will inherit the
dimensions from the parent dataset; input images of different dimensions are cropped or
zero-padded accordingly.
16.37 Image  Stitching
Image stitching combines images of the same dimensions into a single large image map. The
resulting image map is a matrix of the input image maps. The advantage of stitching is that a
customer can combine images from several fast lower-resolution cameras, as opposed to
acquiring with a single slow high-resolution camera.
Setup is as follows:
l  User selects image maps as input.
l  The number of column and row images is specified.
l  For each element in the matrix the selects the input image map to position.
l  Offsets are applied if necessary, relative to the boundaries of each image element
in the matrix.
The interfaces for images differs from vector stitching in that stitching images in the literal
sense means to align images side by side in a matrix fashion, whereas vectors are merged
(technically speaking you can’t stitch vectors). This is, of course, a vast simplification since no
mention is made of overlaps, rotation or translation when aligning cameras. A simple image
matrix up to 4 x 4 images where the user can apply small correction offsets. Images “own” the
matrix element they are assigned to. Any image overlaps is automatically cropped.
PAGE | 548
PAGE | 549
Final image layout: setup governing the layout of images in the final image map.
Row layout: number of rows in the image map matrix.
Column layout: number of columns in the image map matrix.
Applying offsets (x=+50, y=+20) as seen in the diagram below for a 1 x 2 element image mat-
rix:
16.38 Image  Distortion  Correction
This function is aim to dewarp highly distorted images, which cannot be handled by Image
Dewarping. It first determines a transformation grid from target image, and then applies this
transformation matrix to dewarp corresponding particle images or other acquired images,
which has the same distortion as the target images used.
In order to use this function, the user need to acquire target image(s) under the same dis-
tortion conditions. The target image can be either single frame image or double frame image.
It is possible to use single frame target image to dewarp a double image pair, providing the
distortion difference between two frames of image pair to be dewarped is negligible small. In
some applications, the distortion is position dependable, such as engine in-cylinder meas-
urement. In this case, the user can take one target image at each position where images are
to be acquired for measurement. The target image at each position will be stamped with the
position information by cyclic synchronizer or linear synchronizer, and used to generate a
transformation grid at this position, to be used to dewarp the measurement image at the
same position. Notice in this case, more than one target image at one position will not be
accepted by this function.
16.38.1 Determine transformation  grid from  target image
1. To determine transformation grid, select the target image(s) ensemble, and start Image
Distortion Correction from analyze method library from category of Image Processing.
PAGE | 550
PAGE | 551
Fig 1. Select the target image(s)
Fig 2. Select Image Distortion Correction from analyze method library
Fig 3. Recipe of Image Distortion Correction
2. Define the grid:
l  The number of points in X and Y direction
l  The distance between two dots of grid (in the same direction) in real world (in mm)
l  The orientation of X and Y direction
3. After defined the grid, generate the Grid by click ‘Set Grid’. The out frame of this grid will
be displayed then on top of images. The one on the right hand shows the original target
image (distorted), the one on the left bottom shows the target image after distortion cor-
rection.
Fig 4 Grid initialization
4. Move the four corner of the grid’s frame to the corner of interested area of target image.
PAGE | 552
PAGE | 553
Fig 5 move the corner
5. Subdivide the grid by first left click the row or column, then right click and select ‘Subdivide
Row or Subdivide Column’. Move the new generated grid point to corresponding dot in the dis-
torted target image by dragging it.
Fig 6 (a) subdivide row 1;
(b) subdivide row 2;
(c) subdivide row 3
Repeat step 5, until the target image shown in ‘Logical Coordinates’ window (left bottom
corner) looks undistorted.
PAGE | 554
PAGE | 555
Fig 7 make grid
After the transformation grid is generated, the user can select origin from four corners of
the transformation grid.
Fig 8 set origin
6. Move to next target imagery slide the slide bar in Image Settings ->Selected Image value.
In some applications, distortion varies as the optics moves. If this is the case, you must acquire
at least one target image at each position you will acquire particle images. Then apply image
distortion correction to all acquired target images, and use the result to correct the distortion
of acquired particle images. If a cyclic synchronizer or a linear synchronizer is used to acquire
images, the angle/position information will be inherited to transformation grid. In the next
step to use transformation grid to correct distortion of particles, it will look for the trans-
formation grid with the same angle/position stamp as particle images to correct the cor-
responding particle image.
7. Click Ok or Apply in the right hand side to save the transformation grid.
16.38.2 Use transformation  grid to correct image distortion
Select the transformation grid to be used, right click the image to be corrected and select Ana-
lyze. The distortion corrected image is shown in the left bottom corner. Press Apply or OK,
the particle images will then be dwarfed and saved.
Fig 9 correct distortion
16.39 Imaging  model  fit (Legacy Method)
This analysis method is obsolete and included for backward compatibility only.
The recipe is read only and no further development or maintenance will be made.
It will be removed completely in a future release of DynamicStudio.
We recommend using "Multi Camera Calibration" on page?585 instead.
Content:
l  Normal use and background
l  Target Library & Custom Targets
l  Acquiring calibration images
l  The recipe for Imaging model fit
l  Displaying imaging model parameters
PAGE | 556
PAGE | 557
16.39.1 Normal use and background
The Imaging model is a mathematical model that describes how points in "Object Space" (typ-
ically using mm-coordinates) is transferred to the "Image Plane" (where positions are meas-
ured in pixel coordinates). Depending on the choice of imaging model the transformation may
describe perspective, lens distortion (e.g. barrel or pincushion distortion) or some other arbit-
rary spatial distortion. All imaging models have a number of parameters and determining the
value of these parameters in a specific image acquisition setup is referred to as "Imaging
Model Fit", since parameters are fitted to give the best possible match between cor-
responding coordinates in object space and image plane.
The purpose of the Imaging Model Fit is to facilitate later measurements in real world metrics
(e.g. millimeter in object space) on the basis of pixel coordinates in acquired images.
An Imaging Model Fit is thus a required input to the following numerical methods: Stereo PIV
Vector Processing, Image Dewarping, Vector Dewarping, IPI Particle Sizing
A full 3D model describes how points in the object space (world) are mapped onto the image
plane (sensor-chip) of the camera, while a 2D model describes only the mapping of points
from the object plane (light-sheet) to the image plane.
Both types of imaging models include a number of parameters, which uniquely determines
the object-to-image mapping for a particular image recording setup. Moving the camera or
the light-sheet or otherwise changing the image recording setup will require the imaging
model parameters to be recomputed.
DynamicStudio supports a number of imaging models, each with their strengths and weak-
nesses.
Below is a short description of the basic principle of each imaging model.
Imaging model                                     Pros                                                              Cons
"Direct Linear Transform
(DLT)" (on page?571)
Direct linear transform is a lin-
ear affine transform to describe
the position, rotation, scaling
and perspective of the object
space.?
Simple to use, requires only one
image for 2D.
Strictly linear transform, unable
to model any non-linear dis-
tortion.
For 3D calibration a precise tra-
verse system is needed.
"3'rd order XYZ polynomial
imaging model fit" (on
page?572)
A polynomial in 3D that will trans-
form the object space to the
image plane. This model does
not relate to physical measures
like translation and rotation of
the object space.?
Simple to use, requires only one
image for 2D.
Capable of handling severe dis-
tortion caused by the lens or
curved windows.
For 3D calibration a precise tra-
verse system is needed.
Fails with a multilevel target
since 2 Z-levels is not enough.
Poor extrapolation beyond the
target markers found.
"Pinhole camera model" (on       Does not require a precise tra-     In general this model requires
page?573)
Is sort of a combination of the
other two. Hence it uses a DLT
in combination with a poly-
nomial, the latter is used to
model the lens distortion by
using a simplified lens model
(a.k.a. the pinhole model with
radial and tangential distortion)
verse system as it does not
need knowledge of the Z-
coordinates of the target plate.
The model relates to physical
properties of the imaging sys-
tem. (Focal length, optical axis,
translation, rotation, etc...)
Can to some extent extrapolate
beyond the target markers
found.
more images than the other
two.
"Telecentric camera model"
(on page?576)
Mathematically identical to the
Pinhole camera Model, with
some constraints to handle Ima-
ging without perspective
Capable of describing Tele-
centric Lenses without per-
spective.
Requires multiple images from
multiple cameras/views and thus
available only via 'Multi-Camera
Calibration'
For some imaging models the parameters can in principle be calculated from known angles,
distances, etc, but in practice this approach is not feasible. In the actual laboratory setup it is
often difficult if not impossible to measure these angles, distances and so on with sufficient
accuracy. Therefore all imaging model fits require images of a calibration target, which are
analyzed to determine the imaging model parameters.
Internally the Imaging model fit method consists of two separate tasks. The first task is to
identify the pixel coordinates of known points in object space. The second task is to estimate
or fit the model that will transform the 3D object points to 2D image points. The first task is
achieved by placing a calibration target with identifiable target markers. The software will auto-
matically detect these markers in the image, but the position of the target markers (in object
space) need to be known. The second task of fitting the imaging model is carried out using a
modified least square method.
2D imaging models requires that the calibration target is aligned with the light sheet and usu-
ally a single image is sufficient.
For a full 3D imaging model you must either use a multilevel target or acquire multiple images
of a plane target positioned in front of, centered in and behind the light-sheet. In at least one
of these images the target should be aligned with the lightsheet and with the origin at the
desired position.
The calibration target contain markers that defines the object coordinate system and proper
illumination is required to ensure that all markers are clearly visible on the recorded image(s).
Please verify also that the calibration markers covers as much as possible of the cameras field
of view.
The figures below illustrate different calibration targets that the imaging model fit can use.
Most of them are available as both single- and double-sided targets (a double sided target has
markers on both sides and is required for image acquisition setups, where cameras are on
opposite sides of the lightsheet).
PAGE | 558
PAGE | 559
Plane dot matrix target
The larger center dot (zero marker) is used to identify the origin of the coordinate system.
Multilevel dot matrix target
Half the markers are in the same level as the zero marker, the other half are in a 2nd level,
closer to or farther from the viewer.
Checkerboard target (plane)
Origin and orientation of the coordinate system is shown in red
16.39.2 Target library & Custom  targets
DynamicStudio maintains a library of known calibration targets. This library can be accessed
from the 'Tools' menu:
PAGE | 560
PAGE | 561
The Target Manager is a small program launched from inside DynamicStudio, listing all known
targets and offering the possibility of adding new (custom) targets to the list or edit the tar-
gets already in it:
It is possible to edit specifications for predefined targets, but instead it's recommended to cre-
ate a custom target by pressing 'New...':
Specify a name that you will be able to recognize, preferably describing the target so you can
discriminate between different ones. Two target types are supported, 'Dotted' and 'Checker
board', and when you press 'OK' the resulting dialog will depend on the target type chosen
.
If you choose the target type 'Dotted', you will be asked to provide information about dot spa-
cing, dot sizes etc:
The zero marker should always be larger than all the other markers and ~50% of the dot spa-
cing will usually work well.
The main markers must not exceed 50% of the dot spacing and ~33% of the dot spacing is nor-
mally a good choice.
The axis markers (the four nearest neighbors of the zero marker) must be smaller than or
equal to the main markers. Try f.ex. 25% of the dot spacing.
For multi-level targets, put a checkmark in 'Enabled' and specify what the level distance is. This
parameter describes the z- (out-of-plane-) distance from the level where the zero marker is to
the level of the other markers. This parameter is signed in order to distinguish whether the
second level is closer to or further from the camera than the zero marker level. If the zero
marker level is closer to the camera, the level distance should be negative. Depending on the
camera viewing angles you intend to use 20% of the dot spacing is normally a good choice. For
very steep angles acceptable level distance becomes smaller, for modest viewing angles lar-
ger level distance may be OK.
For double-sided targets, put a checkmark in 'Enabled' and specify what the thickness is. This
parameter describes the difference in z-distance for the same marker when viewed from
front and rear. It must be greater or equal to zero.
Finally you must specify whether the (custom) target contain black dots on a white background
or white dots on a black background.
PAGE | 562
PAGE | 563
If you choose the target type 'Checker board' there are fewer parameters to specify, just the
number of tiles horizontally and vertically and of course the tile size, describing the spacing
between crossings in the checker board pattern. The number of tiles horizontally and ver-
tically need not be identical, but both should be odd numbers.
Please note the circles inside 3 of the checker board tiles that identify both the zero position
and the orientation of axes. They are not specifically mentioned in the target definition, but
the zero marker MUST be in the center of the target and it MUST be a white circle in a black
tile. Nominally the circle diameters should be half the tile size, but slightly smaller diameters
work OK, while larger diameters will cause the calibration routine to fail.
16.39.3 Acquiring calibration  images
Acquiring images for calibration is no different from any other image acquisition, but the
images are saved in the database in a slightly different way in order to include, for example
the Z-coordinate, and let the system know that these images are special. If in doubt about
how to do this, please refer to the general description of this topic.
Using Z-axis  traversing
If you wish to make a 3D imaging model fit based on the DLT or the 3'rd order polynomial
model while using a plane target, you will have to traverse the target through a number of Z-
positions, typically 3-5. For each new position acquire new image(s) and 'Save for Calibration'.
You will have the option to save the images in separate ensembles or put them all in a com-
mon ensemble. Dantec Dynamics can supply a special traverse unit, along with the calibration
target.
Specifying Z-coordinates
Unless calibration images were acquired using free hand positioning of the target (see below)
you will need to specify the nominal Z-coordinate for each of the images. This is done using a
dedicated custom property named simply 'Z'. The custom property may be enabled when stor-
ing the calibration images or later by right-clicking the image ensemble and selecting 'Custom
Properties...' in the context menu. In the resulting dialog put a checkmark beside the prop-
erty named 'Z':
In the 'Record Properties' of the calibration ensemble you will now see a property group,
'Coordinates', containing the custom property 'Z':
PAGE | 564
PAGE | 565
By default custom properties such as the the Z-value will apply to all images in the ensemble,
which may be OK if you stored images from different positions in separate ensembles. If a
single ensemble contains images from several Z-positions you must however specify a Z-
coordinate for each individual image. This is done by opening the ensemble (Ctrl-Shift-Enter or
'Show Contents' from the context menu) and then browsing through the images one at a time
entering nominal Z-values for each.
You are strongly advised to enter the nominal Z-coordinates as soon as possible after acquir-
ing the images while you still remember the positions used and the order in which images
were acquired.
Please remember that all subsequent processing that use the imaging model fit will assume
that Z=0 corresponds to the center of the lightsheet. You should keep this in mind both when
aligning the calibration target with the lightsheet, when traversing and when entering nom-
inal Z-coordinates for each of the calibration images acquired.
When using multilevel targets you must specify not only the Z-coordinate of the zero-marker
level, but also the coordinate of the other level of markers. This is done by means of a Z-offset
specified as part of the target definition. Typically there will be two target definitions for each
target, identical except for the sign of this offset. The examples below illustrate when to
choose one or the other of these two target definitions:
Back view                    2nd Level ? D                             2nd Level + D
Orientation
of
Z-axis & tar-
get
Front view                  2nd Level ? D                             2nd Level + D
"Front view" is here defined as the view, where the Z-axis is pointing towards the camera (cam-
era position will have positive Z-coordinate), whereas "Back view" is defined as the view where
the Z-axis is pointing away from the camera (camera position will have negative Z-coordinate).
Using free hand positioning (Pinhole and Telecentric models  only)
The Pinhole and telecentric models can be used with traversed target plate locations as
described above, but in addition offer a more convenient method, where the Z-position of
the target plate does not need to be known, with the exception of the view that defines the
coordinate system (a.k.a. reference view). When using these methods the target plate must
be placed at different positions and orientations, that covers as vide a range of possible poses
as possible, in order to achieve the best results.
The recommended procedure for acquiring free hand calibration images is as follows:
l  Acquire the reference view. Target plate located in XY?plane and Z=0.
l  Acquire four slightly rotated (10°.. 20°) (+X-axis,-X-axis,+Y-axis,-Y-axis) views
l  Rotate the target plate 90° around Z-axis and acquire a single view.
l  Again acquire four slightly rotated (10°.. 20°) (+X-axis,-X-axis,+Y-axis,-Y-axis) views.
16.39.4 The recipe for Imaging Model Fit
The ensemble with the acquired calibration images (with or without the Z-coordinates spe-
cified) are selected in the DynamicStudio database tree. From the Context menu select 'Cal-
ibrate' and in the resulting dialog, select 'Imaging Model Fit' from the 'Calibrations' group.
PAGE | 566
PAGE | 567
The recipe for Imaging model fit is shown below. Three user selections are required before
the imaging model fit can be performed:
l  Identify the calibration target used.
l  Select an imaging model.
l  Specify the orientation of the coordinate system
Coordinate system orientation These four buttons identify the orientation of the (object)
coordinate system as seen from the camera's point of view. Many other combinations are pos-
sible, but for simplicity the X-axis is always assumed horizontal and the Y-axis always vertical.
(Coordinate axes need not be exactly horizontal/vertical: A slight rotation of the camera
and/or the target is normally not a problem).
Only the positive direction need to be specified (i.e. whether X is positive to the left or to the
right, and whether Y is positive upwards or downwards). The default setting will usually be OK
in a single camera system, but multiple cameras may require different settings if results are
to be compared later on. Imagine for example an experiment in which two cameras are posi-
tioned on opposite sides of the light-sheet: For later comparison of data we want the two cam-
eras to share a common coordinate system, and although they may agree that Y is positive
upwards, they cannot both have X positive to the right, since they are looking at the light-
sheet from opposite sides.
In the example above both cameras are on the same side of the lightsheet and thus see the
same orientation of the coordinate system.
PAGE | 568
PAGE | 569
In the example above cameras are on opposite sides of the light sheet and thus see different
orientations of the coordinate system.
The orientation of the X-and Y-axes is specified during calibration while the orientation of the
Z-axis is specified indirectly through the Z-coordinates supplied by the user for each and
every calibration image.
Save imaging model data as text file Imaging model parameters are stored in the Dynam-
icStudio database in binary form, but if you wish to investigate them outside DynamicStudio,
you have the option to specify a text file in which to store a copy of the imaging model para-
meters.
View/Edit (This button is only available when a dot matrix target is selected). The target ana-
lysis should run smoothly if the calibration markers (dots) are clearly visible against an evenly
illuminated background. If however the target analysis fails to find sufficient markers, it is pos-
sible to adjust the algorithm. See the section Adjusting parameters for finding the dot matrix
target for more information on this topic.
16.39.5 Displaying imaging model parameters
Once the imaging model fit has been completed, the calibration result can be overlaid on top
of a calibration image, by drag and drop from database tree. Hereby a grid is displayed on the
calibration image to show marker positions predicted by the imaging model fit and it can be
verified that it matches the actual markers in the image. The info box below the image display
shows the imaging model parameters:
The first line identifies the type of imaging model used, and the next four lines specify
whether it is a 2D or a 3D model as well as the range of X-, Y- and Z-values covered by the cal-
ibration. The upper and lower limits simply corresponds to the highest and lowest marker-
coordinates found during image analysis, and thus represent a coarse measure of the range
of validity not accounting for the effects of perspective distortion.
If markers on the calibration images do not cover the entire field of view of the camera, later
use of the imaging model will probably require extrapolation beyond the range actually
covered by the model fit. Extrapolating is normally not a problem for the Direct Linear Trans-
form or the Pinhole camera model, but may cause problems in connection with the Polynomial
model, especially if the original images are severely distorted.
An important number to look at when evaluating the quality of the calculated imaging model
fit is the average reprojection error. This number describes the average pixel distance from
every marker found to the predicted image location (in the figure below it is the distance
PAGE | 570
PAGE | 571
from a yellow circle to the green grid crossing). The smaller the average reprojection error is
the more accurate is the imaging model fit. For a normal system equipped with a low dis-
tortion lens the average reprojection error should lie well below one.
The figure above shows a zoomed view of the graphical display for the imaging model fit. The
red lines represent the object space coordinate system. The yellow circles are the target mark-
ers found in the image by the automatic target marker detection. Finally the green grid is
based on the target points (in object space) mapped to the image through the fitted imaging
model.
16.39.6 Direct Linear Transform  (DLT)
Derived from geometrical optics, the Direct Linear Transform (DLT) is based on physics, but
cannot describe non-linear phenomena such as image distortion due to poor camera lenses or
complex refractions that may occur for example when measuring through a window from air
into water.
In matrix representation a DLT is described in the following way.
Uppercase symbols (X, Y, Z) represent the object coordinates (millimeter), and lowercase sym-
bols (x,y) represent corresponding image coordinates (pixel).
The same formula is used for both 2D and 3D models, but in a 2D model all coefficients relat-
ing to the Z-coordinate are set to zero.
The window below is displaying a DLT imaging model fit in the numeric view.
16.39.7 3'rd order XYZ polynomial imaging model fit
The polynomial model is strictly empirical, and there are thus no physical arguments to justify
its use, but in experiments where significant non-linearities are present or expected, the poly-
nomial imaging model may prove superior to both the "Direct Linear Transform (DLT)" on the
previous page and"Pinhole camera model" on the facing page.
The 3'rd order XYZ polynomial that will transform a point in object space (Uppercase X, Y and
Z) onto a point in the image plane (lowercase x and y) is given by the formula below:
PAGE | 572
PAGE | 573
Please note that the x- and y-polynomials are indeed 3rd order in X and Y, but only 2nd order
in Z.
The polynomial coefficient of a given imaging model fit can be the viewed in the numeric view
of the imaging model fit. An example is shown below:
Please note that the DLT-parameters are also calculated and shown, even when a polynomial
model has been selected. This is because inverse mapping (from image to object) requires iter-
ation when using the polynomial model, and results from a corresponding inverse DLT-map-
ping is used for the initial guess.
16.39.8 Pinhole camera  model
This section provides detailed information about the mathematics behind the Pinhole camera
model.
The pinhole model is based on a simplified lens model, where the lens is substituted with a pro-
jection center (a.k.a. the Pinhole) as illustrated below:
In order to transform a point in an arbitrary object space coordinate system onto the image
sensor, the object point (Xp?Yp?Zp) is initially transformed from object space into the coordin-
ate system of the camera and normalized. (The camera coordinate system has the Z-axis nor-
mal to the image plane and its origin placed in the projection center.)
The transformation from object to camera coordinates is given by a rotation matrix (R) and a
translation vector (T). (Xc?Yc?Zc) represents the point in camera system coordinates.
To account for lens distortion (radial and tangential) the normalized point (xn?yn) is then adjus-
ted with the distortion of the lens system. The distortion is given by the parameters K1 and K2
describing the radial distortion and P1 and P2 describing the tangential distortion.
Finally the distorted image point (xd?yd) can be mapped onto the sensor by applying the pin-
hole projection. Here fx & fy is the focal lengths along the X and Y axis. cx & cy is the point on
the image sensor where the optical axis intersect with the image sensor. The pinhole pro-
jection is given by the equation below. (xp?yp) is the point in image pixel coordinates.
PAGE | 574
PAGE | 575
The parameters of a given pinhole imaging model fit can be viewed in the numeric view. An
example is shown below:
Above an imaging model fit is shown, in which five images/views of a calibration target have
been used.
The rotation matrix is described by a rotation vector (a.k.a. Rodrigues rotation vector). The
relationship between a rotation vector and the rotation matrix is given by the Rodrigues rota-
tion formula given below:
Rotation in 3D space can be described by a vector (wx wy wz)*q, where the unit vector
(wx wy wz) defines the axis around which rotation take place and q  determines the angle of
rotation. From a known rotation vector the 3x3 rotation matrix R can be determined accord-
ing to:
16.39.9 Telecentric camera  model
This section provides detailed information about the Telecentric camera model and the math-
ematics describing the Telecentric lens.
Optics
A?Telecentric lens can in principle be made by mounting a pinhole one focal length behind a
conventional lens and before the image plane:
Following light rays from points in the object space the pinhole will block all but rays that are
"almost parallel" to the optical axis. Shaded areas in the figure above illustrate light that will
reach the sensor, while dotted lines illustrate rays that are blocked by the pinhole and thus
cannot reach the sensor. The smaller the pinhole diameter is, the closer to parallel the accep-
ted rays become, but of course it limits the amount of light collected also. Parallel rays mean
also that the size of the lens itself limits the field of view in object space. The main reason for
using a telecentric lens is however that the resulting images have no perspective: The image
of an object will remain the same size no matter the distance to the lens (but still blur when
too far from the focal plane).
Please Note:
The absence of perspective means also that you cannot calibrate a single-camera system using
the Telecentric Model, since a mirror image of the image acquisition setup would (in theory)
give the exact same images, so there is no way to find out which system is used. This ambi-
guity can be partly resolved by combining simultaneous views of several poses as seen from
multiple cameras. This means that the Telecentric Imaging Model is only available in 'Multi Cam-
era Calibration', where multiple cameras are calibrated simultaneously.
PAGE | 576
PAGE | 577
Mathematical description
Parallel rays are mathematically described by an orthographic projection. It is mathematically
identical to the Pinhole Model, but some of the parameters are locked at certain values. As
with the Pinhole Model object coordinates are first transformed from the (arbitrary) object
coordinate system to a camera-aligned coordinate system with origin in the center of the lens,
Z-axis along the optical axis and X- and Y-axis aligned with rows and columns of the image
sensor. Afterwards these (metric) coordinates are transformed into pixel coordinates on the
sensor via a camera matrix, containing intrinsic parameters.
Given a point P=[X?Y?Z]T in object coordinates the orthographic projection to the image point
p=[u?v]T is modeled almost the same way as in the Pinhole model:
where p=[u v?1]T and P=[X?Y?Z?1]T are described using homogeneous coordinates by append-
ing an additional 1 at the end and the matrices K and E contain intrinsic and extrinsic para-
meters of the camera respectively.
The extrinsic parameters E of a telecentric lens are exactly the same as the ones for a con-
ventional perspective projection, including a 3x3 rotation matrix R and a 3x1 translation vec-
tor T. Together they describe a mapping from world to camera coordinates (both metric, but
the latter aligned with the image plane). The fact that ZC does not influence object positions in
the image means also that the calibration procedure cannot make a reliable estimate of T3, so
it is simply set to zero (meaning also that we cannot estimate how far the camera is from the
scene).
The intrinsic parameters Alfa and Beta are horizontal and vertical scale factors handling the
conversion from metric to pixel coordinates, while Gamma describe the skewness. In a con-
ventional (and ideal) camera Alfa & Beta would be identical and Gamma would be zero. In prac-
tice the use of a Scheimpflug mount is enough to justify Alfa and Beta being different and if
the Scheimpflug mount allows tilting in both directions (or if the image plane tilt is not per-
fectly aligned with sensor rows or columns) a nonzero Gamma becomes relevant also. Also
the principal point, defined by u0 &?v0 is the same as for the pinhole model, but for the tele-
centric model they cannot be estimated and are simply set to half the sensor width and
height.
In a conventional (perspective) lens K33=1, describes the perspective, but in the orthographic
camera matrix it is zero. This means that the distance ZC from camera to object point does not
influence its position in the image. That is exactly the main property of a telecentric lens.
Requirements
In the following the term "Pose" is used to describe a specific location and orientation of a cal-
ibration target in object space.
The term "View" refers to a given target "Pose" as seen from a specific camera. This will in gen-
eral correspond to a specific image.
A certain camera is said to "see" a certain target Pose, when at least 4 (non-collinear) cal-
ibration markers can be correctly identified on the corresponding image.
(4 is an absolute minimum, a "good" calibration image should contain a lot more than 4 detect-
able markers evenly distributed across the cameras field of view).
A view or image fulfilling this requirement is said to be "valid" below.
The first target pose is used to identify the coordinate system referred to by the final camera
calibrations. Obviously this reference needs to be valid and aligned with the experimental
setup if end results are to be of use. The remaining target poses can be chosen freely and the
Telecentric model works best with more or less random poses, so "Freehand" target ori-
entation is recommended. If/When using a traverse the target location changes, but ori-
entation remains the same for all poses, limiting the amount of new information added with
each new set of images.
For the Multi-Camera Calibration to succeed using the Telecentric model a number of require-
ments must be fulfilled:
l  Estimating intrinsic parameters of a specific camera require at least 4 valid calibration
images (/views) preferably with the target oriented differently.
l  Estimating extrinsic parameters require each target pose to be seen from at least 2 cam-
eras.
(If not fulfilled, the target pose and corresponding views/images will be excluded from
the calibration).
l  Resolving the orientation ambiguity inherent to a telecentric system require the fol-
lowing:
Whenever two (or more)?cameras can "see" a specific target pose, the same two cameras
must be able to "see" at least one other common pose.
Results
The parameters of a fitted telecentric model can be seen in the numeric view. An example is
shown below:
PAGE | 578
PAGE | 579
The average reprojection error indicate how well the model matches the location of the cal-
ibration markers found. Please note that the algorithm to find/locate calibration markers will
itself introduce noise/inaccuracies, that will increase the apparent "error" shown.
The camera matrix listed above is the intrinsic parameters in the K-matrix, but without the
third column (which will be all-zero anyway). Note that u0 and v0 are both 1024; As explained
above they are half the sensor width and height, so this particular example comes from a cam-
era with a total of 2048x2048 pixels.
Lens distortion parameters are inherited from the Pinhole model, but all zero, since it
requires XC and YC to be normalized by ZC. Since ZC cannot be determined we set it to zero
and thus cannot normalize with it.
The reference frame identifies which of the images define the coordinate system. For each of
the views the Extrinsic parameters are shown as Rotation and Translation vectors. Note that
the third component of the translation vector is always zero, since it cannot be estimated
when using the telecentric model.
Last the reprojection error is shown for each individual view. If one of the views show sig-
nificantly higher error than the others you may consider excluding it from the calibration to
perhaps get a more accurate calibration.
16.39.10 Adjusting parameters for finding the dot matrix target
The dot matrix calibration target includes a large dot (the zero marker) surrounded by four
smaller ones (the axis markers) to identify the origin of the coordinate system, and based on
known dot spacing (X, Y) - Coordinates of the remaining markers can be determined.
If the model fit fails, try cleaning windows and optics, remove bubbles caught in the calibration
target, and anything else you can think of to improve the quality of the images. This may take
a while, but it is worthwhile doing it, since poor image quality will at best produce a poor ima-
ging model fit, which is of little use anyway. If you are attempting a 3D model fit, try fitting a
2D model to each of the images involved to see if all or only some of the images fail. In the lat-
ter case examine the faulty images closely to see if you can identify the problem. Especially
the region around the Zero and Axis markers is important.
Here is a little more about how the software find the dots
If everything else fails, you may need to modify parameters in the target analysis setup:
PAGE | 580
PAGE | 581
When analyzing the images they are first thresholded to produce pure black/white images,
and all black pixels touching each other are grouped in objects. For each object the area (pixel
count) and the position (centroid) is calculated. Ideally these objects should correspond to cal-
ibration markers, but in practice they may include small noise "spots", large "stripes" along the
edges of the target and so on. Such erroneous objects should of course be removed before
performing the actual imaging model fit.
Below is a description of the parameters that are used to reject/accept individual objects
(marker candidates).
Minimum dot area: Objects with an area below the Minimum dot area are discarded prior to
calculating the mean object area. This removes isolated high frequency noise.
Border: Objects touching the image boundary are also discarded, while objects that are
"close", but not touching the image boundary may be discarded depending on the parameter
Border. For example Border=0.10 will discard all objects outside the central 90% of the image
(5% of image width is discarded left and right, and 5% of image height is discarded top and bot-
tom).
Dot area tolerance: The mean area (pixels) of all remaining objects is calculated and then
objects significantly smaller or larger than the mean area are discarded as erroneous. The fol-
lowing formulas is used for marker classification:
AMin = (DAxis / DStd)2 * AMean / (Dot Area Tolerance)
AMax = (DZero / DStd)2 * AMean * (Dot Area Tolerance)
Objects with an area smaller than AMin or larger than AMax are discarded and on the basis of
remaining objects AMean is recalculated for later use. Nominal dot diameters DAxis, DStd &
DZero are read from the target library and included to account for the fact that not all markers
have the same size. Beyond this the Dot Area Tolerance accounts for variations due to per-
spective as well as normal variations in the images. Dot area Tolerance must be larger than
one.
Among the remaining objects the largest one is assumed to be the zero marker and it is veri-
fied that the zero marker candidate is big enough to fulfill the condition:
AZero > (DZero / DStd)2 * AMean / (Dot Area Tolerance)
The algorithm then searches for the four nearest neighbors in direction up, down, left & right
of the zero marker. These objects should correspond to the axis markers and it is verified
they are all small enough to fulfill:
AAxis < (DAxis / DStd)2 * AMean * (Dot Area Tolerance)
Zero/Axis  ratio tolerance: Provided Zero and Axis marker candidates pass the above-
mentioned 'global' size tests, a local test is finally performed to compare them directly:
ABS( 4 * (DAxis / DZero)2 * AZero / SUM(AAxis) - 1 ) < Zero/Axis ratio tolerance
Based on nominal marker diameters, the expected ratio between zero and axis marker areas
can be predicted. This test verifies that the actual ratio does not deviate too much from the
expected value.
Dot position tolerance: Finally the algorithm verifies the position of the assumed Zero
marker relative to the assumed Axis markers: The distance from the Zero marker to each of
the Axis markers is determined and the average distance calculated. The distance to each of
the axis markers is compared to the average distance and Dot position tolerance determine
the acceptable deviation that no axis marker candidate must exceed:
ABS( 1 - dAxis / dMean ) < Dot position tolerance.
Minimum dot count: All remaining objects are assumed to be calibration markers and it is
verified that there are at least Minimum dot count within each calibration image. The default
value 25 corresponds to a grid of 5x5 calibration markers.
16.39.11 Image Processing Parameters
Mathematically camera calibration is the task of modifying the parameters of an imaging
model to make the model match sample observations as best possible. Different imaging mod-
els have different parameters and different procedures to optimize the fit, but they all rely
on sample observations where corresponding object- and image-coordinates are known.
Obviously these coordinates originate from the calibration markers and identifying markers in
an image and determining their coordinates requires image processing.
The first step in identifying markers is to binarize the calibration images using a threshold
value determined from the grayscale histogram of each image.
Note
The histogram of the image gray values must have two distinct peaks, and only two.
PAGE | 582
PAGE | 583
First: Original image.
Second: Histogram with peaks and threshold.
Third: Binarized image.
The thresholding is shown. The leftmost peak is assumed to correspond to the dark calibration
markers, while the rightmost peak is assumed to correspond to the bright background of the
calibration target. The threshold is determined as the grayscale value midway between these
two peaks. (The multilevel targets have white markers on a black background and the marker
and background peaks thus swap, but the threshold is still midway between them).
To remove noise the histogram is smoothed before peak-finding. If more than two distinct
peaks remain in the histogram only the two outermost will be used and further peaks in the
central part of the histogram will be ignored.
This is particularly important in cases where the calibration target does not cover the cameras
entire field of view: If very dark or very bright areas are visible outside the target edges the
histogram may contain additional peaks to the left or right of the ones representing target
markers and target background. This will affect the threshold calculation and possibly cause
markers and background to merge as all-black or all-white.
To overcome such problems masking can be used to remove the areas outside the calibration
target: If these areas are made gray by masking they will still produce a distinct peak in the his-
togram. With careful selection of a grayscale value it can be ensured that this peak is posi-
tioned somewhere between the two peaks representing target markers and background.
This extra peak will thus no longer influence the threshold calculation.
PAGE | 584
PAGE | 585
Disturbing background effects removed by masking.
Note the varying background intensity in the original image. The background in the upper
right-hand part of the image is darker than in the left and central part of the image. This is
why the right-hand "bright" peak has a plateau on its left instead of dropping off smoothly.
Even so the peak remains (barely) on the right-hand side of the threshold value and the image
can be binarized successfully. In the upper right-hand part of the binarized image some
scattered black pixels are present in areas that should have been part of the background and
further image processing is required to get rid of these, so only calibration markers remain.
16.39.12 Imaging model fit equals Camera  calibration
In the computer vision society, finding the correct imaging model parameters is commonly
referred to as camera calibration. In the scientific community users may however have a dif-
ferent understanding of the term "calibration", so in DynamicStudio the process of finding ima-
ging model parameters is referred to as "Imaging model fit": Given a set of corresponding
object and image coordinates, find the parameters which gives the best possible fit for the
imaging model chosen.
16.40 Multi  Camera  Calibration
The Multi Camera Calibration method is used to generate an Imaging Model Fit (IMF) using
standard or custom calibration targets with markers laid out in a quadratic grid. Instead of cal-
ibrating each camera individually, the Multi Camera Calibration method creates an IMF from
all cameras involved in the imaging set-up. The method can also be used to calibrate each cam-
era individually, if desired.
Multi camera calibration is also used for calibration using images acquired from Active target.
Where for normal targets the markers are detected using image processing on individual
images representing a target position, a set of images is needed for each target position
using Active target. These sets of images is automatically generated by the Acquisition sys-
tem, but it is up to the user to save the images to the database for processing. (Active target
can also provide a calibration for each camera generated from the acquisition system without
the need of Multi Camera Calibration).
For a detailed description about camera calibration and different calibration models in gen-
eral, please refer to "Imaging model fit (Legacy Method)" on page?556. so the method sup-
ports any of the following methods:
l  Telecentric camera model
l  Direct linear transformation model
l  Pinhole camera model
l  3rd order calibration model
The Multi Camera Calibration supports several different target types. The standard single
plane targets with doted or checkerboard markers are supported as well as dotted dual layer
or double sided targets.
The Multi Camera Calibration method provides a fully automatic or alternatively a semi-auto-
matic calibration, which automatically detects individual markers but the user still has the pos-
sibility to manually specify the origin of the calibration target. In addition, also a Manual
Calibration is provided that lets the user freely create a calibration grid with markers, to be
manually placed on top of the calibration images in order to relate the dimensions of the
image to a coordinate system in physical space.
The input requirements for the method is either single a calibration image ensemble or mul-
tiple calibration image ensembles from multiple cameras, which are then selected as fixed
inputs. For detailed information regarding acquisition of calibration images please "Acquiring
calibration images" on page?563
One or multiple calibration image ensembles with acquired calibration images (with or without
the Z-coordinates specified) must be selected in the database tree. From the Context menu
select 'Calibrate...' and in the resulting dialog, select the method 'Multi Camera Calibration'
from the 'Calibrations' group.
PAGE | 586
PAGE | 587
The Multi Camera Calibration recipe is divided into four frames:
l  Browse Datasets
l  Image view
l  Target Info
l  Calibration model
PAGE | 588
PAGE | 589
By using the slider inside the 'Browse Dataset' frame, it is possible to inspect the individual
images in the ensemble(s). The image is displayed in the image view with a graphic overlay
showing the currently detected markers. For general information on how to manipulate the
image view please refer to "Using the display from within an analysis method" on page?982.
16.40.1 Target Types & Parameters
Active Target
For calibration using Active target calibration images, target information is saved together
with the images. In this case setting up target parameters is done automatically and in this
case the user is prohibited access to changing Target parameters.
Normal Targets
Before starting the calibration, information about the calibration target must be entered into
the tab 'Target Info'.
If 'Use Standard Target' is unselected a drop down list of known target types will be enabled
and you will have to specify the 'Marker Shape' as 'Crosses', 'Dots' or 'Checkerboard'. Sub-
sequently you will have to specify marker spacing and other 'Target Info' parameters that
depend on the target type chosen.
If 'Use Standard Target' is selected a drop down list of known targets will be enabled from
which you must specify the target used. Apart from Standard targets the list also includes cus-
tom targets previously created and saved (See Target Library & Custom Targets in "Imaging
model fit (Legacy Method)" on page?556). In this case most target parameters such as marker
spacing will be locked to values loaded from the target library.
The only exception is 'Target Processing', that is relevant for checkerboard targets only; With
different types of 'Edge Detector' you can choose between different methods for finding the
checkerboard crossings; The first 6, 'Detector_NxN', attempts to identify individual tiles on
the chessboard target and subsequently locate the corners of each tile. The last one,
'Detector_DOH', compute the 'Determinant of Hessian' from gray-scale intensity gradients in
order to identify and locate saddle points (=Crossings) in the calibration images. Generally the
topmost methods in the list are the fastest, that works best with high-quality images, while the
last one is the slowest, but also the most robust against e.g. blurry and/or poor contrast
images.
16.40.2 Dual level and dual sided targets
If for instance the 270x190mm two level dual sided target is used. Please select the cameras
observing it by the spacebar before entering the "Multi camera calibration" method. The
Front side of the target is the one where the 0-marker is located on the outside plane of the
target. as shown in the image below.
"Multi level, 270x190mm,2nd level -4" as the standard target. the -4 means that the 2nd plane
is 4 mm behind the first plane. To have the coordinates right you need to select that is the
front side or the rear side of the target in lower left corner.
Another input that can be made is in the target processing. since often the a reflection is
formed between the 2 planes, a check for the circularity is introduced. Here, 1 would rep-
resent a perfect circle. This parameter needed to be varied if gridpoints are not properly
detected.
Front camera observing the dual sided target.
PAGE | 590
PAGE | 591
Backside camera observing the dual sided target.
16.40.3 Automatic/Semi-automatic Calibration
If 'Target Identification Type' is set to 'Automatic', all the markers are detected based on the
selected calibration target type. If 'Target Identification Type' is set to 'Semi-automatic', the
markers are detected after pressing 'Start Marker Identification' on the 'Calibration Model'
tab.
Once the marker identification has been initiated by pressing the button 'Start Marker Iden-
tification', the following frame will be displayed.
The reference marker (origin of the calibration) is indicated by red coordinate axes in the
image view, the position of the reference marker can be altered by left clicking any of the
detected markers. Once the desired reference marker is set, the button 'Accept' will store
the position and move on to the next image in the calibration ensemble. Since the reference
marker will define the origin of the calibration, it is vital that the same reference marker is
selected in all images.
If the image is not to be used in the calibration, press the button 'Skip', which will disregard
the current image and move on to the next image in the ensemble. The green or red circle (as
seen in the picture above) indicate whether or not the current image is used in the calibration
- Green indicate that the image is used, red that it is not.
To complete the calibration process the desired imaging model and the orientation of the
coordinate system must be selected from the 'Imaging Model' menu. For calibration of mul-
tiple cameras only the Pinhole camera model is available. For calibration of multiple cameras it
is also necessary that at least two cameras observe the calibration from the same side with the
same coordinate system orientation all the time. For a detailed description about the imaging
models and coordinate system orientation please refer to "Imaging model fit (Legacy
Method)" on page?556.
Finally press the 'Apply' or 'Ok' button, to calculate the calibration.
16.40.4 Active Target Calibration
When images are loaded from the database it will detected from Multi Camera Calibration that
the acquired images are images generated with Active Target.
The user interface will in this situation be very limited.
It is possible to select which calibration model to use:
l  Direct linear transformation model
l  Pinhole camera model
l  3rd order calibration model
For Each target position a number of images has been acquired. First a series of images
where all markers are turned on and off. After these image a number of images where the
encoding of the marker position is acquired.
16.40.5 Manual Calibration
When the Target Identification Type is set to Manual, the user is able to freely place the cal-
ibration grid onto the calibration image, by moving the corner markers with the mouse. This is
convenient e.g. in situations where it’s not possible to place a conventional calibration target,
and so other means of calibration is necessary.
When Manual Calibration is chosen, first go to the Calibration Model tab. Here the desired Ima-
ging Model and the Coordinate System Orientation must be selected. For a detailed descrip-
tion about the imaging models and coordinate system orientation please refer to Imaging
model fit.
PAGE | 592
PAGE | 593
Once the settings in Imaging Model Fit are completed, press the button 'Start Marker Iden-
tification' to overlay a default grid of calibration markers on the calibration image. Now, by
going back to the Target Info tab, the parameters of the grid and the markers can be
changed. Adjust the parameters so that the marker grid matches an object with well-known
dimensions in the image.
Grid spacing:
Defines the physical distance in mm between the markers in the grid, in the x and y directions
individually.
Reference point:
Defines the coordinates (mm) for the grid reference point in physical space.
Grid size:
Defines the number of rows and columns of calibration markers used in the grid.
The markers on the grid consist of two concentric circles, and inner one and an outer one.
When zooming in on one of the markers, this is seen more clearly, as shown in the figure
below. By left clicking in the inner circle, the user can use the mouse to move the four corner
markers of the grid to the desired position. If the calibration image is an image of a physical
grid of any type, the user can move the four corners so that the marker grid matches the
physical grid in the image.
By clicking in the outer circle, the user can define the marker to be the reference marker (ori-
gin of the calibration) of the grid. This is indicated by red coordinate axes in the image view, as
shown in the figure below. Since the reference marker will define the origin of the calibration,
it is vital that the same reference marker is selected in all images.
PAGE | 594
PAGE | 595
During this process, the user can alter the parameters of the grid in Target Info at any time,
and the marker grid seen on screen will conveniently be updated accordingly.
Once the marker grid is properly placed on the calibration image and the desired reference
marker is set, return to the Calibration Model tab. Clicking the button 'Accept' will store the
marker positions and move on to the next image in the calibration ensemble (if any). If the
image is not to be used in the calibration, press the button 'Skip', which will disregard the cur-
rent image and move on to the next image in the ensemble.
The green or red circle (as seen in the upper-left corner of the image view below) indicates
whether or not the current image is used in the calibration - Green indicate that the image is
used, red that it is not.
When all the marker grids for the relevant calibration images have been accepted, finally
press the 'Apply' or 'Ok' button to complete the calibration process.
The calibration can be used to dewarp images, to e.g. straighten out the coordinate system in
a skewed image view. For more on this topic, see Image Dewarping.
16.41 Imaging  Model  Fit Import
The Imaging model fit import is used to create a new Imaging Model Fit (IMF) on the basis of a
exported IMF1 text file or by manually entering the IMF parameters.
The screen shot below is picturing the recipe dialog for the method.
In the frame "Range of validity" the user must enter the interval in which the IMF is valid.
The user must also specify which calibration target plate to simulate. The entered values for
the range of validity and the simulated target are only informative and is only used when dis-
playing the IMF as a graphic overlay on an image.
To actually import the imaging model fit the proper imaging model should first be selected
from the drop down list. Afterwards pressing the "Adjust imaging model fit ..." button will
bring up a dialog in which the IMF parameters can be entered or automatic imported from a
file. The look of the dialog is dependant on the choice of imaging model.
Below is a screen shot of the dialog used to enter the Pinhole model parameters.
1An exported IMF file can be created by selecting the calibration record in the database tree and
choosing "Export -> Export as numeric..." from the File menu.
PAGE | 596
PAGE | 597
The desired values can be entered into the relevant text boxes or alternatively the "Import
from file ..." menu option available in the file menu can be used to specify the location an
exported IMF file.
16.42 IPI Processing
IPI processing determines the size of spherical, transparent particles through the fringes pat-
terns observed in a defocused image. To determine diameter only, the image can be single
frame (For velocity information, both images need to be double-framed.). To accurately
determine the position of a particle, two overlapping images are required; a focused image
and a defocused image. Performing IPI data analysis, requires Calibration images to ensure
overlap between the focused and defocused images. Calibration have to be performed
before IPI analysis.
16.42.1 Content
User Interface
Calibration
Recipe <IPI processing>
Advanced Settings
Processing and Presentation
Post Processing
Example
Trouble shooting
16.42.2 User Interface
Select the two target images as show below,?do a right click and select Analyze or from the
tool bar press the Analyze icon
Important: Make sure Camera 1, Camera?2 and calibration images are highlighted before
clicking the right mouse button and selecting
"Analysis…".To?select?images?press?simultaneously shift?keyboard?key?and?right?mouse?button.
Make sure the analysis  is  applied from the first camera folder (Camera 1)  as  above.
PAGE | 598
PAGE | 599
Select the category “Particle Characterization” and then in the adjacent list select “IPI Particle
Sizing”. The text below the selection highlights the necessary inputs to correctly process the
data.
16.42.3 Calibration
Acquire a set of images with the defocused and focused camera and save the images as Cal-
ibration image. From the first image. do a right mouse click and select Calibrate.?
From the calibration window, select the calibration method. For IPI processing, it is recom-
mended to use Imaging Model Fit method. :
Repeat the same for the second image.
For velocity measurement, the scale factor has to be determined. One way of doing that it to
use the calibration image on the focused image and by right clicking on the mouse, select
measure Scale factor.
For further information about calibration methods, refer to the IPI?reference manual and
"Imaging model fit (Legacy Method)" on page?556.
16.42.4   Recipe <IPI Processing>
Initially, the recipe for IPI Particle Sizing appears reduced showing only the most important
parameters as below. To see all available parameters, click on "Show advanced settings" at the
bottom of the recipe and refer to Advanced Settings description.
16.42.5 General
Select image order - Select the image that represents the focused camera and the defo-
cused camera.
Image mask limits  - Enter the limits for the size of the mask used to determine the defo-
cused circle size. Changing the size of the maximum value effects the maximum particle size
that can be measured. Make certain the values coincide closely with those observed in actual
images. The step size controls the integration frequency. The size of the FFT should always be
larger than the maximum circle size.
PAGE | 600
PAGE | 601
16.42.6 Optical Setup
Enter the optical parameters that best reflect the actual setup. The scattering angle is nor-
mally left at 90 degrees to avoid the effects of image warping.?The?aperture?diameter?is?focal
length divided by the F-number. Since the defocused camera aperture is usually left at fully
opened, the aperture diameter,? for a 60 mm lens with an aperture of 2.8, is 60/2.8=21,5
mm.
The calculated  diameter?range?is based on the input optics parameter and the maximum
mask size (see the section General).
Hint: To establish the maximum particle size in a measurement, enter the maximum defo-
cused circle size (see previous page) that best represents the particle size range. By switching
back and forth between the two pages you can determine the size of the circle required,
hence the amount of defocusing required, to measure in the range specified.
16.42.7 Velocity Setup
The PTV analysis requires a cross-correlation to be carried out on the focused image pair to
determine the general flow direction. Careful selection of the interrogation area and offset is
required, since seeding in sprays or other particle ladened flows is often less than what it is in
PIV flows. Too small an interrogation area and the PTV analysis will yield unreliable results.
PAGE | 602
PAGE | 603
Interrogation size is the dimension of interrogation area in pixels: 16, 32, 64, 128, 256.
Overlap percentage is the amount of overlap (%) to use incrementing to the next area: 0, 25,
50, 75%.
Noise removal?in percentage?is the amount of background noise to remove of the camera
resolution.?It operates much like a threshold. Note that it affects the detection of particles (siz-
ing) even?when no velocity analysis is performed.
Maximum particle number is the maximum number of particles to accept per image. It can
be selected between 1-10000.
Tip: Evaluate the focused image pair first using the cross-correlation analysis that is built-in
into the DynamicStudio software. The size of the interrogation area will depend largely on the
seeding quality of the focused images. For flows with few particles, use a large area. A rule-of-
thumb is a minimum of 10-11 particles per interrogation area.
16.42.8 Advanced Settings
To enable the advanced settings, click on the check box at the bottom of the recipe.
16.42.9 Region  Of Interest (ROI)/  Validation
In some measurements it is only required to process a portion of the measured area. You can
limit the areas of interest by entering the dimensions or selecting one of the pre-defined
areas. There are six predefined areas. Uncheck the checkbox labeled "Use entire region” if
you wish to set your own limits.
PAGE | 604
PAGE | 605
The peak level validation rejects particles based on the percentage peak height of the max-
imum peak determined. The overlap will reject particles with too little useable area. Setting a
value of 70% would mean that any particle with more than 70% of its area overlapped would
not be accepted. ・ The frequency ratio in the x- and y-dir is another validation tool. Fringes in
the x-dir will exhibit small frequency peaks in the y-dir, and therefore a high fringe ratio.
Images without fringe information usually exhibit poor fringe ratios.
16.42.10 Window  Setup
The processing of the defocused image yields the fringe frequency, which in turn yields the
particle diameter. The frequency information is determined by applying a 2D FFT over the
selected area and identifying the dominant frequency peaks. It is often useful to apply a win-
dow or filter over the input data prior to processing to smoothen the peaks. The built-in win-
dow is a familiar type known in signal processing as a Hanning Window. While this window has a
clear definition and fixed parameters, a strength factor has been built-in that effects the
“smoothness” of the output data. In addition, the window can be applied horizontally, ver-
tically or in both directions. Since fringes are oriented according to the optical configuration, it
is advantageous to apply the window in the direction perpendicular to the orientation of the
fringes.
The built-in window is a familiar type known in signal processing as a Hanning Window. While
this window has a clear definition and fixed parameters, a strength factor has been built-in
that effects the “smoothness” of the output data. In addition, the window can be applied hori-
zontally, vertically or in both directions. Since fringes are oriented according to the optical con-
figuration, it is advantageous to apply the window in the direction perpendicular to the
orientation of the fringes.
Note: A window strength of 0 % implies the default Hanning window.
16.42.11 Filter
In high concentration particle flows the number of particles can be so high as to reduce the
overall validation simply because the overlap is too large. A workaround to this is to artificially
reduce the detection such that particle neighbors within a user-specified bound are not accep-
ted.  By default, the filter is disabled. To apply?the?filter,?check the
checkbox?and?select?the?minimum?distance?between?particles.
PAGE | 606
PAGE | 607
Tip: A useful rule-of-thumb is to take the largest circle size and divide by five; i.e. for a defo-
cused image size of 150 pixels, use a particle filter with a minimum separation distance equal
to 30-40 pixels
Alternatively, the spacing between particles may be such that areas of high concentration suf-
fer through validation while neighboring areas of lower particle density are left unaffected.
Here you may wish to adjust the distance parameter to improve the overall validation.
Sometimes the camera is advertently moved after calibration (resetting the aperture, for
example). In this case the calibration is affected and therefore particles in cameras A and B will
not line up as expected. In extreme cases it is recommended to redo the calibration. In cases
where the offset induced is minor and can be visibly measured on the images, the user can
apply an offset on the defocused image in both X and Y directions.
16.42.12 Laser setup
Laser settings are also used to set validation criteria. The position of the origin of incident light
plays a role in how the fringes are rotated as a function of particle position in the image. The
position of the laser source is measured from the front lens of the lightsheet optics to the
front lens on the camera.
16.42.13 Processing and Presentation
After pressing Apply or OK the IPI processing will execute, adding an IPI record beneath the
selected image datasets. The resulting data may then be displayed in tabular form by clicking
on the spreadsheet icon in the toolbar, or in graphical form by double clicking the mouse dir-
ectly on the IPI record.
Tabular presentation  of  IPI  data
The tabular format of IPI data will depend on whether single or double frame images were
acquired and processed. Single-frame images yield only size and particle position information.
Double-frame images will yield velocities also:
PAGE | 608
PAGE | 609
l  X, Y , Z: Position of the particle in the image (pixel).
l  Cs: Size of the defocused image (pixel).
l  D: Diameter of the particle (micron).
l  U, V, W: Velocity of the particle (m/s).
l  Fx: fringe frequency in the x-dir (1/pix).
l  Fy: fringe frequency in the y-dir (1/pix).
Note: A circle size of zero indicates a detected particle that did not have a corresponding
image on the defocused image. This usually indicates that the particle was not transparent. A
diameter of zero for a non-zero circle size marks an invalid particle.
Double image datasets yield the above information for each image plus velocity in X and Y.
Velocity is determined through particle displacements from image A to image B. The velocity
determination is independent of whether the particle was validated or not. Therefore, vec-
tors of non-transparent particles are also visible. Refer to the DynamicStudio manual for
options related to formatting or exporting tabular data.
Graphical presentation of IPI data
The graphical display by default displays the data as circles over each particle detected. The
positioning of the circles coincides with the position of particles on the defocused image. The
diameter of the circle coincides with the diameter of the defocused particle images. Green
circles indicate validated particles, the red ones invalid particles. The color scheme is user
selectable.
The particle display can be dragged over the raw defocused images to test the calibration. The
circles should overlap the raw particle images, though small deviations are common and are
often due to irregularities in the particle images themselves. Raw defocused images are often
ellipsoidal and therefore introduce small deviations in the positioning of the resulting
particles. Normally this is of little concern, though large deviations may be the result of inad-
vertently moving the camera lens between calibration and measurement. If the deviations
are reasonable an offset can be applied to “move” the particles back into place. Large devi-
ations will require recalibrating the cameras.
Display  options  for IPI data
Clicking the right mouse button over the IPI data display will bring up the context menu. Here
you are presented with several possibilities. Selecting the menu item “Display Options…” will
display a tab dialog for controlling the appearance of the data.
PAGE | 610
PAGE | 611
The display options dialog allows you to control the way the IPI data is displayed. As men-
tioned above, the default presentation is to present circles over each particle detected.
The Colors tab of the Display Options page enables the following items to be displayed: ・
l  Valid particles: by default these items appear as green circles that circumscribe the defo-
cused particle image. Color is user selectable by clicking on the color felt to the right of the
item. ・
l  Invalid particles: by default displayed as red circles as in above. ・
l  Vectors: in cases where double images produce valid vectors, vectors or particle dis-
placements are displayed as arrows of a user selected color. ・
l  Particle size: circles based on particle size, normalized by the software. The user can
increase or decrease the default scaling by clicking on the size scaling tab.
In addition, the user can select to have vectors from invalid particles displayed or not. The
background color can be changed by clicking on the color selector to the right. The Size Scaling
tab of the Display Options allows the user to control the size of the circles used in rep-
resenting particle size.
Scaling options:
l  Auto-scaling: scaling is normalized by the software, the user can increment or decrement
the relative size of the diameter circles. ・
l  Fixed scaling: scaling is normalized by the user.
In the case of double images, the Vector Scaling page is displayed. Here the user can adjust
the size of the vectors in much the same way vectors plots found in other parts of Dynam-
icStudio.
PAGE | 612
PAGE | 613
The Validation page of the Display Options dialog gives the user control over two validation cri-
teria:
l  Peak validation: adjusting this parameter the user can see whether the validation level
selected during processing was sufficient. Increasing this value will reject particles with
insufficient peak height.
l  Vector length: by adjusting this parameter vectors with incorrect lengths due to poor cor-
relations can be rejected.
16.42.14 Post processing
Once the IPI datasets are processed, the user can apply the following post-processing pro-
cedures.
To process a series of datatsets, follow the steps below: ・ Select the IPI datasets you wish to
include in the histogram. You can do this by selecting similar datasets or pressing the CTRL key
over each dataset. Refer to the DynamicStudio manual regarding dataset selection. ・ Click the
right mouse over the first dataset and select “Analysis…”. Select from the particle char-
acterization category, Histogram or Spatial Histogram.
For further information about Diameter histogram and Spatial Histogram see "Diameter Stat-
istics" on page?477 and "IPI Spatial Histogram" on page?622.
16.42.15 Example
Here is an example of a spray measurement: Raw image on the top and processed data
below.
PAGE | 614
PAGE | 615
16.42.16 Trouble shooting guide
This section will present several cases, which typify problems associated with IPI meas-
urement. Solutions are then offered after each case. For problems related to setup of the
hardware please refer to the DynamicStudio User’s manuals.
Message: “Too many  particles”
The DynamicStudio IPI detects particles on the focused image based on pixel intensity and dis-
tribution. The removal of background noise in the setup may adversely affect this detection
by accepting too many particles, or too few. Saturated images require care in setting the
amount of noise to remove. The noise level can be adjusted in the menu as shown below:
Since the noise level is uniform for all images under a particular setup, adjust the first image
to determine the proper filtering and then reprocess all the other images afterward. Redu-
cing the noise filter means higher detection rate, increased chance of accepting noise as a
particle. Increasing the noise filter level reduces detection, increasing the likelihood of missed
particles. There is a tradeoff.
If there are many particles, increase the maximum particle level (Velocity setup page).
Poor validation or incorrect particle size
One of the hallmarks of the DynamicStudio IPI is that you can see quite clearly if a particle is
correctly measured or not. Depending on the system configuration, the resulting images may
lack the proper contrast, either through lack of sufficient laser power or weak signal from
particles with low refractive index ratios when compared with the medium. There are several
ways to enhance the quality of the image or processing method to produce better results.
1) Use the window
The IPI processing module provided by the DynamicStudio IPI has a built-in Hanning window.
This window will refine the input data prior to FFT analysis to yield clearer and more distinct
peaks.
PAGE | 616
PAGE | 617
2) Pre-process the image using the IPL module
If you have purchased the DynamicStudio IPI together with the IPL option, you can filter and
enhance the input image in a variety of ways. Refer to the IPL Users Manual for further guid-
ance on using the IPL module.
l  For weak fringes or fringes that show intensity variation, use a high pass filter to remove
the low frequency information. The laplacian 5x5 is a useful filter for enhancing fringes.
l  Use a low pass filter to remove extraneous background noise.
Effect of filtering the raw defocused image. A Laplacian (5x5) filter was used:
Too much rejection due to too many  particles
In the case of high concentration flows, the size of the defocused circles and the particle count
effect the overlap validation and thus many particles are rejected outright. There are two solu-
tions:
PAGE | 618
PAGE | 619
1) Reduce the size of the measurement volume
A change in lens and/or addition of an extension-ring, or the repositioning of the camera will
increase the magnification and thereby increase the spacing of particles on the defocused
image. The magnification should just be enough to validate the particles.
2) Use a particle filter
A particle filter will remove otherwise detected particles that are spaced too closely to one
another, thereby artifically decreasing the detection rate and improving the overall val-
idation.
Particle filtering where the minimum distance is set to 20 pixels:
Particle filtering where the minimum distance is set to 40 pixels:
PAGE | 620
PAGE | 621
Poor matching between focused and defocused images
There can arise a situation where the results do not overlap on the defocused image, as
shown in the figure below. This is often the result of the camera being moved after cal-
ibration. If the offset between the particles and circles is uniform throughout the image then
the user can apply an offset to counteract the effect and produce better overlap.
Poor overlap between results and raw defocused image:
Calibration offset fix in IPI Recipe dialog:
Application of offset increases validation and improves centering of circles:
16.43 IPI Spatial  Histogram
The histogram display refers directly to the underlying histogram dataset, which is a subset of
the Shadow dataset.
PAGE | 622
PAGE | 623
The spatial histogram is defined as a 2D scalar map of a user-selected quantity. The user can
select the binning in both X and Y directions. The following scalar quantities can be binned:
l  Particle counts
l  Diameter mean
l  Area mean
l  Volume mean
l  Sauter mean
When selecting Shadow Histogram, the following window appears.
Calculation
Select the histogram type from the drop down list. Histogram type can be selected bewtween
Particle count, Diameter mean, particle area mean, particle volume mean, Sauter mean (D32),
U mean and V mean where U is the vertical velocity and V the horizontal velocity.
Number of cells
The image is divided in spatial cells. The mean value will be calculated from the particles detec-
ted in each cell.
16.43.1 Process
In double frame, mode, the data can be processed from the first frame (check Image A), from
the second frame (check B) or from both.
The 2D scalar plot that results is of a type that is built-in into DynamicStudio. This type of plot is
usually displayed as a contour plot. Data can also be viewed in tabulated form.
16.44 3D  Least Squares  Matching
16.44.1 Introduction
When performing reconstruction based 3D velocity measurements, the analysis procedure
consists of two parts:
1.   Volumetric Reconstruction see "Voxel Reconstruction" (on page?926)
2.   Velocity analysis (Least Squares Matching)
The Least Squares Matching (short LSM) can perform both methods. But sometimes it might
be handy to store the voxel spaces in order to save computational time for LSM settings test-
ing, or for large voxel volumes (for more Information see "Voxel Reconstruction" (on
page?926)). To analyze the velocities, the Least Squares Matching performs translation,
deformation and rotation of the interrogation volumes (later referred as "cuboids"). More
details on the computational method can be found in the references. See also "2D Least
squares matching (LSM)" (on page?421).
The recipe is located in the Volumetric category of the Analysis Method menu :
PAGE | 624
PAGE | 625
If the method is not applied onto a reconstructed voxel space,it needs at least 3 ensembles of
images (i.e. one for each camera that is used for the acquisition) and the corresponding num-
ber of calibration. All the needed datasets needs to be selected by a checkmark (by pressing
the spacebar when a dataset is selected), as seen in the image above. Moreover, the LSM
recipe can only be applied to double frame images. If the dataset consists in ensemble of
single frame images (i.e. Time-Resolved measurement), please use the dedicated recipe to
obtain double frame images (see "Make Double Frame" (on page?662)).
16.44.2 The Least Squares Matching Recipe
The Least Squares Matching recipe, presented in the following picture, is divided by 4 tabs
into 4 sections that correspond to the 4 steps of the analysis procedure. Please note, when
the LSM is applied on an already reconstructed voxel space, there are just the 3 tabs neces-
sary for the LSM method:
o  Voxel space reconstruction
o  Volume Pre-Processing
o  Vector Grid
o  LSM  algorithm
Each of these point will be addressed in the following of this document.
Voxel space reconstruction
The voxel Voxel Space reconstruction tab is the same as in the voxel reconstruction method
for the different setting and more information about the reconstruction itself please see
"Voxel Reconstruction" (on page?926)
Volume Pre-processing
The tab for the volume pre-processing can be seen in the following image:
PAGE | 626
PAGE | 627
If "Use Pre-processing" is selected, one can choose between different thresholds and a
smoothing of the voxel space, before the LSM is applied.
Lower threshold: The lower threshold defines a gray value from which all values smaller then
the threshold value are set to 0.
Upper threshold: The upper threshold, if a non 0 value is selected, clamps all gray values in
the voxel space exceeding the threshold to the selected gray value.
Gaussian Blur iterations: The Gaussian Blur defines the amount of Gaussian smoothing iter-
ations are applied on the volume. For the Gaussian filter a domain of 3x3x3 voxel is used for
each iteration specified.
Vector grid
The tab for the Vector grids looks like the one given in the image below:
The LSM setup are used to analyze the voxel space and compute the particle displacements in
between two consecutive voxel space in time.
o  IV  size
The Interrogation volume size defines the size used for the analysis of a single vector, sim-
ilar to an interrogation window for 2D PIV but here it is extended into the 3D space. The
user defines the size of the desired cuboid that will be used in the analysis (in odd number
of voxels).
The size is depended of the seeding density, it is therefore possible to estimate a min-
imum cuboid size
to be defined with the knowledge of :
- Cppp: the seeding density, i.e. the number of particles per image divided by the number
of pixels,
- Np : the number of particles desired in an Interrogation volume,
- R : the voxel resolution,
-       : the measurement volume physical depth.
Assuming a cubic interrogation volume, the IV size can be expressed as :
PAGE | 628
PAGE | 629
The 3D LSM needs at least 8-9 particles inside the cuboid of an IV in order to perform well.
Thus, the following figure indicates what cuboid size to chose depending on the
seeding density Cppp and the number of voxels present in the depth of the volume R*      .
o  IV  step
The shift defines the overlapping of two consecutive cuboids. Enter half the size of a
cuboid for an overlapping of 50 %
o  Start position
Start position (in voxel) of the analysis in the voxel space. Note that first voxel will be on
the IV size + 1.
o  End position
End position (in voxel) of the analysis in the voxel space.Note that last voxel can at max-
imum be the maximum number of voxel - the IV size - 1.
LSM  algorithm
The LSM?algorithm tab includes different parameters the tab is shown below:
o  Iterations
The iterative procedure is stopped when all parameters have converged or when the
number of Maximum iterations has been reached. The maximum amount of iterations
can be set here.
o  Use pyramid Scheme
The pyramid scheme can be used to accelerate the computation. The analysis is first per-
formed on a coarse grid, which results are used as an initialization for the finer one..
o  Apply  Significance tests
A validation scheme can be computed on the results in order to detect outliers. This is the
same scheme as a universal outlier detection.
o  Affine parameters  to determine
By enabling or disabling these checkboxes, the user can choose to compute all the LSM
parameters or not. The more parameters to be determined the more intensive is the
computation, but the more precise are the results.
16.44.3 Results of the analysis
Results of the analysis are plotted in the 3D vector display allowing the plott iso-surfaced and
Iso-contours. "3D Display" on page?1024
PAGE | 630
PAGE | 631
16.44.4 References
[1] J. Kitzhofer, P. Westfeld, O. Pust, H. G. Maas and C. Brucker. Estimation of 3D deformation
and rotation rate tensor from volumetric particle data via 3D Least squares matching. In: Pro-
ceedings of the 15th Int Symp on Applications of Laser Techniques to Fluid Mechanics. Lisbon,
Portugal, 05-08 July, 2010.
[2] T. Nonn. Application of high performance computing on volumetric velocimetry pro-
cessing. In: Proceedings of the 15th Int Symp on Applications of Laser Techniques to Fluid
Mechanics. Lisbon, Portugal, 05-08 July, 2010.
[3] P. Westfeld, H.-G. Maas, O. Pust, J. Kitzhofer and C. Brucker. 3-D least squares matching
for volumetric velocimetry data processing. In: Proceedings of the 15th Int Symp on Applic-
ations of Laser Techniques to Fluid Mechanics. Lisbon, Portugal, 05-08 July, 2010.
[4] D. Hess, M. Sastuba, J. Kitzhofer and C. Brucker. 4D analysis of flow around a Tumbling
Ring-Type Particle using Scanning PIV and 3D Leas Squares Matching . In: Proceedings of the
10th Int Symp on PIV. Delft, Netherlands, 02-04 July, 2013.
[5] F. Hegner, D. Hess, C. Brucker. Volumetric 3D PIV?in heart valve flow. In: Proceedings of
the 11th Int Symp on Applications of PIV. Santa Barbara, California, USA, 14-16 September,
2015.
16.45 LIEF  Processing
16.45.1 1.  LIEF  spray analysis
Dantec Dynamics’ LIEF processing for fuel spray analysis strives to determine two images of a
fuel spray, one showing only the liquid phase of the spray and the other one showing only the
vapor phase.
1.1  LIEF concept ?  Laser-Induced Exciplex Fluorescence
The theory of Laser-Induced Fluorescence (LIF) is the basics for Laser-Induced Exciplex Fluor-
escence (LIEF) and describes the interaction between light (photons) and matter (atom-
s/molecules). Light of specific wavelengths can be absorbed by matter, leaving it in an
electronically excited state, and after a short period of time the matter releases the excess
energy again by emission of light, which is called fluorescence. A detailed description of the
theory is beyond the scope of this text and will not be covered.
Laser-Induced Fluorescence is commonly used for diagnostics of combustion and fluid dyna-
mics phenomena. Usually the fluid under investigation is either a liquid (e.g. liquid mixing pro-
cesses) or a gas (e.g. air flow or combustion processes). However, in the case of fuel spray
diagnostics the fluid appears in both liquid phase and vapor phase, and Laser-Induced Exciplex
Fluorescence (LIEF) is a technique that can be used to image the liquid phase and the vapor
phase separately, but simultaneously in a single acquisition.
The technique relies on achieving fluorescence in different spectral regions depending on
whether the fluid is in liquid or in vapor phase, and have two cameras acquire images of the
spray. Two different filters are placed in front of the cameras, one transmitting the fluor-
escence from the liquid and the other one transmitting the light from the vapor.
In practice this is achieved by mixing a small fraction (well-known proportions) of two com-
pounds into the liquid fuel before injection: one fluorescent dye, or monomer (M), and one
ground-state partner (G). A sheet of laser light of a suitable UV wavelength is then used to illu-
minate the spray to excite the tracers in questions.
Vapor phase:
In vapor phase the monomer can absorb the UV light from the laser, which leaves the
monomer in an excited state M*. Shortly afterwards the excited monomer M* will return to a
lower energy state again by emission of fluorescence which is red shifted with respect to that
of the laser light.
M + hνLaser ?> M*
M* ?> M + hνM
Liquid phase:
Also in liquid phase is the monomer M excited by UV light from the laser. Just like in the liquid
phase the excited monomer can relax to a lower energy state by the emission of a photon.
But in addition M* can also react with the partner G to form MG* which is a molecule that is
stably bound in the excited state but not in the ground state. The newly formed species MG*
PAGE | 632
PAGE | 633
is called excited state complex, or exciplex. The fluorescence from this exciplex is red shifted
with respect to that of the excited monomer M* itself.
M + hνLaser ?> M*
M* + G ?> MG*
MG* ?> M + G + hνMG
The subsequent fluorescence is recorded by two cameras looking at the same field of view by
means of a dual camera mount or beam splitter arrangement. The camera filters have a trans-
mission corresponding to the fluorescence spectrum of the exciplex and the monomer,
respectively.
The formation of MG* by M* and G is in principle possible also in vapor phase, but since the dis-
tance between the molecules in vapor phase is much greater than in liquid phase this process
is very unlikely to occur. Therefore, the images acquired by the camera with the filter trans-
mission corresponding to exciplex fluorescence are concidered to show only the liquid phase
of the spray.
The images acquired by the camera with the filter transmission corresponding to monomer
fluorescence on the other hand, will not only show the vapor phase of the flow, but will also
have some influence from the liquid phase. This is referred to as cross-talk and cannot be elim-
inated completely by the experimental setup. The effect of the cross-talk is instead minimized
by image processing in the software, described in the sections below.
Note:
In order for the technique to work properly the generated fluorescence should come only
from the added tracers and not from the fuel itself. Real fuels for internal combustion
engines, such as gasoline or diesel fluoresce strongly if illuminated by UV light, and can there-
fore not be used. Real fuels have to be replaced by non-fluorescent reference fuels such as
Iso-octane, N-hexane or N-dodecane. An example of a suitable reference fuel and exciplex
tracer system along with the corresponding excitation and detection wavelengths are given in
the table below, for Diesel and Gasoline applications respectively.
Diesel              Gasoline
Fuel                                               N-Dodecane   N-hexane
Ground state partner                 Naphthalene  Diethylmethylamin
(DEMA)
Monomer                                     TMPD               Fluorobenzene
Laser excitation wavelength     355 nm           266 nm
Camera filter, liquid phase         550 nm           355 nm
Camera filter, vapor phase        380 nm           295 nm
Table. The table gives an example of possible reference fuel and tracer species along with the cor-
responding excitation and detection wavelengths for Diesel and Gasoline spray applications,
respectively.
16.45.2 1.2 Spatial calibration  of two cameras
To be able to do the image processing to remove the cross-talk between the vapor phase and
the liquid phase in the acquired images, it is important to ensure a good pixel-to-pixel overlap
between the two cameras. First of all it should be noted that the two views will likely be
mirrored with respect to oneanother. This is taken care of by simply fliping one of the images
in Image Format BEFORE the acquisition, so that left and right (and up and down) in the phys-
ical experiment are the same for both camera views.
Once you have ensured the same orientation of the two camera views a spatial calibration can
be made in the following way:
1.2.1  Multi Camera Calibration:
By using a calibration target plate, a multi camera calibration can be created. For further
information, please refer to the Multi Camera Calibration section.
If no calibration target is available, a correspondence vector map can be used to bring the two
cameras into alignment.
1.2.2  Alignment Vector Map
The alignment vector map can be generated using the Cross-Correlation or similar correlation
analysis method. For best results, the source images must contain a texture that has a suf-
ficiently large set of identifiable features that can be matched between the cameras. A plate
covered with a somewhat random texture can be used. In order to generate the cross-cor-
relation, select one image from each camera using the space bar, and right click one of the
images, choose: Analyze->PIV Signal->Cross-Correlation. In the recipe set the correlation
options. Larger interrogation areas create more stable vector maps, whereas smaller inter-
rogation areas will be able to describe local warping better.
16.45.3 1.3 Launching the LIEF  Processing analysis method
The LIEF Processing analysis method can be launched in two ways:
・ It can be launched using two calibrations and two source images
・ It can be launched using a displacement vector map and two source images.
PAGE | 634
PAGE | 635
16.45.4 1.4 Determining and correcting for the cross-talk
As mentioned above, the images from the camera with the filter for exciplex transmission
show only the liquid phase of the spray, whereas the images from the camera with the filter
for monomer transmission, will not only show the vapor phase, but will also have some influ-
ence from the liquid phase.
The purpose of the processing described below is to determine and compensate the vapor
images for this cross-talk.
Since the liquid phase images are considered unaffected by cross-talk, the task will be to
determine how much of the liquid phase image is recorded on the "vapor phase" camera.
Once this is known, this portion (K) of the liquid phase image can be subtracted from the vapor
phase image.
The cross-talk compensation factor K can be set in three different ways.
To determine the cross-talk one needs to acquire images of fluorescence with both cameras,
either in a situation where only liquid phase is present (e.g. by placing a glas cell filled with
liquid fuel in the field of view) or in a well known spray in which it is known that at a certain loc-
ation only liquid phase is present. In the recipe this area of pure liquid phase shall be selected,
using the red box. This is done in the left hand (Vapor Image) or center image (Liquid Image)
in the recipe dialog.
1.The K value is the calculated as the scaling factor between the average image intensity
within the area in each respective image. A preliminary result of the Vapor Image com-
pensated for the cross-talk from the Liquid Image is shown immediately on the right hand
side of the recipe dialog.
2. The desired K value can also be entered, using a numeric value in the text box, this method
is recommended for fine-tuning of the K value.
3. For a rougher but more intuitive way of entering the K value, the slider can be used to
select the K value.
Adjust the K value, until the cross-talk has been completely removed from the target image,
using one of the three methods highlighted. Once this is done press OK and the software will
run through the entire selected data set and compensate all vapor images for the cross-talk
from the corresponding liquid image, based on the cross-talk K value determined in the
recipe.
If the analysis method was launched by executing the analysis method on the wrong source
image, the images can be swapped from within the analysis recipe, using the "Swap Images"
button, located below the source images.
16.46 LIF  Calibration
16.46.1 Custom  properties of the calibration  images
To calibrate a LIF setup, images must first be acquired at known experimental conditions and
stored as calibration images ("Calibration Images" on page?102). The user must specify the
experimental conditions as custom properties of the images in the database. Finally all this
information is processed by the LIF?software to calibrate the camera pixel by pixel, so LIF sig-
nal images can later be transformed into scalar images describing the corresponding scalar
quantity (f.ex. concentration or temperature).
PAGE | 636
PAGE | 637
For a successful LIF?calibration it is mandatory to supply at least two images acquired with dif-
ferent, but known scalar quantities such as concentration, temperature or whatever the
LIF?experiment is intended to provide information about. In the following we will assume that
the aim is a concentration measurement, but it might as well be temperature, pH-value or
some other scalar quantity.
It is recommended to include more than two known concentration values in the calibration
and also to acquire multiple images for each of them in order to reduce the effect of random
fluctuations.
Optionally the calibration may also include variations in laser pulse energy in order to later
compensate for shot-to-shot variations when processing the actual LIF measurements. If you
have a Pulse Energy Monitor connected to an Analog input you need not specify a nominal
energy.
When storing the calibration images you may merge them all into a single ensemble or store
each series of images as separate ensembles. If you merge them into a single ensemble nom-
inal concentration values and so on will have to be entered for each individual image ('Show
Contents List' and browse through the images one by one, entering custom properties as
described below).
In the following example calibration images have been stored in separate ensembles, which
have been renamed to clearly indicate nominal concentration and laser energy of the images
contained.
Please note:
DynamicStudio does not use the ensemble names for anything, you may leave the default
ensemble names or assign a more descriptive name of your own choice. The latter improves
readability of the database and makes navigation easier, but is not mandatory.
You are strongly advised to enter nominal concentration and optionally laser energies as soon
as possible after acquiring them. You can quickly acquire a lot of images and easily lose track
of which images correspond to which experimental conditions. Ideally you should do this
already while saving ("Calibration Images" on page?102), but you may of course do it after-
wards.
If you haven't already selected the appropriate custom properties, right click each calibration
ensemble and pick 'Custom Properties...' from the context menu:
This will bring up a dialog, where you can specify which custom properties to include with the
images:
When the appropriate custom properties have been included, press OK and they will now
appear in the calibration image properties under the heading 'Variables':
PAGE | 638
PAGE | 639
Enter the corresponding property values for each ensemble or image.
Please note:
The units for both concentration and laser energy are arbitrary, but you should of course be
consistent. Whatever units you apply here will be used in subsequent LIF Processing.
16.46.2 Performing the calibration
When all nominal concentration values have been entered along with laser pulse energies (if
appropriate), the actual calibration can be performed: Select (Mouse-Click + Space-Bar) all the
ensembles containing calibration images, then right-click one of them (usually the first or the
last of them) and pick 'Calibrate...' from the context menu:
Select 'LIF?Calibration in the 'Calibrations' group and click 'OK' to bring up the LIF Calibration
Recipe:
First you can specify which scalar quantity the LIF?experiment is measuring, Concentration,
Temperature or pH.?This affects only the textual description of results and has no effect on
the numerical values. You must also specify which custom property the calibration should fit to
when performing the calibration:
You can in principle pick any available custom property, including user defined properties, but
normally you will choose a custom property matching the scalar type, i.e. Concentration, Tem-
perature or pH.
In the Scalar setup you specify the range of scalar values that you wish the calibration to
cover. Initially you will typically include all the calibration images, but later you may discover
that the LIF response is outside the linear range (typically due to saturation and/or re-absorp-
tion of the fluorescent light). In that case you may wish to modify the calibration (or make a
new one), such that the highest concentrations are excluded (in which case the subsequent
LIF?experiments should of course also be limited to this reduced max concentration).
Similarly 'Laser power compensation' is used to specify the range of nominal laser energies
that you wish to include in the calibration. In general the highest energies will also produce
the strongest response to fluctuating concentrations or temperatures, but you should of
course avoid saturating the camera or the fluorescent dye. In the worst case scenario too
PAGE | 640
PAGE | 641
high laser pulse energies may cause photo bleaching, where the fluorescent molecules break
apart under the influence of too high local laser intensity. If you decide to reduce the max
laser power used you need not make a new calibration, you can simply modify the recipe of an
existing calibration such that the highest laser pulse energies are excluded.
In 'Emission input' you need to tell DynamicStudio where to find information about the laser
pulse energy; If you have a pulse energy monitor, specify the analog input channel (0-3) to
which it is connected, if not, select 'Variable', to activate the next drop down selection and pick
the appropriate custom property there (typically 'Emission').
Please note:
It is not mandatory to supply information about the laser pulse energy. If there are no sig-
nificant shot-to-shot fluctuations in the laser pulse energy, you can exclude this from the cal-
ibration and subsequent LIF processing.
The last entry in the LIF?Calibration Recipe will be enabled only if the calibration images are
acquired in Double-Frame mode, in which case you may choose to calibrate on the basis of the
first (A) or second (B) frame from each. Whatever you choose the subsequent
LIF?measurements should of course be acquired and processed in the same way for the cal-
ibration to remain valid. It is generally recommended to use frame 1 since fluorescence in
frame 2 might be disturbed by the first of the two laser pulses (especially important if the
time between the two pulses is short).
You can now click 'Apply' or 'OK' and a calibration dataset will appear in the DynamicStudio data-
base:
Opening this calibration will show one or more curves illustrating the typical response of an
average pixel to variations in the scalar quantity (concentration, temperature or pH):
The actual calibration includes such curves for each and every pixel in the entire image, some
of these will have stronger response to varying concentrations, some will have weaker
response.
Each of the lines in the figure above illustrates the response at varying laser intensities and
you will normally see multiple lines only if you manually entered nominal laser pulse energies
as custom properties. The vertical bars illustrate the level of variation in grayscale values
when nominal concentration and laser energy is otherwise constant Click any of these and the
corresponding line will be highlighted in red and nominal laser energy E will be listed along
with correlation coefficient R for the linear fit. R-values close to 1 indicate good linearity, but in
practice you should evaluate linearity visually by looking at the curve and the bars. To properly
estimate whether or not you remain in the linear regime you will probably need the cal-
ibration to cover more than the 3 nominal concentration values used in this example. In prac-
tice you should probably aim at minimum 4-5 nominal values.
16.47 LIF  Processing
Using a LIF?Calibration dataset together with LIF (Laser Induced Fluorescence) images allows
you to determine the spatial distribution of scalar quantities such as concentration, tem-
perature or pH.
When performing LIF experiments most of the work lie in careful calibration ("LIF Calibration"
on page?636). When performing the actual LIF?measurements you of course need to make
sure you stay within the calibration's valid range of concentrations, temperatures, laser ener-
gies etc, but the actual processing of the acquired images is quite straight forward:
Preselect the LIF-Calibration dataset (Click + Space) and then select the ensemble(s) containing
the LIF images that you wish to process
PAGE | 642
PAGE | 643
This will open the recipe for LIF?processing:
The main thing to consider when performing LIF?Processing is how to compensate for fluc-
tuating laser pulse energies.
If the chosen LIF Calibration does not cover fluctuations in Laser Pulse energy, the recipe sec-
tion labeled 'Laser power compensation' will be grayed out and not accessible.
Otherwise you may have four options regarding how to measure and compensate for fluc-
tuating laser pulse energy:
l  Region of interest
Choosing region of interest will allow you to specify one or more regions within the cam-
eras field of view, where the scalar value (concentration, temperature or pH) is known
and constant:
PAGE | 644
PAGE | 645
In the example above a jet of clean water is injected sideways into a flow of Rhodamine
seeded water. Upstream of the jet a ROI of constant concentration is chosen.
l  Custom property  value
The recipe option 'Use input value (E) from variable' require that you have entered nom-
inal Laser Pulse energy as a custom property of the measurement ensemble.
l  Nominal value
The recipe option 'Use nominal E-value of' allows you to enter nominal laser pulse energy
directly in the recipe thereby overriding pulse energy values from other sources (if avail-
able).
l  Value from analog input (Pulse Energy Monitor)
The recipe option 'Use analog channel' should be used if you have a pulse energy monitor
attached to the laser and connected to an analog input in the measuring system. You must
specify which of the available analog inputs the Pulse Energy Monitor is connected to.
Finally you may choose to 'Clamp to calibration limits' and if parent ensembles contain double-
frame images, specify whether to process frame 1 (Image A) or frame 2 (Image B).
It is recommended to always 'Clamp to calibration limits', which will truncate computed scalar
values at the upper and lower limits covered by the calibration. It is not recommended to
extrapolate beyond the calibration's range of validity and doing so will also allow extreme val-
ues to pass unhindered. Extreme values can be caused by unusually dark or bright areas (f.ex.
air bubbles) and will at best disturb the display routine such that nothing can be seen.
If double-frame images are used it is generally recommended to process on the same frame
as the one used when the calibration was performed (assuming this was done using double-
frame images also).
Finally press 'OK' or 'Apply' to perform the actual LIF processing and obtain a floating point
image describing concentration, temperature, pH or whatever the experiment is designed to
measure:
16.48 1.  Mie-LIF  SMD  -  General
Both SMD Calibration and Process require a data structure as below:
Project
---Calibration
---Camera 1# (Calibration Target)
---Camera 2# (Calibration Target)
---Camera 1# (Dye=ON)
---Camera 2# (Dye=ON)
---Camera 1# (Dye=OFF)
---Camera 2# (Dye=OFF)
---Run Z=Zmin
---Camera 1#
---Camera 2#
---Run Z=Zmin + Zstep
---Camera 1#
---Camera 2#
…
---Run Z=Zmax
---Camera 1#
---Camera 2#
PAGE | 646
PAGE | 647
Please refer the Reference and User Guide of Mie/LIF SMD measurement for details regard-
ing how to acquire images.
NOTE:
The calibration method and SMD process expect that camera 1# is the LIF camera and camera
2# is the Mie camera. This must not be changed by the user.
To determine the quantitative SMD a PDA is used as a reference to correct for any serious
deviations. Diameter and SMD data are collected over a 3D traverse grid enclosing the spray
of interest. The mathematical model used to fit the PDA data to the SMD imaged result is:
(a)
where :
X = Reference SMD (PDA data)
Y= Mie-Lif ratio
γ= Absorption
a0, a1, α ,s , and are the coefficients to be determined.
The coefficients are determined by calculating over a range of those four parameters men-
tioned above, using linear regression then and determining the values that give the smallest
residues.
The absorption due to the spray between the measurement plane and the camera is also con-
sidered in the SMD calibration and process, and will be calculated by comparing the signal
acquired at different ‘Z’ positions.
In DynamicStudio, the dataset is first calibrated by SMD calibration to determine the para-
meters shown in Equ. (a). Once the parameters are calculated, the SMD calibration results can
be applied to the dataset to generate a 2D SMD distribution map by the SMD Process.
16.49 2.  SMD  Calibration
The typical input data is introduced in section 1, please follow the procedure below to find the
right coeffients in Equ. (a)
1. Select all the dewarped images as input in Dynamic Studio (Fig. 1, left), including back-
ground images with and without dye as well as spray images at acquired at all ‘Z’ position.
2. Right click one of images residing in the ‘Calibration’ category, select ‘Calibrations’ and enter
the interface to select ‘SMD Calibration’, as shown in Fig. 2 (right).
Figure 1 Select ‘SMD Calibration’
3. Press ‘OK’ and a recipe dialog will be displayed consisting of six tabbed pages of input:
・ Mie optical setup
PAGE | 648
PAGE | 649
Figure 2 Mie optical Setup in SMD Calibration
The Mie optical setup (shown in Fig. 2) contains the efficiencies for all the components related
to the transmission of laser light into the measurement volume for the Mie process. The user
can select the efficiencies for the mirror, beam splitter, laser and windows. If the image is a
double imaged the user can select which image is relevant. Any component that is missing or
deactivated is simply marked with unity (1.0).
・ LIF optical setup
Figure 3 LIF optical setup in SMD Calibration
The LIF optical setup (shown in Fig. 3) is identical to the Mie setup and in most setups it will
have indentical parameters.
・ SMD setup
Figure 4 SMD setup in SMD Calibration
The SMD setup (shown in Fig. 4) contains four adjustment parameters that affect the filtering
of the final SMD result:
? Mie coefficient: low pass filter based on the percentage of the Mie mean image value ?
mainly to remove strong reflection.
? LIF coefficient: low pass filter based on the percentage of the LIF mean image value ?
mainly to remove strong reflection as well.
? Minimum SMD: the minimum acceptable SMD value.
? Maximum SMD: the maximum acceptable SMD value.
・ PDA input setup
PAGE | 650
PAGE | 651
Figure 5 PDA setup in SMD Calibration
The PDA input setup (shown in Fig. 5) assists in the loading of one or more PDA statistics data-
sets. The user can select the amount of acceptable deviation in position and the assignment of
coordinate axes. The user can either press ‘Add’ button to import data from .lda project (data
file acquired from Dantec Dynamics PDA system), or press ‘Import’ to import data from .txt or
.xls file.
Note
If the PDA results are imported from text or excel file, it should contain data needed as the
same layout; no headline is allowed in this file.
・ Coordinate setup
Figure 6 Coordinates setup in SMD Calibration
In this page (shown in Fig. 6) the user needs to define the relation between the coordinates
used for imaging and the one used for PDA measurement. The user can also define where
the nozzle is located in the acquired images.
Once everything is defined, click ‘Apply’, the SMD calibration will be performed. The calibration
results can be displayed either as Fig. 7 or as numerical (Fig. 8), which will also show the value
of those coefficients in Equ. (a).
Figure 7 SMD calibration results display
PAGE | 652
PAGE | 653
Figure 8 Numerical display of SMD Calibration results
X, Y, Z: coordinate of each point for comparison;
SMD ref: SMD results measured by PDA;
SMD Mie/LIF: SMD results measured by Mie/LIF (without calibration);
A0, A1, α, β: correspond to parameters shown in Equ. (a) ? They are displayed as identical for
all points;
Corr. & Residual: Correlation & Residual between calibrated SMD Mie/LIF results to SMD PDA
results. ? They are displayed as identical for all points;
16.50 3.  SMD  Process
Once the SMD calibration is performed, the calibration results can be applied on spray images
with similar conditions (mainly regarding ambient pressure, ambient temperature and spray
optical density) by SMD processing, which will provide the SMD 2D distribution in the end.
Similarly to the calibration the user is required to select the dewarped background images
(both Dye ON and Dye OFF) for all cameras, dewarped spray images at all ‘Z’ position and the
calibration results. Right click one of the selected image residing within the ‘Run’, and select
‘Analyze’; the interface to select Analysis Method will pop up, as shown in Fig. 9.
Figure 9 Select required inputs and choose ‘SMD Processing’ from ‘LIF Signal’ Categories in the
‘Select Analysis Method’ interface.
By selecting ‘SMD Processing’, the SMD process interface (Fig. 10 ? a-c) will pop up, which is
similar as the interface of SMD Calibration:
a)
PAGE | 654
PAGE | 655
b)
c)
Figure 10 Interface of SMD Processing
Please refer to the section of ‘SMD Calibration’ for how to setup these parameters. If the hard-
ware setup is identical with the one used for calibration, just check ‘Use optical parameters
from calibration data’; then the software will automatically read the setup from calibration and
used for SMD Processing.
Once SMD Processing is configured, press ‘Apply’, the dataset will be processed and the res-
ults will be displayed.
16.51 LII Calibration
This method is used to calibrate LII images using only 2 reference regions on the images. The
interest is obvious; The calibration is fast and simple as it relies on 2 regions with known soot
equivalent carbon concentration. Naturally, the signal inside the regions is corrected by light
sheet, global energy and local fluence before it is related to concentration values.
To use this method, select first the file of interest and call the method <LII Calibration by ROI>
located in the 'LII Signal' category. Once the dialog window is available:
1.   Select the emission channel; i.e. the analogue input used for recording the laser energy.
When no energy pulse monitor is connected, select the 'N/A' option.
2.   Select the calibration type. Typically, the Log-Log calibration is used but when the clean
combustion process is clean, a Linear-Linear calibration can be used.
3.   Write down the equivalent carbon concentration for Region_1 and Region_2 (in A.U. or
typically g. Carbon / m3)
Dialog window for LII image calibration by ROI methodology.
4.   And press the 'Region of interest 1...' to define the location of the calibration zone no. 1,
which corresponds to the reference signal 1. Run the same operation with the reference
zone no. 2
PAGE | 656
PAGE | 657
ROIdefinition for calibrated LII image processing.
5.   Switch to the 'Light sheet setup' Tab and complete the data requested.
Dialog window for the definition of the light sheet specifications.
6.   Press the 'OK' button to calibrate the LII image. Make sure the setup is not modified when
using this calibration file for data analysis.
16.52 LII Gas  composition  calibration
This method is used to calibrate LII signal according to the gas used when using image pro-
cessing based on Line-of-Sight (LoS) methodology. Naturally, the signal inside the region of
interest is corrected by the light sheet characteristics, global energy budget and local fluence
before it is related to concentration values. The result is a calibration file containing detailed
information on LII signal decay time and gas-composition dependent coefficients that are
used with the 'LII Processing' method with Line-of-Sight calibration.
To use this method, complete first the 'Properties...' of the calibration files with the exposure
time (gate time) used for each conditions (i.e. E = x.xx;). Multi-select then these files and call
the method <LII Gas-calibration calibration> located in the 'LII Signal' category. Once the cal-
ibration dialog window is available:
1.   Select the emission and transmission channels; i.e. the analogue input channels used for
recording laser energy levels. (Make sure the channels have been calibrated and the ana-
logue signal rescaled to mJ before the method is used)
2.   Write down the aperture size (diameter) of the transmission energy pulse monitor (in
mm)
3.   Specify the region on the image in which LII signal decay time and gas composition para-
meters are calculated and preferably check the 'Non-linear compensation' option for
gradient compensation in sooty conditions (See Application Manual for additional inform-
ation).
LII Gas-composition dialog window/ ... Tab for set up definition.
4.   Switch to the 'Light sheet setup' Tab and complete the laser wavelength (532 or 1064 nm),
the direction of light propagation on the LII images recorded (for signal correction before
calibration) and the light sheet characteristics (namely thickness, center position, height,
energy profile quality and optical transmission level - See LII Application Manual for fur-
ther details).
LII Gas-composition dialog window/ ... Tab for Light and light sheet specifications.
5.   Press the 'OK' button to calibrate the gas-composition.
PAGE | 658
PAGE | 659
16.53 LII Processing
This method is used to process LII images either using a calibration map defined by Region-Of-
Interest (ROI) methodology or directly using on-line Line-of-Sight (LoS) extinction coefficient
as calibration value. For further information of these approaches, please read the LII Applic-
ation Manual.
To use this method, select first the file(s) of interest and call the method "LII Processing" loc-
ated in the 'LII Signal" category. Once the <LII Processing> dialog window is available (see pic-
ture below), select the emission and transmission channels. When no transmission channel is
used, select the 'N/A' option. At that step, the software automatically switches to the
adequate processing mode and enabled/disabled options and the 'Light sheet setup' Tab
accordingly.
Dialog window for LII image processing.
Content:
l  LII processing procedure using calibrated Region-Of-Interest (ROI) methodology
l  LII processing procedure using Line-of-Sight (LoS) methodology
16.53.1 LII data  processing by Region-of-Interest <ROI> methodology
When selecting the region-of-interest processing method, most of the options are disabled as
the set-up is read from the calibration file defined with the numerical method <LII Calibration
by ROI>. Follow the instructions given below to complete the data processing sheet:
1.   Press the 'Region -of-interest...' button, click on the right button of the mouse and draw
the region to consider during data processing. When needed, adjust the (X1, Y1) and (X2,
Y2) coordinates of this region of interest manually. (Note it is not necessary that this
region corresponds to the light sheet position; It can be smaller or even larger.)
2.   Select the reference LII calibration file...
3.   ... and press the 'Apply' button to preview the results and 'OK' to accept it.
PAGE | 660
PAGE | 661
Example of calibrated LII image.
16.53.2   LII data  processing by Line-of-Sight (LoS) methodology
LII processing by on-line Line-of-Sight measurement relates the absorption coefficient to the
extinction coefficient, which by definition is a correct physical assumption when the soot
aggregates size is smaller than app. 120-140 micrometer.
Complete the sections in the 'Image processing' Tab that have been enabled by the selection
of this processing type:
1.   Report the aperture of the transmission monitor (typically 5-10 mm) .
2.   Specify the gate time value used on the intensifier during the measurements. (When
using double MCP intensification technology for which the effective gate time is not lin-
early related to the time set on the digital synchronize, refer to the Intensifier Unit user
manual.)
3.   Check the 'Non-linear compensation' option to enable compensation for energy gradients
.
4.   Press the 'Region -of-interest...' button, click on the right button of the mouse and draw
the area to consider during data processing.
5.   Select the composition-calibration file (see related help document)
Switch to the 'Light sheet setup' Tab and complete it as follows:
6.   Set the wavelength (532 or 1064 nm) for soot complex refractive index calculations.
7.   Specify in which direction the light sheet is propagating; namely from the right-to-the left
of the image or vice-versa. When necessary, use the method <Rotate image> available in
the Image Processing Library. (Note that this parameter is fundamental when calculating
and compensating for light extinction, energy budget calculation and energy absorption
compensations.)
8.   Complete the light sheet characteristic; namely thickness, peak position, height, energy
profile type and optical transmission of the optics used - Make sure the proper dimensions
are used!
Tab 'Light sheet setup' for LII/LoS data processing.
9.                 And press the 'Apply' button to preview the results and 'OK' to accept it and store
the LII map in the database.
16.54 Make  Double  Frame
This method is used to transform single-frame images into double-frame images by com-
bining them 2 and 2. To use this option, select an ensemble with single frame images and call
the method 'Make Double Frame' located in the category 'Image Conversion'.
As the recipe indicates you will always combine image 1 &?2 into a Double-Frame, but after
that you may continue with image 3 & 4 or combine also image 2 &?3 into a Double-Frame. If
the input ensemble contains a total of N single frames, the output ensemble will contain N-1
or N/2 double-frames depending on your selection (if N is odd you will get N-1 or N/2-1).
PAGE | 662
PAGE | 663
Press the 'Apply' button to preview the result and, if needed, check the 'Reverse frames'
option to reverse the frame order. Click on the 'Ok' button to accept the processing and
extend it to the rest of the images selected.
16.55 Make  Single  Frame
This method is used to extract a given frame from double-frame records. To use it, select the
double-frame ensemble(s) of interest and call the method 'Make Single Frame' located in the
category 'Image Conversion'. In the dialog window that appears, specify the frame to extract
and press the 'Ok' button to start the extraction.
Dialog window for image extraction from double-frame records
16.56 Make  Reverse  Frame
This method is used to swap the frames of a double-frame image so frame 1 becomes frame 2
and vice versa. To use this option, select an ensemble with Double-Frame images and call the
method 'Make Reverse Frame' located in the category 'Image Conversion'.
Press the 'Apply' button to preview the result and click on the 'Ok' button to accept the pro-
cessing and extend it to the rest of the images in the selected ensemble.
16.57 MATLAB Link
With the MATLAB Link data can be transferred from the DynamicStudio database to
MATLAB's workspace, and data-analysis performed using MATLAB scripts supplied by the
user. Results can be transferred back to the DynamicStudio database for safe keeping.
MATLAB should of course be available from the PC where DynamicStudio is running and
MATLAB should have run at least once before attempting to use the link, otherwise Dynam-
icStudio may not be able to find MATLAB.
16.57.1 Contents
Recipe for the MATLAB Link
Selecting data for transfer to MATLAB
DynamicStudio data in MATLAB's workspace
Image map input
Scalar map input
Vector map input
Generic data input
Parameter String
"General" on page?672
The Output variable
Image map output
Scalar map output
Vector map output
Generic data output
Metafile graphics output
Troubleshooting, Tips &?Tricks
Connecting DynamicStudio &?MATLAB
Running DynamicStudio and MATLAB on 64-bit platforms
User interface of MATLAB
16.57.2 Recipe for the MATLAB  Link
The recipe for the MATLAB-Link has three tabs. The first one is used to identify which MATLAB
script should be applied to process the data from DynamicStudio:
PAGE | 664
PAGE | 665
With the button labeled 'Folder...' you can identify a default folder, where you will normally
wish to look for MatLab scripts for processing.
To select a script file click the down-arrow at the right hand side and select 'Browse...'.
Having identified a processing script you can open MATLAB's script editor to investigate or
possibly modify the script by clicking the button labeled 'Edit Script'.
The second entry in this recipe tab is labeled 'Parameter string' and allows you to transfer vari-
ous parameters to the script. This way you can affect processing without modifying the script
itself.
The blue area at the bottom of the recipe is used for various status and/or error messages
that may be returned from MATLAB when you attempt to transfer data and/or run a pro-
cessing script.
The second tab identifies how you wish to transfer the contents of the DynamicStudio
ensemble(s) to MATLAB. You may choose to transfer the datasets one at a time in which case
you will be prompted after processing of the first dataset, whether or not you wish to con-
tinue processing the remaining datasets in the ensemble. If you answer yes each dataset will
be transferred to MATLAB one at a time and processed using the same script, and finally all
the results will be stored in a new ensemble.
Alternatively you may choose to transfer all datasets immediately, meaning that all data are
transferred to MATLAB before running the processing script and you will be able to return
just a single result.
With the button labeled 'Transfer Inputs to MATLAB' you can try out the transfer without run-
ning the script, which will allow you to investigate the resulting data structure in MATLAB's
workspace before starting the actual processing.
The last of the three tabs in the recipe defines the coordinate system used and is relevant for
scalar and vector maps only. You may choose to use metric units, so positions are in mm and
velocities in m/s, or you may choose pixel coordinates, providing positions and displacements
in pixel. Finally you may choose to transfer no positions at all.
PAGE | 666
PAGE | 667
The checkbox at the bottom of this recipe labeled 'Clear all variables on Apply' may be useful
to clean up MATLAB's workspace before each processing session. If you do not do this vari-
ables from a previous MATLAB session may be left behind in MATLAB's workspace and may
disturb the present session. In other cases you may specifically want to have access to data
from a previous MATLAB session in which case you should of course leave this checkbox
unchecked.
16.57.3 Selecting data  for transfer to MATLAB
If you just wish to transfer data from a single ensemble, simply select that ensemble and 'Ana-
lyze', selecting the MATLAB link from the list of possible analysis methods.
In some cases you may wish to combine different ensembles, f.ex. when combining different
types of data or combining data from two different cameras.
This require that you identify beforehand which ensembles you wish to transfer to MATLAB
by 'Selecting' them. This can be done by right-clicking each ensemble and then 'Select' in the
context menu, or simply by left-clicking each ensemble while holding the Ctrl-Key on the key-
board. Selected datasets will be identified by a small checkmark on the ensemble icons in the
DynamicStudio database tree view:
When you right-click in the database tree you also have the option to 'Unselect All'. Doing this
prior to identifying the ensembles you wish to transfer may be helpful to ensure that no
other ensembles remain selected from previous work on your data.
16.57.4 DynamicStudio data  in  MATLAB's workspace
In the following various MATLAB-specific data types and the like are used without further
explanation since the user is supposed to be familiar with MATLAB beforehand. Please refer
to separate MATLAB documentation for explanation about f.ex.. Cell Arrays.
When DynamicStudio data is transferred to MATLAB a variable named 'Input' is created in
MATLAB's workspace to hold the data.
Input is a Cell Array and each cell will contain information from one ensemble. If you transfer
data from just one ensemble Input will thus be a [1x1 cell array], while data from multiple
ensembles will make Input a [Nx1 cell array] (where N=the number of ensembles from which
data is transferred).
Input{1} is always the parent dataset, where output from MATLAB will be stored as a derived
dataset once processing has been completed. If further ensembles have been selected for
transfer they will be stored in Input{2}, Input{3} and so on starting from the topmost in the
database.
Individual cells in a cell array are identified using curly brackets, so to look at data from the
first ensemble, type Input{1} at the command prompt and you may see something like this:
>> Input{1}
ans =
name:                  'iNanoSense model 3E'
dataType:           'image'
fromMethod:    'Acquired'
datasetCount:    12
startTime:           [2006 6 27 12 39 9.7810]
PAGE | 668
PAGE | 669
imageSize:          [1280 1024]
imageOffset:     [0 0]
gridSize:             [1280 1024]
cameraName:    'NanoSense.0'
cameraIndex:    0
sensorSize:         [1280 1024]
pixelPitch:           [1.200e-05 1.200e-05]
scaleFactor:        1
timeBtwPulses: 5.000e-04
pixelDepth:        8
dataset:              [1x12 struct]
The example above shows the transfer of acquired images and all but the last entry are gen-
eral information that apply to all images in the ensemble.
The actual data is a level deeper and can be accessed through the entry named 'dataset'. If
you transfer just one dataset, you can type simply 'Input{1}.dataset' at the command prompt,
but if you transferred more than one image from the ensemble, you will have to specify which
image you wish to investigate. To see f.ex. the 2nd image in the ensemble, type:
>> Input{1}.dataset(2)?
ans =
index:          2
timeStamp: 4
frame1:       [1024x1280 uint8]
frame2:       [1024x1280 uint8]
-Obviously this example shows the transfer of 8-bit Double-Frame images from Dynam-
icStudio to MATLAB.
Please note the MATLAB convention of specifying matrix (image) height first and width
second, where you would normally say WxH, i.e. 1280x1024.
Extracting the two frames in the second Double-Frame to separate variables can thus be
accomplished with the following commands:
>>? Frm1=Input{1}.dataset(2).frame1;
>>? Frm2=Input{1}.dataset(2).frame2;
For Single-Frame images the entry named frame2 would simply not be there.
For other types of data the ensemble header will be similar, but the contents of 'dataset' will
of course change depending on the type of data transferred:
Image maps
Input{n}                   Nx1 cell array
name                 char string          Name of ensemble
dataType           char string          'image' for image maps
fromMethod    char string          'Acquired' for acquired images
datasetCount   double                 # of datasets in ensemble
startTime          1x6 double          Year, Month, Day, Hour, Minute Second
imageSize         1x2 double          Height x Width
imageOffset     1x2 double          From lower left corner of image sensor
gridSize             1x2 double
cameraName   char string
cameraIndex   double                 Identify which of multiple cameras
sensorSize        1x2 double
pixelPitch          1x2 double
scaleFactor       double
timeBtwPulsesdouble
pixelDepth       double                 For example 8, 10 or 12 bit
dataset(n)         1xN struct array
index                 double                 Identify this dataset in the ensemble
timeStamp       double                 mSec since start of acquisition
frame1              HxW uint8           -or uint16 for images of >8-bit
frame2              HxW uint8           Only present for DoubleFrames
For scalar imaging images may even be of type double representing f.ex. concentration or
temperature determined in each pixel position.
Scalar maps
Input{n}                     Nx1 cell array
name                  char string            Name of ensemble
dataType            char string            'scalars' for scalar maps
fromMethod     char string            Analysis method used to create these data
datasetCount    double                  # of datasets in ensemble
startTime           1x6 double           Year, Month, Day, Hour, Minute Second
imageSize          1x2 double           Height x Width
imageOffset      1x2 double           From lower left corner of image sensor
gridSize              1x2 double
cameraName    char string
cameraIndex    double                  Identify which of multiple cameras
sensorSize         1x2 double
pixelPitch           1x2 double
scaleFactor        double
timeBtwPulses  double
dataset(n)          1xN struct array
index                  double                  Identify this dataset in the ensemble
timeStamp         double                  mSec since start of acquisition
X                          HxW double
Y                          HxW double
S                          HxW double
Status                 HxW double
PAGE | 670
PAGE | 671
Vector maps
Input{n}                     Nx1 cell array
name                  char string            Name of ensemble
dataType            char string            'vectors' for vector maps
fromMethod     char string            Analysis method used to create these data
datasetCount    double                  # of datasets in ensemble
startTime           1x6 double           Year, Month, Day, Hour, Minute Second
imageSize          1x2 double           Height x Width
imageOffset      1x2 double           From lower left corner of image sensor
gridSize              1x2 double
cameraName    char string
cameraIndex    double                  Identify which of multiple cameras
sensorSize         1x2 double
pixelPitch           1x2 double
scaleFactor        double
timeBtwPulses  double
dataset(n)          1xN struct array
index                  double                  Identify this dataset in the ensemble
timeStamp         double                  mSec since start of acquisition
X                          HxW double
Y                          HxW double
U                         HxW double
V                          HxW double
W                         HxW double         Only present for 3D vector maps
Status                 HxW double
Generic column data
Input{n}                   Nx1 cell array
name                 char string          Name of ensemble
dataType           char string          'generic' for generic column data
fromMethod    char string          Analysis method used to create these data
datasetCount   double                 # of datasets in ensemble
startTime          1x6 double          Year, Month, Day, Hour, Minute Second
imageSize         1x2 double          Height x Width
imageOffset     1x2 double          From lower left corner of image sensor
gridSize             1x2 double
cameraName   char string
cameraIndex   double                 Identify which of multiple cameras
sensorSize        1x2 double
pixelPitch          1x2 double
scaleFactor       double
timeBtwPulsesdouble
dataset(n)         1xN struct array
index                 double                 Identify this dataset in the ensemble
timeStamp       double                 mSec since start of acquisition
ABC                   Nx1 double         Column name assigned by user
16.57.5 Parameter String
An optional string named 'ParamStr' can be available if you choose to enter such a string in the
recipe. The string is intended for various parameters that the script may require to perform
the intended analysis. The string format is chosen because it allows a mix of different para-
meter types to be transferred together without any prior knowledge or assumptions about
the format, but the script will have to evaluate the string in order to extract the desired para-
meters. If for example the parameter string contain two numbers separated using normal
MATLAB syntax they can be extracted with the following simple commands:
Params=str2num(ParamStr); % Convert string to numbers
Param1=Params(1);                 % Extract 1st parameter
Param2=Params(2);                 % Extract 2nd parameter
16.57.6 General
Apart from Input and Parameter String the transfer of data from DynamicStudio to
MATLAB?will also create a variable called 'General'.
It is a MATLAB 'Struct' with four fields:
Field name                                                            Type          Description
General.when                                              string       Date and time when data was transferred
from DynamicStudio to MATLAB
General.sessionIndex                                double     1, 2, 3, ..., N when transferring ensemble
datasets one at a time.
General.positionCoordinates                    string       'none', 'pixels' or 'metric' corresponding
to the coordinate system chosen in the
'Advanced' Tab
General.acquisitionSystemSettingsXML  string       Unformatted xml-string describing where
the data in 'Input' comes from and how it
was acquired
The parameter 'sessionIndex' is similar to Input.dataset.index, but the latter refers to the
acquisition index, which may contain gaps in the sequence and does not necessarily start at
one.
16.57.7 The Output variable
In order to retrieve analyzed data from MATLAB a global variable named Output must be
used.
Please remember to declare the variable global before assigning any data to it, by using the
following simple statement in your script:
global Output
Having declared the Output variable as global you can start assigning values to it, choosing
from the following list as appropriate:
PAGE | 672
PAGE | 673
Field name                           Type                        Values    Description
Output.name                    string                                   Ensemble name
Output.type                      string                   image
scalars
vectors
generic
figure
Optional dataset
type
Output.pixelDepth          double                 8-16 or
64
Pixel depth for
image
Output.dataset                struct
Output.dataset.frame1  (u)int8
(u)int16
double
Pixel data
Output.dataset.frame2  (u)int8
(u)int16
double
Pixel data
Output.dataset.U            2D-array of
doubles
Vector data
Output.dataset.V             2D-array of
doubles
Vector data
Output.dataset.W           2D-array of
doubles
Vector data
Output.dataset.S             2D-array of
doubles
Scalar data
Output.dataset.Status    2D-array of
doubles
Binary coded status
value
Output.dataset.(Xxx)      Int32
uint32
float
double
A column named '
(Xxx)' will
be created in the
generic
output dataset
Output.type
If field Output.type is present then an output dataset is created as indicated:
Values          Dataset class  created
image
images
imagemap
Image Map
scalar
scalars
scalarmap
Scalar Map
vector
vectors
vectormap
Vector Map
generic
columns
Column data plus figure if one is open when data is returned
figure
picture
metafile
Dummy column data plus metafile of figure
Values are not case-sensitive.
If field Output.type is not present then the output dataset's type is determined from field
names:
l  image when 'frame1' is found
l  3D vectors if U and V and W
l  2D vectors if U and V, but not W
l  1D vectors (Scalars) if only U or S
l  otherwise a generic column based dataset is created
Output.dataset
Image Map output
Allowed fields in Output.dataset are frame1 and frame2. Must be 2D arrays of int8, uint8,
int16, uint16 or double. Image width and height are deducted from the dimensions of
frame1:
image.width = GetN(frame1)
image.height = GetM(frame1)
Verification of frame2 dimensions is performed and user notified on mismatch.
If Output.pixelDepth is not present pixelDepth is set to 8, 16 or 64 according to array type.
Scalar or Vector Map output
Allowed fields in Output.dataset are U,V,W,S, and status or Status. Must be 2D arrays of int32,
uint32, single, or double. Grid dimensions are deducted from U (or S) and all other fields are
verified against the array dimensions of U (or S).
Generic output
All fields in Output.dataset will result in a column of the same name and type. Must be 1D
arrays of int32, uint32, single, or double. Both vertical (Mx1) and horizontal (1xN) 1D arrays
are allowed, but 2D arrays are not supported (you can of course return such arrays column by
column). Columns do not need to be of same length.
From within DynamicStudio such a generic column dataset can be displayed numerically or
using a XY-plot (See "XY Display" (on page?1040)). To do so select the MATLAB ensemble in the
database tree and click the XY-Plot icon () in the toolbar. Double-click the resulting display to
choose which values to show on which axis.
If the XY-plot does not suffice you can also use MATLABs built-in visualization tools to gen-
erate a figure and save a screendump of such a figure in the DynamicStudio database along
with the numerical values: An optional metafile is supported to save a copy of a MATLAB fig-
ure if a figure window is open at the time data is returned to DynamicStudio. Multiple figures
PAGE | 674
PAGE | 675
can only be returned if they are shown in the same display window. If multiple figure windows
are open only the most recently accessed will be transferred.
Generic output - metafile only
When Output.type = figure, picture or metafile a column dataset is created with zero columns,
but a metafile with a copy of figure window contents can be transferred.
16.58 Troubleshooting,  Tips  &?Tricks
Connecting DynamicStudio &?MATLAB
In general the installation of DynamicStudio should be postponed until MATLAB has been
installed, run, closed again and the PC restarted. In most cases this will ensure that Dynam-
icStudio can find MATLAB once installed.
If DynamicStudio cannot find MATLAB or fail to connect to it, MATLAB?may not appear in the
Windows registry database as a COM server. Try reinstalling MATLAB and restarting the PC. If
this does not help you can register MATLAB?manually, by opening a Command Prompt Win-
dow as an administrator:
Click Start -> All Programs -> Accessories -> Right-Click 'Command Prompt' and select 'Run as
administrator':
Answer 'Yes' when the User Account Control prompts you to confirm that 'Windows Com-
mand Processor' should be allowed to make changes to the computer.
In the resulting command window type 'matlab -regserver' and press Enter:
MATLAB should now start with just the command window, and the Command Prompt Window
above can be closed.
DynamicStudio should now be able to find and connect with MATLAB, if not you may have to
restart the computer once more.
Alternatively you can register MATLAB as an Automation Server from the MATLAB Command
Window instead of the Windows Command Window described above;
This requires that you start MATLAB with the 'Run as administrator' option :
PAGE | 676
PAGE | 677
When MATLAB is up and running type ...
>> regmatlabserver
...and press enter.
If you started MATLAB normally (i.e. not 'Run as Administrator') you will get an error message
if you try this.
Otherwise you can now close MATLAB, restart the PC and power up MATLAB and Dynam-
icStudio normally.
When DynamicStudio has established connection with MATLAB once, it should be able to do so
again without using the Windows Command Prompt, so you need to register only once.
Running DynamicStudio and MATLAB on 64-bit platforms
On a 64-bit hardware platform running 64-bit Windows, MATLAB will by default be installed as
a 64-bit application. DynamicStudio 4.00 and onward are 64-bit applications also, but Dynam-
icStudio 3.41 and older are 32-bit applications. They will run smoothly on a 64-bit platform, but
be unable to connect with 64-bit applications, meaning that the link between the two pro-
grams cannot be established.
To overcome this problem you may install MATLAB as a 32-bit application, with which Dynam-
icStudio should have no problems establishing a connection. According to MathWorks, the sup-
pliers of MATLAB, 32-bit MATLAB on a 64-bit platform is not officially supported, but should
work for most users:
Quote (Solution ID 1-1CAT7): ...
"Since MathWorks offers 64-bit MATLAB binaries for both Windows and Linux, we do
not support running 32-bit MATLAB binaries on 64-bit Windows or Linux architecture.
However, even though we do not officially support these configurations, installing
and running 32-bit MATLAB on 64-bit Windows or Linux machines is expected to
work for most users."
...endquote.
User interface of MATLAB
For historical reasons and compatibility with older versions, MATLAB will by default be
launched with a simple command window when started from inside DynamicStudio. With later
versions of MATLAB?a graphical user interface was introduced and it can in fact be set up to
accept connections from other software such as DynamicStudio. It requires that you start up
MATLAB?manually and enter the following command:
>> enableservice('AutomationServer',true);
-if you use the MATLAB-Link frequently you may choose to add this command to the script
startup.m, which will be executed automatically every time MATLAB?is started. If you start
MATLAB?before attempting to use the link and enable the automation server as described
above, DynamicStudio will establish a link with the running version of MATLAB instead of open-
ing a new command window. Once data has been transferred from DynamicStudio to MATLAB
you can open, edit and display variables from inside the graphical user interface and run or
debug scripts using MATLAB's integrated script editor.
16.59 Moving  Average  Validation
This method is used to validate vector maps by comparing each vector with the average of
other vectors in a defined neighborhood. Vectors that deviate too much from their neighbors
can be replaced by the average of the neighbors as a reasonable estimate of true velocities.
16.59.1 Using the <Moving-average validation  > method
To use this method the ensemble containing vector maps of interest and look for the method
in the category 'PIV Signal'. Parameters such as:
l  The averaging area size (MxN)
l  The acceptance factor (Ap)
l  The number of iterations (n)
l  (and addition al options as well)
are set by the user. (Press the 'Default' button to get values of reference.)
PAGE | 678
PAGE | 679
Note that the size of the (M x N) averaging area has no maximum and that M and N can be set
independently. However, the larger this area, the smoother the vector field becomes which
leads to a loss of resolution. (In most cases, M = N = 3, 5 or 7)
The acceptance factor 'Ap' is the single parameter used to determine whether a spurious vec-
tor is found (inside the area M x N) and shall be replaced by a vector. This 'new' vector is cal-
culated by local interpolation using 'n' iterations. (Typical values are Ap = 0.12 - 0.15 and n = 2
or 3).
(Top) Instant velocity vector map calculated by Adaptive correlation methodology (16 x 16, 25
% overlapping) and (Bottom) moving-average vector results.
16.60 N-Sigma  Validation
N-Sigma validation is very common and straight forward for uni-variate (one-dimensional)
measurements, where you simply compute mean, μ, and rms, σ, from a series of data and sub-
sequently reject all samples lying more than N・σ from the mean.
Introducing the normalized radius r this means that each sample x is considered valid only if
the following is fulfilled:
PAGE | 680
PAGE | 681
…or equivalently…
where acceptance limit 'N' is user defined while mean μ and standard deviation σ are com-
puted from a series of M samples by the classic formulas:
Assuming the data is normally distributed the probability density function is:
In which case, we should find that…
68% of the (valid) samples are within μ±1・σ,
95% of the (valid) samples are within μ±2・σ,
99% of the (valid) samples are within μ±3・σ.
In practice N-Sigma limits of 4-6 will normally include all valid samples and reject outliers even
if the data does in fact not follow a normal distribution.
Please note that the squared normalized radius r2 is used both for validation and in the defin-
ition of the probability density function itself.
For multi-variate (multi-dimensional) measurements you can of course apply N-Sigma val-
idation to each variable independently, but you may falsely validate outliers, especially if the
data is correlated. Instead the N-Sigma validation is based on a fit to a multi-variate Normal dis-
tribution, defined by the probability density function:
…where…
k is the dimensionality (i.e. the number of values in each sample),
x is the k-dimensional sample vector,
μ is the k-dimensional mean vector and
Σ is the k?x?k covariance matrix:
… introducing the shorthand Cij=Cov{Xi,Xj}. Note that Cii=Cov{Xi,Xi}=Var{Xi}.
Σ is sometimes referred to as the dispersion matrix, since it describes the “spread” of the k?
dimensional distribution. The uppercase Σ symbol corresponds to the lowercase σ normally
used to describe the spread of a uni-variate (1-dimensional) normal distribution and does thus
NOT represent summation as is otherwise conventional.
|Σ| and Σ-1 is respectively the determinant and (matrix) inverse of Σ. If the determinant |Σ|
is zero, the covariance matrix is singular and Σ-1 does not exist. Generally this means that the
true dimensionality of the samples is smaller than k.
For k=3 we could f.ex. be measuring 3-dimensional velocity vectors (u,v,w) and get:
Fitting sample data to a multi-variate normal distribution require us to estimate the mean vec-
tor μ as well as the Covariance matrix Σ:
(…where Σ means summation as usual…)
For k=1 we get |Σ|=σ2 and Σ-1= σ-2, so the probability density function becomes the classic
uni-variate (1-dimensional) Gaussian above.
Comparing the probability density functions for the uni- and multi-variate normal distributions
the squared normalized radius r2 remains a scalar, but is now computed by matrix mul-
tiplications. For the uni-variate normal distribution r2was validated by comparison with a user
defined N2 and that remains the same for the multi-variate case:
Each sample x is considered valid only if the following is fulfilled:
-where acceptance limit N is specified by the user.
The recipe for N-Sigma Validation looks like this:
PAGE | 682
PAGE | 683
There are 4 groups of selections to make:
In the topmost group the user specifies whether or not previously invalidated and/or sub-
stituted input data should be included in or excluded from the analysis.
In the second group the user identifies the temporal window from which the statistics should
be extracted. The default is 'Include All' and the recommended choice if the physical phe-
nomenon under investigation can be considered stationary at least for the duration of the
experiment. If it is not stationary you may choose to extract the statistics from a sliding tem-
poral window centered around each dataset. If used the temporal window length must be
odd (to ensure symmetry) and the lowest accepted (but not recommended)?window length is
9, meaning current sample plus 4 before and 4 after. Please note that at the start and end of
a dataseries (i.e an ensemble) the sliding window is truncated since there are no data available
before the first or after the last dataset. Consequently the number of datasets included is
even lower and with a nominal window length of 9 you can have as few as 5 neighboring data-
sets included. Statistics derived from just 5 samples is obviously not very reliable and very
likely to let outliers pass undetected.
The third group of recipe selections will show the various data available in the parent
ensemble and let the user choose which of them to include in the analysis. It is the users own
responsibility to make meaningful choices. In the example above you can f.ex. validate on the
basis of pixel displacements or measured velocities, but it will make little sense to include both
in the same analysis since the two are strongly correlated and including f.ex. U-velocity will
provide no new information if U-displacement is already included.
The last and final recipe setting is the limit for computed N-Sigma above which a sample will be
considered an outlier. Provided you have enough data for reliable statistics reasonable limit
values will typically be 4-6, but please note that the number of samples included in the stat-
istics limits the N-Sigma value you might find regardless of the presence of outliers. If you
compute statistics on M samples the computed N-Sigma value is limited to:
With M=25 samples you will f.ex. never get r-values above 4.8, so setting the acceptance limit
to 5 or more is guaranteed to find no outliers at all no matter how bad they are.
16.61 Octave  Link
The Octave Link can transfer data from the DynamicStudio database to Octave's workspace,
and data-analysis can be performed using Octave scripts supplied by the user. Results can be
transferred back to the DynamicStudio database for safe keeping. For general information
and documentation on Octave see https://www.gnu.org/software/octave/. Octave itself can
also be downloaded from there.
The implementation and integration of the Octave Link was by design made as close as pos-
sible to the MATLAB Link. This means that most scripts developed for the MATLAB?Link can
usually also be used on Octave Link.
16.61.1 Contents
Recipe for the Octave Link
Selecting data for transfer to Octave
DynamicStudio data in Octave's workspace
16.61.2 Recipe for the Octave Link
The recipe for the Octave-Link has four tabs. The forth tap is named Configuration. Before
you can use the Octave Link DynamicStudio must know where Octave is installed. Click Brows
and brows down in to were Octave is installed. Select the file named Octave.exe and click OK.
PAGE | 684
PAGE | 685
Ones this has been done DynamicStudio will remember this path.
The first one is used to identify which Octave script should be applied to process the data from
DynamicStudio:
With the button labeled 'Folder...' you can identify a default folder, where you will normally
wish to look for Octave scripts for processing.
To select a script file click the down-arrow at the right hand side and select 'Browse...'.
Having identified a processing script you can open Octave's script editor to investigate or pos-
sibly modify the script by clicking the button labeled 'Edit Script'.
The second entry in this recipe tab is labeled 'Parameter string' and allows you to transfer vari-
ous parameters to the script. This way you can affect processing without modifying the script
itself.
The blue area at the bottom of the recipe is used for various status and/or error messages
that may be returned from Octave when you attempt to transfer data and/or run a pro-
cessing script.
The second tab identifies how you wish to transfer the contents of the DynamicStudio
ensemble(s) to Octave. You may choose to transfer the datasets one at a time in which case
you will be prompted after processing of the first dataset, whether or not you wish to con-
tinue processing the remaining datasets in the ensemble. If you answer yes each dataset will
be transferred to Octave one at a time and processed using the same script, and finally all the
results will be stored in a new ensemble.
PAGE | 686
PAGE | 687
Alternatively you may choose to transfer all datasets immediately, meaning that all data are
transferred to Octave before running the processing script and you will be able to return just
a single result.
With the button labeled 'Transfer Inputs to Octave' you can try out the transfer without run-
ning the script, which will allow you to investigate the resulting data structure in Octave's work-
space before starting the actual processing.
The third of the four tabs in the recipe defines the coordinate system used and is relevant for
scalar and vector maps only. You may choose to use metric units, so positions are in mm and
velocities in m/s, or you may choose pixel coordinates, providing positions and displacements
in pixel. Finally you may choose to transfer no positions at all.
The checkbox at the bottom of this recipe labeled 'Clear all variables on Apply' may be useful
to clean up Octave's workspace before each processing session. If you do not do this variables
from a previous Octave session may be left behind in Octave's workspace and may disturb the
present session. In other cases you may specifically want to have access to data from a pre-
vious Octave session in which case you should of course leave this checkbox unchecked.
Octave does log the communication with Octave in the Log windows of DynamicStudio. The
checkbox at the very bottom of this recipe labeled 'Use global Log windows' will if checked add
even more information during data transfer.
16.61.3 Selecting data  for transfer to Octave
Please see MATLAB link for a description.
16.61.4 DynamicStudio data  in  Octave's workspace
Please see MATLAB link for a description.
16.61.5 The Output variable
Please see MATLAB link for a description.
PAGE | 688
PAGE | 689
16.62 Oscillating  Pattern  Decomposition
The Oscillating Pattern Decomposition (OPD) method is based on Principal Interaction
&?Oscillation Patterns (PIPs & POPs) as introduced in 1988 by K. Hasselmann in the field of cli-
matology.
OPD is based on stability analysis of the mean flow and should be applied to time-resolved
data only. Any fluctuation of the flow is considered a kind of perturbation, which can either
grow or decay exponentially in time (if the flow is unstable or stable respectively).
The fluctuating part of the Navier-Stokes equation is modeled by a Langevin equation for a lin-
ear Markov process:
,
where u(t) is a vector of velocity fluctuations, B is the deterministic feedback matrix and
is noise driving the system (can be interpreted as the influence of smaller, unresolved scales).
The noise          forms a covariance matrix Q , while the process itself is characterized by the
covariance matrix    :
.
The Langevin equation is a stochastic differential equation, which can be transformed into a
Fokker-Planck equation. It can be rewritten for lag time    as follows:
,
where G is the Green function, which in turn can be estimated from the lag-   and lag-0 cov-
ariance matrices::
The eigenvalues gk of the Green function G are related to the eigenvalues      of the feedback
matrix B as follows:
.
The real part of eigenvalues      characterizes the decay/e-fold time       of the k'th mode
(should be negative for a stable system), while the imaginary part of eigenvalues      gives the
oscillation frequency fk of the k'th mode:
.
The e-fold time of a mode (    ), describes the time it takes for the signal amplitude to decay by
a factor 'e', i.e. the time it takes for the amplitude to decay from 1 to       .
The spatial modes are the eigenfunctions of the matrix and they are thus the empirically com-
puted eigenmodes of the system.
Note that B and G matrices are obtained solely from the time series data u(t). If the system is
well described by a linear Markov process, then our estimate of G will be independent of the
choice of    . On the other hand, if nonlinear effects are important, then G will vary sig-
nificantly with   . As long as the linear approach holds, the knowledge of the Green function
can be used for short-time forecasting of the system behavior (ignoring the noise).
The matrix G is real, but non-symmetric, so eigenvalues      and corresponding eigen-vectors,
-functions and -modes are complex.
In theory the analysis could be applied directly on the state vectors of fluctuating velocity com-
ponents u(t), but even a modest sized vector map could quickly produce very large covariance
matrices, so computations would be come very CPU demanding.
To mitigate this DynamicStudio applies the OPD analysis to the outcome of a previous Proper
Orthogonal Decomposition (POD Analysis), dramatically reducing the complexity of the ana-
lysis.
Stability analysis is performed on the Chronos of the POD modes and each eigenvalue
provides directly the frequency and e-fold time of a mode. Via the POD Topos corresponding
eigenvectors are mapped back to the spatial domain, producing complex spatial OPD modes.
Meaningful OPD analysis require time-resolved input. According to Nyquist theory we can
resolve frequencies up to half the sampling rate, meaning that the highest frequency is
sampled twice per cycle. In practice OPD?analysis require more than 4 samples/cycle and we
strongly recommend 6 samples/cycle. Reliable detection of low frequencies require the dur-
ation of the experiment (sampling rate times number of samples) to cover a reasonable num-
ber of cycles. Again you should aim for at least 4 cycles, preferably 6 or more.
The requirement of at least 4 complete cycles with at least 4 samples/cycle leads to a the-
oretical minimum sample count of 16. In practice reliable results require a much higher num-
ber of samples, at least 100 ideally 1000 or more.
Having selected an input ensemble of POD?modes, the OPD Analysis is found in the analysis
category 'Vector &?Derivatives':
PAGE | 690
PAGE | 691
The OPD recipe has two groups of settings, one to choose which POD modes to include or
exclude from the analysis, and another to sort and filter the resulting OPD modes:
The first group 'Mode selection criteria' provides options to include or exclude various POD modes from the
OPD?analysis
All of these are optional and if none of them are selected the OPD-Analysis will include all but
the last of the POD modes:
The first two selection criteria are related to the Amplitude/Energy of the various
POD?modes:
l  Setting a lower limit for Modal Energy Fraction will include all POD?modes that contribute
more than the specified fraction of the total energy.
l  Setting a lower limit for the Residual Energy will keep adding more POD?modes until the
residual energy is below the specified limit.
The last two selection criteria are related to the Chronos, one including POD?modes, the other
potentially excluding them:
l  Temporally well resolved Chronos should correlate with themselves at small lags. The Lag-
1 Auto-Correlation quantifies temporal coherence.
l  The Kurtosis of a Chronos will be high for intermittent modes, where Chronos is almost
zero most of the time, but occasionally very large.
The first three of the criteria are 'Including' and a POD mode fulfilling any of them will be
included.
The last of the four criteria is 'Excluding' and any POD?mode with Kurtosis above the specified
limit will be excluded regardless of any other selection criteria.
To ensure that the OPD?analysis can generate results, mode 0-2 is always included and the last
of the POD?modes is always excluded, no matter which selection criteria are enabled and no
matter their limit values.
Typical level of Kurtosis-Values:
A Chronos with constant values will generate a Kurtosis-value of 1.0.
A Chronos describing a clean, noise-free sine-wave will generate a Kurtosis-value of 1.5.
A Chronos dominated by Gaussian white noise will theoretically generate Kurtosis-values
around 3.0 (valid signal may be hiding in such noise).
Kurtosis-values significantly above this will typically come from Chronos with just a few large
values among a lot of very small ones. Such Chronos may be caused by and describe only
undetected outliers in the input datasets and the corresponding modes should of course be
excluded from a subsequent analysis. If intermittent phenomena is present or expected in
the experiment they too may generate high Kurtosis-Values and should of course not be
excluded, so in that case this test should be disabled.
The group 'Post processing' contain various options to filter and sort OPD?modes after they have been com-
puted:
'Minimum accepted e-fold time / acquisition timestep' provides you with the option to discard
modes with exceptionally small e-fold times:
If the computed e-fold time is significantly smaller than the acquisition timestep you may ques-
tion the reliability of the result; How can you reasonably expect to say anything reliable about
phenomena that appear to have time scales significantly below the temporal resolution of
your measurement system? Indeed such modes rarely contain anything but noise and you
may thus choose to discard them altogether.
'Sort the OPD?mode by ...'
PAGE | 692
PAGE | 693
Here you must choose to have the resulting OPD?modes sorted by frequency (ascending), by
e-fold time (descending) or by periodicity (descending). If you wish to plot the results in a style
similar to a conventional frequency spectrum, sort by frequency. If on the other hand you
want to see the most interesting modes first, you should sort by e-fold time, since generally
the longest e-fold times correspond to the most interesting modes, showing the clearest
coherent structures. Finally you may choose to sort by 'Periodicity', which is the product of fre-
quency and e-fold time. In this case the fist modes listed will be the ones where e-fold time is
long compared to the period of the corresponding oscillation.
Output from the OPD?Analysis
OPD Analysis generates modes, that are stored as separate datasets in an ensemble, but you
can not say beforehand how many there will be. The list of "raw" eigenvalues will of course con-
tain as many values as POD-modes included in the analysis, but the eigenvalues      typically
come in complex-conjugate pairs, where the one with negative imaginary part describes an
oscillation with negative frequency. Such modes with negative frequency are discarded since
they provide essentially the same information as their positive frequency counterparts. There
may be several modes with zero frequency, which are of course kept, so the number of OPD-
modes will typically be somewhat larger than half the number of POD-modes included in the
analysis. If you choose to discard modes with very small e-fold time the remaining number of
OPD modes will of course be smaller still.
As in POD Analysis Mode 0 will always be the temporal mean (DC) of the input datasets fol-
lowed by the actual OPD modes sorted as specified in the recipe.
The default display of an OPD mode is the spatial mode, inheriting the datatype of the
ancestor datasets; When the ancestor is a series of vector maps, all spatial modes will be vec-
tor maps also. This is similar to the POD?Topos, but where Topos are real the OPD modes are
complex. If you right-click the display of an OPD mode you can choose to 'Show Complex
Vector Box':
The Complex Vector Box includes a slider, with which you can vary the phase angle used in the
current display; You can step forward or backward using PgUp/PgDn to change the Phase
Angle in steps of 15 degrees or you can move it back and forth manually with the mouse. You
can also animate the phase angle forward or backward by clicking the play buttons. You can
increase or reduce animation speed by clicking repeatedly on the forward or backward but-
tons or stop it by clicking the pause button. Provided the hardware supports it frame rate is
fixed at 24 fps, while phase angle can increase or decrease in steps of 1, 2, 3, 5, 9 or 15
degrees. The fastest option will animate a full cycle in one second.
Apart from the spatial distribution each mode has its own unique frequency and e-fold time,
which are accessible either numerically via 'Open as Numeric' or graphically via 'Open as XY
Line Plot':
PAGE | 694
PAGE | 695
The first 4 columns contain Mode numbers and corresponding frequencies, e-fold times and
periodicity values. These are common for all modes and remain the same as you step through
the modes. Please note that Mode 0 (the mean) has been assigned an e-fold time of 0, even if
the mean by definition is assumed to remain constant over time and thus should have had an
e-fold time of infinity. Note also that the numerical display has two columns for each velocity
component so that you can see both real and imaginary part of the complex vectors.
By default the XY Line Plot will show frequency, e-fold time and periodicity as separate curves,
all as functions of the mode number. In the display options you may choose to change this to
show a scatter plot of e-fold time and/or periodicity as a function of frequency and get a plot
such as the one below:
In this plot the 'Show Marker' function has been enabled in the context menu in order to high-
light the current mode with a vertical line. Note also that the y-axis has been set to logarithmic
scale and thus cannot show zero. This means that mode 0 (=the mean) with e-fold time zero is
not shown at f=0.
16.62.1 Numeric Export of OPD results
If you wish to visualize OPD?results outside of DynamicStudio or process them further you can
export results to text files using 'Numeric Export'.
This is done by first selecting the ensemble with the OPD results in the database and then pick-
ing 'Export' in the File or Context menu:
Either way you reach the Numeric Export dialog:
PAGE | 696
PAGE | 697
At the top of the dialog you specify where to put the exported files, what to call them and
what index number should be assigned to the first one (it will increase by one for each file).
You also specify file type from a number of predefined options, in the example above Tecplot
has been chosen. (Read more about it here: "Numeric Export" on page?973).
The Numeric Export more or less makes a copy of the Numerical Display, which for
OPD?results is a bit unusual:
As explained above the first four columns, Mode No., Frequency, e-fold Time & Periodicity are
common for all modes and remain the same as you browse through them, while the last 14
columns are uniquely describing the spatial distribution of (complex) vectors in each
OPD?mode. There is one row per mode in the first four columns and one row per vector in the
last 14 columns, but there is no connection between the mode and vector that share a row.
Think of the Numerical Display as two separate tables shown side by side. Normally the num-
ber of vectors will exceed the number of modes, so in the first four columns you will typically
get some empty space below the last mode, which by default will be filled with zeros.
If you think of the OPD?results as two separate tables it makes sense to also export them sep-
arately. With the checkboxes in the bottom left side of the export recipe you can choose what
data to include or exclude in the exported files. To export mode frequencies and e-fold times
uncheck all but the first four:
When exporting the spatial distribution of complex modes uncheck the first 4 and pick (check)
whatever you find relevant among the rest:
In this example we've chosen to export only the metric vector locations and the (complex)
metric velocity components, while excluding grid index positions as well as pixel positions and
displacements.
For both of these exports you can of course adjust the format such as number of decimals for
each of the parameters exported. Read more about formatting in "Numeric Export" on
page?973.
16.62.2 References
[1] K. Hasselmann, (1988):
"PIPs and POPs: The Reduction of Complex Dynamical Systems Using Principal Interaction and
Oscillation Patterns"
" Journal of Geophysical Research", 93, D9, 11.015-11.021.
PAGE | 698
PAGE | 699
[2] H. von Storch, G. Burger, R. Schnur, J.-S. von Storch, (1995):
"Principal Oscillation Patterns: a Review"
"Journal of Climate", Vol.8, pp.377-400.
16.63 Particle  Tracking  Velocimetry (PTV)
Where conventional PIV (Particle Image Velocimetry) estimate the average displacement of
particle clusters within an Interrogation Area, Particle Tracking Velocimetry (PTV) aim to
determine the frame-to-frame displacement of individual particles.
Particles are detected as grayscale peaks exceeding a certain threshold value, defined as a
percentage of maximum grayvalue supported.
In the example above the threshold is set at 24%, which for an 8-bit image with a max gray-
value of 255, means that peaks with grayvalue below 61 will be ignored.
When particles have been identified on both frame 1 and 2 we need to solve the cor-
respondence problem; For each particle on frame 1 find the corresponding particle on frame
2. This is no trivial task and most likely to succeed if seeding density is low and displacements
are small.
To get a rough idea about where to look for corresponding particles a conventional one-pass
cross-correlation is performed first; Interrogation Areas are square and the user must
choose IA Size and overlap; Generally small IA's give the best spatial resolution and are least
affected by gradients in the flow, but IA's must be large enough to enclose at least 3-4
particles each and also 3-4 times bigger than the largest expected displacement. The default
IA?Size of 64x64 pixels work well in many cases.
If you do not wish to process the entire image, the second tab of the recipe allows you to spe-
cify a rectangular ROI (Region-Of-Interest) within which to identify and track particles. You can
specify upper, lower, left and right boundaries of the ROI freely or click the shortcut buttons
to get one in the center or one of the corners:
The example below show an example where PTV?has been applied to a flow that spirals out
from the center:
The overall spiral flow can be recognized, but there are also many outliers, probably due to
false matches in the attempted solution to the correspondence problem.
16.64 Peak Validation
There are four possibilities to validate on the correlation peak.
Relative to zero peak is  only  used in Auto correlation
In Auto Correlation the zero peak represent the sum of all gray values in the images. The
ideal second highest peak is 0.5 height.
Relative to peak 2
This is the relative height of the highest peak compared to that of the second highest. In
many cases of few particles, this number is not very usable. For higher particle concentration,
it is often quite satisfactory to use 1.1. If a very strict validation is required a value as high as 2
can be used.
Peak width, Minimum & Maximum
The peak width is estimated with a 3 line parabolic fit in each direction and evaluated as the
PAGE | 700
PAGE | 701
square root of the product wx*wy (for more details see the reference manual). The peak
widths are given in pixels.
16.64.1 Interactive setting and finding good parameters
With an open vector map, it is possible to have the Validation box enabled (right-click on the
map and enable). It is at the same time possible to have an active histogram (right-click on the
map and choose histogram) of the peak widths. This gives the opportunity to find optimal set-
tings in an interactive way.
Tool tip: On the vector map validation box, click on the slider and use the keyboard cursor for
fine-tuning.
16.64.2 Example using the peak validation  for phase separation
The figure below shows an instantaneous snapshot of air bubbles injected into a water tank
seeded with red fluorescent particles. The top image comes from camera 1, which was fitted
with a green filter. In principle, one should one see the bubbles. But as it can be seen on the
histogram to the right there is a large portion of correlation peaks around 2.3 pixels dia-
meter. The reason being is that the fluorescent particles also scatter some green light from
their surface.
The bottom image comes from camera 2, which was fitted with a red filter. In principle only
the seeding should be seen. It is seen in the histogram, that the bubbles are seen as well. The
scalar map in between the picture and the histogram is the peak widths generated from 400
images using the Average Correlation.
Using the peak validation, based on judgement from the peak width proportions of the Aver-
age Correlation, it was possible first to validate vector maps for identifying the fluid velocities
represented by the seeding with correlation peaks sizes around 2.3 pixels. The bobble velo-
cities were found using large interrogation regions and peak validation allowing only large cor-
relation peaks.
Below is shown the stream traces generated in Tecplot from Amtec, from the two phases.
PAGE | 702
PAGE | 703
16.65 Phase  Boundary Detection
The phase boundary detection module takes an image or double-framed image dataset and
determines the phase boundaries in two-phase flows (flame fronts for combustion data or mix-
ing of two liquids, etc). The method uses a combination of global thresholding and local
thresholding to determine the phase boundaries. The gradient can also be used to sub-
sequently filter away boundaries without a sharp gradient, which is mostly useful in flame
front investigations in combustion diagnostics.
There are three different forms of output depending on the user choice in the recipe:
l  An image mask, where the output is 8-bit image where one phase has the pixel intensity 0
and the other phase has the pixel intensity 255. This is normally used for Dynamic Masking
of each phase during phase separated PIV analysis.
l  A boundary mask, where the output is 8-bit image where the pixels on the phase bound-
aries have the pixel intensity 255 and the remaining pixels are 0. This line is normally used
to visualize the phase boundary. (red line in bottom right image in Fig. 1)
l  A numeric dataset, where the output stores all the phase boundaries/flame fronts as a list
of pixel coordinates, as well as the phase tag and hierarchical information if available.
The GUI for the Phase Boundary Detection looks like this:
Fig. 1 Graphical user interface for the Phase Boundary Detection for a mixing experiment
16.65.1 A) Preview  settings
Image number: The image index can be used to choose among the images from the parent
ensemble to be previewed.
Use region of interest: When checked, a ROI rectangle is added on the original (top left) pre-
view image, which can be used to define the ROI of the image. The other three preview
images will only show the chosen ROI. When dealing with combustion data, this should be used
to define the ROI such that the light sheet is placed on the boundary of the ROI. See Fig. 2.
Preview mode: The preview mode drop down menu can be used to select to show the ori-
ginal, threshold, gradient and result preview at the same time or show only a specific one of
them in full view.
16.65.2 B) Global threshold
Lower threshold: All pixels with intensity below this threshold are defined to be part of
phase 2 (darker phase).
Upper threshold: All pixels with intensity above this threshold are defined to be part of
phase 1 (brighter phase).
Suggest: When clicked, the module will attempt to find a suitable lower and upper global
threshold based on the currently selected preview image, which can still be further adjusted
by the user.
16.65.3 C) Local threshold
Region size: The local threshold is computed for each pixel in the image to determine which
phase it belongs to. The local threshold is computed using Otsu's method on a region around
the pixel. The region size defines the width and height of this region, with origo at the pixel.
For further information on Otsu’s method: https://en.wikipedia.org/wiki/Otsu's_method. The
phase 1 and 2 as a result of the thresholding operations can be previewed in the “Threshold-
ing” image (top right).
16.65.4 D) Filters
Flame filter: If checked, a gradient threshold is used to remove any found phase transitions
without a sharp gradient. This is especially useful for flame front investigations in combustion
diagnostics, where the flame front is by definition the surface where combustion is taking
place, and not the outer surface of the flame. See Fig. 2. Normally the flame filter is only used
for detection of flame fronts and is left unchecked otherwise.
Gradient threshold: The gradient threshold is one of the filters to remove unwanted sec-
tions of the phase boundary computed as a result of the thresholding step. There is often
sharp gradients on the flame front, and smooth gradients at the outer edges of the flame
where the flame diffuses softly. The gradient threshold is adjusted to exclude the soft gradi-
ents which are not related to the flame front.
PAGE | 704
PAGE | 705
Fig. 2 Graphical user interface for the Phase Boundary Detection for a combustion dia-
gnostics experiment.
16.65.5 E) Output
Force closed contours: When flame filter is enabled, the method attempts to remove bound-
aries without a sharp gradient. This will usually result in finding the boundary of the flame
front as an open contour of the actual flame front as shown in Fig. 3. With open contours, it is
not possible to produce a mask a mask of the different phases, so these output options are dis-
abled. By enabling “Force closed contours”, the method will attempt to keep contours closed
such that phases can be defined for each contour and masks can be outputted.
Mask of phase 1: The output dataset will be an 8-bit image mask, where all the pixels in
phase 1 have the pixel intensity 255 and all the pixels in phase 2 have the pixel intensity 0.
Mask of phase 2: The output dataset will be an 8-bit image mask, where all the pixels in
phase 2 have the pixel intensity 255 and all the pixels in phase 1 have the pixel intensity 0.
Boundary  as  image: The output dataset will be an 8-bit image mask, where all the pixels on
the phase boundary have the pixel intensity 255 and the remaining pixels are 0.
Boundary  as  numeric: The output dataset will be a numeric dataset, where all the phase
boundaries are saved numerically as point lists.
Fig. 3 Graphical user interface for the Phase Boundary Detection for a combustion dia-
gnostics experiment with open contours.
16.65.6 F) Phase label
Phase 1: A custom name can be entered for phase 1, which will be saved in the numeric data-
set.
Phase 2: A custom name can be entered for phase 2, which will be saved in the numeric data-
set.
16.65.7 Publications:
A hybrid phase boundary detection technique for two-phase-flow PIV measurements
FG Ergin, J Olofsson, P Petersson, N Fog-Gade-Nielsen
Flow Measurement and Instrumentation, 101776
https://doi.org/10.1016/j.flowmeasinst.2020.101776
16.66 Dynamic  Masking
The phase masks generated using Phase Boundary Detection can be used for dynamic mask-
ing. Moving masks can be generated from the original image ensemble by isolating the mov-
ing object that is to be masked. This can be achieved by using the image processing functions
from DynamicStudio's Image Processing Library or by using the Phase Boundary Detection. It
is essential to get zero gray scale values in the region that is to be masked out and non-zero
gray scale values in the rest of the image. Then the mask ensemble should be marked as such
by checking the tick box under “Custom Properties”. Like regular “static” image masks, the
PAGE | 706
PAGE | 707
dynamic mask is defined as fixed input by clicking on the space bar, and can then be applied on
the particle images.
Fig. 4 Analysis tree showing an image ensemble, a dynamic mask ensemble, and the resulting
ensemble of masked images. An application example is shown in Fig 5., where dynamic mask-
ing is used for the study of a 50 μm long micro-organism (Euglena Gracilis) swimming in water.
Fig. 5 (Left) Micro PIV image of Euglena Gracilis and particles in seawater, (center) Dynamic
mask generated using image processing functions, (right) Masked image where the organism
is removed from the FOV.
16.67 PIV  Uncertainty
As implied by the name the analysis method PIV?Uncertainty strives to estimate the uncer-
tainty of PIV results, specifically the uncertainty of each displacement vector in the parent vec-
tor map.
The analysis and this description operate on estimated particle displacements in pixel, but res-
ults scale to physical velocities the same way displacements do.
Please note that measurement 'Uncertainty' is not to be confused with measurement 'Error':
The 'Error' δ is the (signed) difference between a measured quantity X and the true value of
that quantity. Since the true value is unknown so is the error. The total error δ is normally
treated as the sum of systematic and random errors:
Where the systematic error β is the same every time you repeat the measurement, while the
random error ε changes (but will on average be zero).
A systematic error is notoriously difficult to deal with: If we knew about it we could (and would)
correct the measurements to come closer to the true answer. If we don't know about the sys-
tematic error there is little we can do to extract information about it from the measured data
itself. The random error however can be estimated by repeating the experiment a number of
times and then evaluate how the results fluctuate.
As a measurement technique Particle Image Velocimetry has matured enough that we can
safely say that any modern PIV system will be bias free if used properly.
In the following measurements are thus assumed bias free and 'error' refers to the random
error ε unless we specifically say otherwise.
The random error ε is assumed to follow a normal distribution with zero mean and standard
deviation σ. The standard deviation σ is used to quantify the magnitude of the random error ε
and referred to as the 'Uncertainty'. It is not a fixed upper or lower bound, on the contrary
the error can still be arbitrarily large. We can however say for example that we are 68% con-
fident the measured value deviates at most σ from the truth, 95% confident that it deviates at
most 2σ and 99% confident it deviates at most 3σ.
Uncertainty estimation strives to find a reasonable σ-value for the vector in question.
For PIV the uncertainty can be estimated for either the individual displacement components
or for the total (Euclidian) distance from the estimated displacement to the (unknown) true dis-
placement.
The figure below is a (synthetic) example of an error distribution, each of the small red circles
represent the error of a specific vector with (signed) errors εU and εV on the displacement
components and (positive) radial error εR(Dark red lines highlight an example of such an
error):
PAGE | 708
PAGE | 709
Errors on horizontal displacement U and vertical displacement V are independent and both
are normal distributed with zero mean and identical standard deviations σU=σV.
If you look at each displacement component individually 68% of the errors are within ±1σ (illus-
trated for the V?component by the shaded blue area in the left hand side) and 95% of the
errors are within ±2σ (illustrated for the U-component by the shaded blue area under the
Gauss-curve at the bottom). The errors εU and εV are independent, so the odds of both of
them being within ±1σ at the same time is only 0.68*0.68=46%, corresponding to the fraction
of the red dots within the central blue square above.
The radial errors εR are expected to follow a Rayleigh distribution with scale parameter σ (see
below). The thick green circle illustrate εR=σ and 39% of all radial errors are expected to be
within this limit. The thin green circle illustrate εR=2σ and 86% of all radial errors are expected
to be within this limit.
16.67.1 Rayleigh  distribution
If εU and εV are independent and Gaussian with zero mean and identical standard deviations
σU=σV=σ, the radial error εR will follow a Rayleigh distribution with scale parameter σ:
Note that the Scale parameter σ of the Rayleigh distribution is identical to the standard devi-
ation σ of the Gaussian (Normal) distributions.
Below the two probability density functions are plotted for comparison: 39% of all radial
errors are expected to be less than or equal to σ, while 68% of the horizontal/vertical errors
are expected to be in the range -σ<ε<+σ. The left half (-σ<ε<0) shown below covers 34% due
to symmetry.
16.67.2 Methodology
PIV?Uncertainty analysis requires a vector map as parent. The recipe for PIV?Uncertainty
offers two different ways to estimate the uncertainties:
l  Particle Disparity (aka Image Matching) as proposed by Sciacchitano et al in 2013.
l  Peak Height Ratio as proposed by Charonko &?Vlachos in 2013.
PAGE | 710
PAGE | 711
The particle disparity technique identifies individual particles on frame 1 &?2 and tries to
match pairs based on the measured displacement vectors. If particle pairs in the vicinity of a
vector all show comparable displacements the uncertainty is considered small, if particle dis-
placements in the vicinity of the vector fluctuate the uncertainty is higher. The particle dis-
parity technique will output uncertainties for both U, V & Radial errors plus the number of
matching particle pairs found within the interrogation area corresponding to the vector. Sciac-
chitano et al state that 6-7 matched particle pairs are preferred for reliable statistics, but it is
mathematically possible to perform the analysis with just 2.
For further details please see "Sciacchitano et al (2013)"PIV uncertainty quantification by
image matching"Meas Sci Technol, 24 045302." on page?718.
We deviate from the paper by deriving uncertainties for U &?V independently and by fitting a
Rayleigh to the radial errors instead of combining mean and rms of radial errors.
The Peak Height Ratio approach is based on an empirical relationship between the radial error
and the ratio between the highest and second highest correlation peak. It returns only radial
uncertainty estimates. For further details please see "Charonko &?Vlachos (2013)"Estimation
of uncertainty bounds for individual particle image velocimetry measurements from cross-cor-
relation peak ratio."Meas Sci Technol, 24(6) 065301." on page?718.
For both approaches uncertainties are returned both in pixel and converted to m/s.
Both approaches require that a parent vector is neither invalid nor substituted, since the
uncertainty analysis is otherwise meaningless and abandoned.
In such cases the resulting uncertainties are set to zero and the vector tagged as invalid (the
same happens if Particle Disparity fail to find at least two matching particle pairs within the cor-
responding interrogation areas).
16.67.3 Error messages
The Particle Disparity technique require detailed information about the interrogation areas
used to compute each vector. Vector maps generated by DynamicStudio 5.0 or later include
this information, but older vector maps do not, so you will get an error message if trying to
apply Particle Disparity to those:
To get the interrogation area info you can simply reprocess the parent vector map with the
exact same recipe settings as before.
16.67.4 Example
(From the Emerson Cavitation Tunnel, Courtesy of Newcastle University).
The following example is from a PIV experiment on a cavitating ship propeller. The propeller
itself generates a vortex with a lot of cavitation bubbles trapped at the center and further
cavitation bubbles generated at the tip of the propeller blades and traveling downstream with
the flow. A?light sheet is entering from below and we're looking at the lower half of the vortex
just downstream from the propeller itself:
PAGE | 712
PAGE | 713
In the following the image is inverted for better visibility so particles appear dark on a bright
background.
Applying Adaptive PIV generates a vector map like this one:
Applying PIV?Uncertainty analysis on this vector map will return the vector map unchanged,
but with scalar uncertainties appended to each vector map. They show up in the numerical dis-
play and in Vector Map Display Options as possible Scalar maps:
PAGE | 714
PAGE | 715
The uncertainties are referred to as UncX, avoiding the greek σ symbol.
As we might expect uncertainties are highest where cavitation bubbles and/or strong gradi-
ents are present and smallest where gradients and velocities are low:
Note the blank patches, where uncertainty analysis was given up either because the parent
vector was rejected or substituted or because the analysis failed to find two or more matching
particle pairs between frame 1 &?2. As shown above the parent vector map had 132 rejected
vectors and the uncertainty analysis rejected another 177, ending up at 309 rejected vec-
tors/scalars.
The colormap used for the uncertainty scalar map is chosen in display options 'Scalar Map
Style' and the special color for rejected vectors are chosen in the tab named 'Colors':
PAGE | 716
PAGE | 717
16.67.5 Expanded uncertainties
Please note again that the radial uncertainty returned here (UncR) is the Scale Parameter σ of
a fitted Rayleigh distribution, providing only 39% confidence that the measured displacement
deviates at most σ from the truth. If you want higher confidence you must compute an expan-
ded uncertainty. If for example you want 95% confidence the computed radial uncertainty
must be multiplied by 2.45. This can be derived from the cumulative distribution function F of
the Rayleigh distribution:
So we are 95% confident that the measured displacement (or velocity) deviates at most 2.45σ
from the truth ( σ = UncR in this case).
Similarly the uncertainties for displacement/velocity components U &?V can be expanded if
you want higher (or lower)?confidence than 68% which is the default.
Again this is based on the cumulative distribution F, but this time for the Normal Gaussian dis-
tribution (again aiming for 95%?confidence):
(relying on the inverse error function).
So we are 95% confident that the measured U or V displacement/velocity deviates at most
1.96σ from the truth ( σ = UncU or UncV in this case).
References:
Sciacchitano et al (2013)
"PIV uncertainty quantification by image matching"
Meas Sci Technol, 24 045302.
Charonko &?Vlachos (2013)
"Estimation of uncertainty bounds for individual particle image velocimetry measurements from
cross-correlation peak ratio."
Meas Sci Technol, 24(6) 065301.
16.68 Proper  Orthogonal  Decomposition  (POD  Analysis)
Proper Orthogonal Decomposition (POD) is closely related to Principal Component Analysis
(PCA) from linear algebra and was first introduced in the context of Fluid Mechanics by Lumley
in 1967 1 The original or 'Classic' POD analysis will find spatial modes only, but adding the tem-
poral modes also is fairly straightforward. Both spatial and temporal modes are orthogonal,
so Aubry et al 2called it 'Bi-Orthogonal Decomposition' and also coined the names 'Topos' and
'Chronos' for spatial and temporal modes respectively.
There are many other names for the technique, but we will use the popular 'POD', while adapt-
ing the names 'Topos' and 'Chronos' from Aubry et al.
Classic POD?is energy optimal, but all modes are broadband in the frequency domain. Fourier
Decomposition on the other hand is spectrally pure, but far from optimal in terms of modal
energy distribution. As a compromise between energy optimality and spectral purity Mendez
et al proposed Multi-Scale POD in 20193,4and this has been incorporated into Dynam-
icStudio's POD?as an option.
16.68.1 Theory
A 'signal ' that varies in space and time can be decomposed as a series of 'modes', each with a
scalar amplitude, a spatial distribution and a temporal evolution:
=  Global amplitude of the n'th mode
=  n'th Spatial mode, 'Topos'
=  n'th Temporal mode, 'Chronos'
By design both Topos and Chronos are orthonormal:
The square of the modal amplitudes describes modal 'energy' and modes are sorted by des-
cending amplitude/energy, so the dominant modes are listed first.
The Topos describes spatial variations and are independent of time.
PAGE | 718
PAGE | 719
The Chronos describe how each of the modes contribute to the reconstruction as a function
of time. These are independent of spatial locations.
POD is typically used for reduced order modeling (ROM), since an excellent approximation to
the original signal can be made from just a few of the first (high-energy) modes. The first
modes typically describe large scale coherent structures, while the last modes often describe
only noise.
16.68.2 Supported input
Most of the following examples assume that u represent velocity vectors from a time series
of vector maps, but it could just as well represent scalar values from a scalar map or pixel val-
ues from an image.
The Topos will inherit the type of the parent data, so if input data are vector maps, Topos will
be vector maps also, if Scalar Maps are provided as input the Topos will also become scalar
maps and if input is images, Topos will become images as well (double-frame images are not
supported).
Amplitude, Chronos etc are the same no matter what kind of input is used.
16.68.3 Handling the Mean
Prior to the actual POD Analysis the system computes the temporal mean and subtracts it
from the input data, such that the analysis works on the fluctuating parts only. When the POD
Analysis is complete the mean is appended to the results as 'Mode 0'. This DC-Mode is split in
three parts, Amplitude, Topos & Chronos, and also scaled the same way as all the POD-Modes,
but Mode 0 is not orthogonal to the other modes and thus not really a POD-Mode like the oth-
ers. Including the mean will however facilitate later reconstructions and its presence does not
generate any problems as long as you are aware that 'Mode 0' is special and differs in nature
from all the other modes.
16.68.4 Handling Outliers and Masked out data
POD?Analysis will check the status codes of input datasets and accept both Valid and Sub-
stituted data. Masked out or Rejected data (i.e. known Outliers) will however be excluded
from the analysis. Masking is normally not a problem since a (Static) mask will have accepted or
rejected data from a specific location the same way for all datasets in the parent ensemble, so
each location will have either all valid data or no data at all. Dynamic Masking or Outliers that
have been detected, but not substituted, does however present a problem since data from
these locations will be gappy, providing information for some, but not all of the timesteps in
the parent ensemble. POD?Analysis will attempt to handle the situation by averaging over the
number of valid data samples from each location, but due to the missing data an exact recon-
struction of the parent datasets is no longer possible. For a modest number of outliers in the
parent ensemble reconstruction errors will however be small and you should still be able to
make a good approximation.
To avoid this problem it is recommended to make sure that the input datasets contain no
known outliers before applying the POD?Analysis.
"Universal Outlier Detection" on page?873 can for example be used to substitute outliers with
a reasonable estimate based on valid neighbors.
16.68.5 Mode Count
POD Analysis generates modes, that are stored as separate datasets in an ensemble. The
number of POD?modes will correspond to either the number of datasets in the parent
ensemble or to the number of values in each dataset, whichever of these numbers is the smal-
lest. Imagine for example that you have an ensemble with 1000 vector maps, each with 39x29
2D vectors. The number of datasets (=timesteps) is of course 1000, while the number of val-
ues in each dataset is 39x29x2=2262 (39x29 vectors, each with a U- and a V-component). In
this case POD?analysis will create 1000 modes, corresponding to the 1000 timesteps in the par-
ent ensemble. If you repeat the experiment, but acquire more images so you end up with
5000 vector maps, still with 39x29 2D?vectors in each, POD?analysis will however not create
5000 modes, but just 39x29x2=2262.
Most literature on POD?assume that the number of values in each dataset is (much) larger
than the number of datasets and this is by far the most common scenario.
Should you end up in the opposite scenario, a warning will appear in the recipe:
You are free to proceed with the analysis anyway, but consider reprocessing the parent data
sets in such a way that you get more values in each dataset. For vector maps you could for
example reduce the spacing between neighbor vectors in order to get more vectors in each
vector map.
16.68.6 Recipe settings
Having selected an input ensemble of (single-frame) images, scalar or vector maps, the POD
Analysis is found in the analysis category 'Vector &?Derivatives':
PAGE | 720
PAGE | 721
Classic Snapshot POD
Input data will start loading as soon as the recipe opens as shown by a progress bar in the top
left of the recipe. The data in each dataset will be organized in a so-called 'Snapshot' vector
and a Covariance matrix is built up by computing the dot product between such snapshot vec-
tors from different datasets. On the main diagonal of the Covariance matrix we store the dot
product of snapshot vectors with themselves and right next to those we store the dot product
of temporal neighbors. N datasets will create N snapshot vectors and a symmetric NxN Covari-
ance matrix. While filling in the Covariance matrix as many snapshots as possible are stored in
RAM, but with many and/or large datasets there may not be room for them all in which case
some of the datasets will have to be loaded from disk several times. For every N'th of the
Covariance values filled in the progress bar is updated and a preliminary 2D Fourier Trans-
form computed from the present Covariance matrix (with zeros for the values not yet filled
in). From the complex FFT a real power spectrum is computed and all energy mirrored into
the 1st quadrant for display in the recipe as an image (greyscale values follow a logarithmic
scale):
The DC is at frequency (0,0) in the lower left corner and the top right corner corresponds to
the Nyquist limit at half the sample rate of the original data.
Below the 2D frequency spectrum a 1D spectrum is computed and shown by integrating each
column from the 2D spectrum display. Frequency 0 (=DC) is to the left and the Nyquist limit at
the right.
Multi-Scale POD (mPOD)
For Classic POD these spectra are of limited use and for information only, but if the Multi-Scale
AddOn is enabled it will be possible in the top right of the recipe to switch from Classic Snap-
shot POD to Multi-Scale POD (mPOD). This will enable the right hand side of the recipe allowing
the frequency range to be divided into blocks:
PAGE | 722
PAGE | 723
In the example above the frequency range has been divided into 6 blocks with 5 cutoff fre-
quencies between the outer limits f*=0.0 and f*=0.5. All frequencies are relative to the
sample rate of the parent data so f*=0.5 corresponds to half the sample rate, i.e. the Nyquist
limit. You can use the slider to browse frequencies or type in a desired value in the entry field
at the top. Pressing the button 'Add Cutoff' will round the desired frequency to the nearest
bin edge, add it to the list of cutoff frequencies and show corresponding lines over the spec-
trum displays. To remove a cutoff frequency select it in the list and click 'Clear Selected'. 'Clear
All' will remove all but the outer limits 0.0 and 0.5 so you can start over. Performing an
mPOD?analysis with no cutoff frequencies is equivalent to performing a Classic POD analysis. It
is not possible to edit cutoff frequencies already in the list, only to remove old ones and add
new.
mPOD?analysis
When you press 'OK' or 'Apply' the frequency domain representation of the Covariance mat-
rix is masked, so squares along the diagonal are kept, while the rest will be masked out as
shown in the recipe by shading. The parts masked out represent mixed signal containing both
high and low frequencies, while the squares along the diagonal represent frequency bands.
After masking the Covariance matrix is transformed back to the temporal domain and eigen-
decomposition produces eigen-values and -vectors, which in turn leads to POD?mode
Amplitudes and Chronos. Corresponding Topos are finally determined by projection of the
parent data.
Comparing results from Classic vs Multi-Scale POD the two sample Topos below clearly show
the same physical pattern (Classic POD at the top, Multi-Scale POD at the bottom):
Looking at the corresponding Chronos and their spectra the difference is however clear (Clas-
sic at the top and mPOD at the bottom still):
PAGE | 724
PAGE | 725
In both cases the black curve is the Chronos and the red one the corresponding Spectrum.
The x-axis is just a simple index, since Chronos and Spectrum have no meaningful common
ground.
Both spectra show a low frequency peak, but the result from Classic POD include high fre-
quency content, clearly visible in both the spectrum and the Chronos itself. In the mPOD res-
ults this high frequency content has been filtered away and ensure a smooth Chronos and a
'clean' spectrum. The high frequency content is accounted for by other modes.
Transition bandwidth /  Tapering
So far the frequency domain masking have been described as 'brickwall' filters with sharp
edges jumping from a transmission of 0% to 100% and back again from one frequency bin to
the next. This may produce undesired ringing phenomena in the temporal domain, which can
be reduced by tapering the filter edges so the transition from 0% to 100% and back again is
gradual over a number of frequency bins. This is controlled by the 'Transition bandwidth' in
the lower right of the recipe. The smallest nonzero transition bandwidth corresponds to 2 fre-
quency bins and filter transmission will then be 0%-25%-75%-100%. From there transmission
bandwidth can be increased in steps of 2 frequency bins up to a maximum of 16 bins. To
ensure that transmission will in fact reach 100% before it starts going back down again, trans-
ition bandwidth is also limited to half the width of the smallest of the frequency blocks chosen.
Similarly any nonzero transition bandwidth will limit how close to one another cutoff
frequencies can be set. It is therefore recommended to leave the transmission bandwidth at
0 (brick wall filters) until all cutoff frequencies have been set and then apply filter edge taper-
ing at the end.
Tapering is done exclusively in the passband of each filter in order to minimize overlap and
thus crosstalk between neighboring frequency bands.
Mode Normalization /?Scaling
As explained above, both Topos and Chronos are by default orthonormal, meaning that not
only are the individual modes normal to one another, they also have unit length.
For the Topos this means that by default the Root (of the) Sum (of the) Squares of all dis-
placement components from all vectors will equal one. For the Chronos we find similarly that
the Root (of the) Sum (of the) Squares of all contributions should also equal one. From a math-
ematical point of view this is a perfectly normal approach and corresponds to the default
Mode Normalization 'Root Sum Square' in the recipe above.
This approach however means also that the magnitude of individual vectors in each Topos will
depend on the total number of vectors and similarly the Chronos-values will depend on the
number of timesteps (/snapshots) included in the analysis. This makes it difficult to compare
results from different analyses, so you may set Mode Normalization to 'Root Mean Square'
instead. This causes both Topos and Chronos to be scaled up so the Root (of the) Mean (of the)
Squares of all elements equals one. In effect this means that the average Topos vector has a
length of one no matter how many vectors there are and similarly the Chronos values will
have an average magnitude of one no matter how many snapshots/timesteps were included
in the analysis.
To make sure the fundamental decomposition remain valid, the modal amplitudes are scaled
down, when corresponding Topos and Chronos are scaled up. Examples are shown below.
Please Note: For vectors the normalization apply to the raw pixel displacements, not to the
metric velocities derived from them and shown as default by the vectormap display. Similar
considerations apply to scalar maps such as vorticity, derived from vector maps. In the
examples below the vector map display has been set to show pixel displacements instead of
metric velocities.
Mode 0 will always be the mean of the input datasets followed by the actual POD modes sor-
ted by descending amplitude/energy.
The default display of a POD mode is the Topos, inheriting the datatype of the parent data-
sets; When input is a series of vector maps, all Topos will be vector maps also.
PAGE | 726
PAGE | 727
The Topos above are identical apart from the normalization. The first figure shows RSS-Scaled
Topos, the second figure RMS-Scaled Topos. The two are topologically identical, but note the
different numerical values in the colorbar at the bottom.
The following examples show what Topos may look like when POD?is applied to Scalar and
Image maps (both are RMS-scaled):
PAGE | 728
PAGE | 729
The POD dataset contain also Amplitude &?Chronos, and to see those graphically you must
click 'Open as XY-Plot'?(         ) in the toolbar, or select it in the File Menu:
This will open a X/Y-plot with a number of different curves. Double-Click it to get a list of the
data available and choose which to show on the x-axis, which on the y-axis and which (if any) to
hide:
This lists all the information that a POD?mode contains beyond the Topos. The example above
has been set up to plot Chronos as a function of Time and you should get a plot like this:
PAGE | 730
PAGE | 731
The first figure shows the result when using the (default) 'Root Sum Square' normalization,
while the second is the same data, but using 'Root Mean Square' normalization instead. As for
the Topos the two are identical except for scale (note the values on the y-axes). For clarity the
plots above are zoomed in on the first ~50 Chronos values instead of showing the full set.
You can also see a frequency domain spectrum of the Chronos:
Black curve is the Chronos, red curve the corresponding spectrum. Only positive frequencies
are shown and zeropadding has been applied to get as many frequency bins in the spectrum
as there are timesteps in the Chronos. A bit of smoothing has been applied, but some residual
jitter may be seen in the spectrum. The spectrum is scaled to fulfill Parseval's theorem, so sum
of squares of the spectrum correspond to the sum of squares of Chronos values. Apart from
that the actual spectrum values have neither units nor physical meaning, the spectrum aims
only to describe frequency content of the Chronos and identify dominant frequencies if any.
With the XY Display Setup you can in principle plot any two parameters as functions of one
another, it is your own responsibility if the resulting curves make any sense. Chronos is for
example the only parameter that makes sense as a function of time and Spectrum the only
that makes sense as a function of frequency. All other parameters should be shown as a func-
tion of the Mode Number. Mode Amplitude distribution is one such plot:
As before the first figure shows POD?results when the default 'Root Sum Square' nor-
malization is used, while the second figure shows the same data when modes are normalized
using 'Root Mean Square'. As explained above switching from RSS- to RMS-Normalization will
PAGE | 732
PAGE | 733
increase Topos- and Chronos-values and decrease modal Amplitudes. This is clearly seen when
comparing values on the y-axes of the two plots above. For clarity both plots are zoomed in on
the first 40 modes instead of showing the full set.
From the modal amplitude distribution we can directly derive the energy distribution in vari-
ous forms:
l  The Energy Fraction is proportional to the square of the Amplitude, but scaled so Sum
(Energy Fractions)=1.0.
l  The Accumulated Energy is the sum of Energy Fractions up to and including the current
mode.
l  The Residual Energy equals 1.0 - Accumulated Energy and describes the amount of
energy NOT covered by modes 1..N.
From each of the Chronos you can furthermore derive temporal Lag-1 AutoCorrelations,
quantifying the degree of similarity between temporal neighbors:
This somewhat unusual formula is in fact the intraclass correlation exploiting the knowledge
that both values in each pair come from one and the same distribution and has a mean value
of zero.
The Lag-1 AutoCorrelation of a Chronos quantifies the degree of similarity (coherence)
between consecutive Cronos-values. For the DC-mode, where all Chronos-values are equal,
we get a Lag-1 AutoCorrelation of 1.0, while temporally well-resolved modes will produces
AC1-values that are "large", but smaller than one. If the Chronos evaluated is a pure, noise-
free sine or cosine, the AC1-value equals the cosine of the phase angle between consecutive
samples. If for example there is 8 samples/cycle the phase angle between consecutive
samples will be 360/8=45 degrees and the Lag1-AutoCorrelation will become AC1=cos(45)-
)=0.71. At the Nyquist limit of 2 samples/cycle there will be 360/2=180 degrees between con-
secutive samples and the Lag-1 AutoCorrelation will become AC1=cos(180)=-1. This is very
unlikely in practice, but negative AC1-values are not unusual among the higher order modes.
In a temporally well resolved experiment the first few modes should however produce AC1-
values in the range 0.5-1.0.
High and low Lag-1 AutoCorrelations can for example originate from a set of Topos and Chro-
nos such as these:
PAGE | 734
PAGE | 735
Top: Topos. Bottom: Chronos. Left: High AC1-values. Right: Low AC1-values.
The last parameter you can plot as a function of Mode number is the Kurtosis, which is derived
from the Chronos of each mode. The Kurtosis is the 4th order standardized moment of the
Chronos:
The Kurtosis can tell us if the Chronos-values have reasonably constant magnitude over time
or if they are mostly very small with a few large exceptions.
If Chronos is constant over time (as in the DC-Mode for example), Kurtosis will become 1.0.
If Chronos describe a clean, noise-free sine-wave, Kurtosis will become 1.5.
If Chronos describe white noise with Gaussian distribution, Kurtosis will be around 3.0.
Some textbooks subtract 3 from the formula above to get 'Excess Kurtosis'; -This will be zero
for a normal distribution, negative for distributions that are more 'Peaked' than the normal
distribution and positive for distributions that are more 'Flat'.
Modes with Kurtosis-values significantly above 3 may be caused by and describe only (undetec-
ted) outliers in the input data. Such modes should of course be excluded from further analysis
and the Kurtosis is intended as a tool to aid the user in identifying them. Please note however
that being a fourth order moment, Kurtosis is a statistical quantity that may depend heavily on
the number of samples (i.e. the number of input datasets). In the figure above it appears for
example that Kurtosis-values in the range 2-4 is perfectly normal, so you may question
whether the single Kurtosis-value exceeding 5 is 'Significantly' bigger than 3. If in doubt
please take a look at the corresponding Chronos and/or Topos to see whether it contains
evenly distributed noise or a (few) dominant outlier(s).
Apart from graphical displays, the POD modes can of course be displayed numerically as well:
PAGE | 736
PAGE | 737
The first 10 columns are unique to the POD dataset, describing Chronos as a function of Time
as well as Amplitudes, Energies, Lag-1 AutoCorrelations and Kurtosis as a function of Mode
Number. The remaining columns describe the Topos in the same way normal image, scalar or
vector maps are displayed numerically.
If you browse through POD modes one by one you will see that apart from Topos, Chronos-
values are the only ones changing. The remaining parameters are common for all modes.
16.68.7 Input requirements
POD Analysis can in principle be applied to any image, scalar or vectormap ensemble, but
meaningful Topos require input that is spatially well resolved. Similarly meaningful Chronos
require input that is temporally well resolved. If the flow being measured contain frequencies
above half the sampling rate of the measuring system, aliasing is to be expected and will
affect POD results also.
16.68.8 Numeric Export of POD results
If you wish to visualize POD results outside of DynamicStudio or process them further you can
export results to text files using 'Numeric Export'.
This is done by first selecting the ensemble with the POD results in the database and then pick-
ing 'Export' in the File or Context menu:
Either way you reach the Numeric Export dialog:
PAGE | 738
PAGE | 739
At the top of the dialog you specify where to put the exported files, what to call them and
what index number should be assigned to the first one (it will increase by one for each file).
You also specify file type from a number of predefined options, in the example above Tecplot
has been chosen. (Read more about it here: "Numeric Export" on page?973).
The Numeric Export more or less makes a copy of the Numerical Display, which for POD res-
ults is a bit unusual:
As explained above the first 2 columns describe the Chronos for each mode, while the next 8
columns are common for all modes and remain the same as you browse through them, and
the last 12 columns are uniquely describing the spatial distribution of data in each Topos.
There is one row per timestep in the first two columns, one row per mode in the next 8 and
one row per data point in the last 12 columns. But there is no connection between the
timestep, the mode info and the data point that share a row. Think of the Numerical Display as
three separate tables shown side by side. Normally the number modes match the number of
timesteps, while the number of datapoints will typically be higher, so in the first 10 columns
you will typically get some empty space below the last timestep/mode, which by default will be
filled with zeros.
If you think of the POD results as three separate tables it makes sense to also export them
separately. With the checkboxes in the bottom left side of the export recipe you can choose
what data to include or exclude in the exported files. To export mode timesteps and cor-
responding Chronos values uncheck all but the first two:
To export the modal energy distribution and the like select only the next 8 modes:
-you can of course exclude e.g. Kurtosis, AC1-values or other parameters if you don't want
them.
To finally export the spatial distribution in the Topos check the columns among the last 12 that
you want:
PAGE | 740
PAGE | 741
In this example we've chosen to export only the metric locations and the vector components,
while excluding grid index positions as well as pixel positions and displacements.
This example was based on POD?analysis of a series of vector maps. If you perform POD ana-
lysis of e.g. Image or Scalar maps instead the Topos will of course change acordingly.
For all of these exports you can of course adjust the format such as number of decimals for
each of the parameters exported. Read more about formatting in "Numeric Export" on
page?973.
1
J. L. Lumley (1967):
"The structure of inhomogeneous turbulent flow".
-In A. M. Yaglom and V. I. Tatarski, editors:
"Atmospheric Turbulence and Radio Wave Propagation", pages 166-178.
2
Aubry, Guyonnet & Lima (1991):
"Spatiotemporal Analysis of Complex Signals: Theory and Applications"
"Journal of Statistical Physics", vol. 64, Nos. 2/3, 1991, pp.683-739.
3
Mendez, Balabane & Buchlin (2019):
"Multi-Scale Modal Analysis of Complex Fluid Flows"
"Journal of Fluid Mechanics", Volume 870, pp. 988-1036, 10 July 2019
4
Mendez, Watz, Hess & Buchlin (2020):
"Multiscale proper orthogonal decomposition (mPOD) of TR-PIV data
-a case study on stationary and transient cylinder wake flows".
"Measurement Science and Technology", Volume 31, Number 9, 25 June 2020
16.69 POD  Reconstruction
Proper Orthogonal Decomposition (see "Proper Orthogonal Decomposition (POD Analysis)"
on page?718) is often used for reduced order modeling since an excellent approximation to
the original datasets can be made from just a few of the modes. Since the spatial modes,
'Topos', are orthogonal to one another they can be used as a basis and approximate any data-
set with identical structure and content by projecting onto these modes. This is exactly what
POD?Reconstruction does; Given a set of POD?modes, reconstruct the parent dataset by pro-
jecting onto a selected subset of the modes. If the parent dataset went into the POD?Analysis
that generated the modes it can be reconstructed exactly by including all modes, but a reas-
onable approximation can be made if the parent dataset just originate from a similar exper-
iment and/or dominant POD?modes are included in the reconstruction.
Having subtracted the temporal mean, the fluctuating part of the original data can be recon-
structed from Amplitude (λ), Topos (ψ) and Chronos (φ) as follows:
... where ...
=  Global amplitude of the n'th mode
=  n'th Spatial mode, 'Topos'
=  n'th Temporal mode, 'Chronos'
In fact the mean is included as Mode 0, so summing from n=0 to n=N will reconstruct the ori-
ginal data completely.
Using the POD?modes as a basis an arbitrary signal is projected onto each Topos using a simple
dot product between the signal and the Topos:
-with i=1..M summing over all measured values in all positions covered (for vector maps this
means summing over all u- and v-velocities from all vectors in the vector map).
Resulting scalar value an represents the contribution from Topos ψn in the approximation of
u. Summing up multiple such contributions u is approximated better and better.
Please note that projecting onto Mode 0 would be incorrect as the mean is not orthogonal to
all the other modes even if it is stored as Mode 0. Instead the mean is subtracted first, pro-
jections done on the remainder and the mean then added again if requested.
Please note: In the examples shown here POD?Analysis and subsequent POD?Reconstruction
is done on Vector Maps, but both can be done on Images or Scalar Maps also in exactly the
same manner.
In this example the parent dataset is among the ones used to generate the POD Modes:
POD Reconstruction is found in the category 'Vector & Derivatives':
PAGE | 742
PAGE | 743
The only choice to make in POD?Reconstruction of a parent dataset is which of the POD?modes
to include and which to exclude in the reconstruction. The recipe offers two different ways of
doing this; 'Energy Fraction' or 'Selected Modes':
The last one, 'Selected modes', is the simplest;?You specify directly which modes you wish to
include in the reconstruction. The syntax corresponds to the one used in a printer-dialog,
where you can specify which pages from a document you wish to print. You can specify indi-
vidual modes numerically separated by commas and/or spaces. You can also specify a range of
modes by putting a dash between the first and the last of the mode numbers in the sequence.
And you can combine these two, so for example '1-4, 6-7' would reconstruct using modes 1-4
and 6-7 (i.e. 1-7 except 5).
The other option, 'Energy Fraction' is semiautomatic, including all modes from 0 or 1 up to the
mode where the accumulated energy of the modes included reach or exceed the desired frac-
tion of total energy summed over all modes. The checkbox 'Include Mean' determine
whether or not to include Mode 0 (the mean) in the energy based reconstruction.
Please note that energy based mode selection rely on the modal energy distribution from the
POD?Analysis and thus from the ancestor data that went into the POD?Analysis originally. The
'Energy' of the parent dataset plays no role here, but that should be OK as long as the parent
dataset originate from an experiment that is 'similar' to the one from which the POD?Modes
were derived. If it does not POD Reconstruction makes little sense anyway, but Dynam-
icStudio has no way of knowing, so it is the user's own responsibility to choose suitable POD
Modes when using POD?Reconstruction.
The figure below shows a POD mode energy distribution from POD?Analysis, both for indi-
vidual modes (blue) and accumulated (red). In this example modes 1 & 2 are comparable and
combine to cover more than 70% of the total energy. Modes 3 & 4 are weaker, but still rel-
evant and including these the accumulated energy exceed 90%. Energy based reconstruction
with the default 90% energy fraction would thus include up to mode 4 in this case. From the
blue curve you might expect something relevant from mode 5 & 6 even if they contribute
very little, but modes 7 and up probably has little relevance. (The first 25 of 249 modes are
shown below).
Figures below show an example of Parent data and corresponding POD Reconstruction with
the default target energy fraction of 90% (i.e. modes 0-4 in this case):
PAGE | 744
PAGE | 745
As you can see the Reconstruction resembles the Parent vector map so much that you can
barely tell the difference. Vorticity is shown as a scalar map behind the vectors to visualize that
the two vector maps are not identical; The reconstruction is a bit smoother than the parent.
You can also reconstruct from specific modes. Including only Mode 0 is a (complicated) way of
getting the temporal mean. Neighboring modes of almost the same energy will often rep-
resent 90° phase shifted versions of the same traveling vortices, so in the example above
modes 1 & 2 are candidates for this as is mode 3-4 and even 5-6:
16.70 Pressure  from  PIV
Pressure from PIV analysis method estimates scalar pressure fields from given PIV velocity
vector fields. The analysis is performed by solving a Poisson equation for the pressure, con-
structed from explicit velocity-based high-order derivatives from the PIV field, along with com-
patible boundary conditions for the pressure field given by the user.
Two Analysis methods are involved to compute pressure from PIV, to be used in the following
order;
1.   "Define Objects" on the facing page (optional)
2.   "Pressure from PIV" on page?749
Both are found in the Analysis method Category 'Vectors & Derivatives' when analyzing an
image or vector map respectively:
PAGE | 746
PAGE | 747
Please note:
The current version of Pressure from PIV assumes that the flow is 2-dimensional, incom-
pressible and at constant-density (i.e., divergence free flows).
Violating these assumptions might lead to incorrect and non physical results.
16.70.1 Define Objects
Estimating pressure distribution from velocity fields relies heavily on boundary conditions that
need to be specified. Boundary conditions at the edges of the vector maps are specified from
within the Pressure computation recipe, but if there are walls or other physical objects in the
vector field 'Define Objects' must be used beforehand to define their location and shape. This
includes walls at or near the edge of parent images.
The analysis method 'Define Objects' takes an image as parent, but a vector map can be
chosen as a secondary input also. If present the vector map will be overlaid on the parent
image in the recipe, but is not used for anything, it is merely an optional visual aid in defining
walls and objects.
The parent image is always shown inside the Boundary Conditions recipe and initially there are
no Objects. With subsequent use the recipe will remember objects from before and show
them again.
You can zoom in and out by using the mouse scroll wheel while pressing Ctrl on the keyboard.
You pan similarly by pressing Ctrl and then click and drag with the mouse.
To add an object click the button 'Add Object' in the top let corner of the recipe. This will cre-
ate a small triangular object , which you can then manipulate to describe walls and objects in
the experiment at hand. If necessary you can add several such objects.
For the triangle and any other object you can click and drag with the mouse to move corner
points or entire segments of the polygon. By right-clicking a line segment you can delete an
entire object or insert extra points and turn the triangle into a polygon.
In the example above two objects have been inserted;
The one that defines the top wall remains triangular with one point moved outside the field of
view.
The object in the top left corner defines a rectangular obstacle and has had an extra point
inserted.
For each segment of the polygon the (right-click) context menu also give access to 'Prop-
erties', that specify whether the segment should be a 'Spline' or a 'Straight' (line)':
The default is 'Spline', but in the example above walls inside the field of view have been
changed to 'Straight', while the ones outside have been left as 'Spline'. Being outside the field
of view they will be ignored later and have no influence on the computed pressures.
Splines can be useful to describe curved surfaces that vary smoothly as shown in the example
below:
PAGE | 748
PAGE | 749
The object above is defined by just 6 control points of which all but one are in fact outside the
field of view. In this case object segments inside the field of view remain splines, while the
ones outside have been changed to straight (as explained above it doesn't really matter since
they are outside so we can ignore them). The two points close to the right hand and top edge
of the image control that the spline intersects image boundaries at the right place, while the
rightmost and topmost points control the slope of the spline at the edge of the field of view.
Please note that they're outside the field of view so their exact location matters little and
you're thus free to use them to control the slope of the spline where it enters or leaves the
field of view. The last point in the top right ensures simply that the object created covers the
top right corner of the field of view.
Whether segments are splines or straight lines all objects are designed to remain closed con-
tours, but as shown above they may extend beyond the edge of the parent image. By design
neighboring spline segments will furthermore share a common slope at the control point
where they meet, ensuring a smooth object shape with no jumps or sharp corners. If sharp
corners are needed straight line segments must be used, possibly in combination with splines
as shown in the wing profile below:
The leading edge in the left hand side is defined by splines, while the trailing edge in the right
hand side is defined by straight lines. Where a spline meets a straight line discontinuities in
slope must be expected, it is up to the user to define the object in such a way that these dis-
continuities are minimized.
16.70.2 Pressure from  PIV
A 2D Vector Map is the parent dataset, but an Object Definition dataset can be used as a sec-
ondary input to identify the location and shape of walls and objects in the field of view.
The output will be a scalar map with the pressure field.
In the recipe you must specify the physical properties of the fluid as well as boundary con-
ditions along the left, right, top and bottom edges of the Field of View. The method construct
a Poisson equation for the pressure by applying the divergence operator to the Navier-
Stokes equations, which needs compatible boundary conditions for the pressure field to solve
the system.
You can manually fill in Density and Kinematic viscosity for any fluid or you can press the short-
cut buttons to have them filled in with properties for Air or Water at a temperature of 290K
and (average) pressure of 1bar = 105 N/m2. Liquids such as water are normally incom-
pressible, but for air and other gases density changes with pressure. For flow velocities well
below Mach 1 pressure fluctuations are however so small that density can be considered con-
stant for all practical purposes.
For each of the four boundaries there are conditions for the pressure that can be specified
independently:
PAGE | 750
PAGE | 751
Boundary  conditions  for pressure
Zero Gradient         Pressure gradients normal to this boundary is or will be zero.
It should be applied in the far field inlet flows where the inlet velocity is
known.
Fixed                         For this boundary condition the user must specify what the pressure at
the boundary is.
A uniform value will be applied to the complete boundary.
Free Stream            This boundary condition provides a free-stream condition for pressure.
Where fluid enters the domain it will apply a zero gradient condition and
where fluid leaves the domain it will use a nominal pressure value sup-
plied by the user.
This boundary condition is the most universal and it switches auto-
matically between "Zero Gradient" and "Fixed" as a function of the flow
direction.
'Zero Gradient' is the only boundary condition for which the user need not specify a nominal
value.
Boundary  conditions  for walls  and objects
If a secondary input identify walls and/or objects in the flow-field they will override the gen-
eral boundary conditions in the recipe above. For walls and objects boundary conditions are
Zero Gradient for the pressure and zero for the velocity (No-Slip condition). If there are for
example walls at both top and bottom it does not matter what kind of boundary conditions
you specify there. If an object just partially covers the edge of the field boundary conditions
from the recipe will apply to the part of the edge not covered by the object.
In the example below there is a wall along the entire top of the field, so here boundary con-
ditions will be 'wall' no matter what the recipe says. Furthermore an object in the top left
corner partially overlap the left boundary of the flowfield, so here boundary conditions spe-
cified in the recipe apply only to the lower half:
16.70.3 Solver
The pressure fields are calculated by solving a Poisson equation, constructed from explicit
source terms in the RHS  which are only function of the velocity vector field.
The Poisson equation is constructed and solved using a standard OpenFOAMR solver, which
presents two main variations depending on the type of parent data:
1.   If the parent data is a time-series of velocity vector fields, like PIV snapshot data, then the
solver treats the case as a transient calculation. This assumes/requires that the input is
time-resolved so meaningful temporal derivatives can be derived from consecutive data-
sets.
2.   If the parent data is a vector statistics output, being this from a time-series of PIV data,
then the solver treats the case as a steady-state calculation.
In general, the following form of the Navier-Stokes equations must be satisfied:
l  Momentum conservation:
;
l  Mass conservation:
;
l  Viscous stress tensor:
;
where ρ is the density, τ the viscous stress tensor, ν the kinematic viscosity, u the velocity field
and p the pressure field.
Under constant density and viscosity conditions, every term in this equations is a function of
the velocity. Therefore, if adequate boundary conditions for the pressure field are provided,
then an explicit solution can be obtained.
PAGE | 752
PAGE | 753
Transient solver
This solver takes as an input the time-dependent Navier-Stokes equations in an incom-
pressible, constant-density form. Applying the divergence operator and rearranging the
terms, the Poisson equation for the pressure P for this case reads:
,
where u is the velocity vector field and τ the viscous stress tensor and P is the kinematic pres-
sure (P=p/ρ).
Steady-state solver
This solver takes as an input the time-averaged Navier-Stokes equations in an incompressible,
constant-density form. Applying the divergence operator and rearranging the terms, the Pois-
son equation for the pressure P for this case reads:
,
where u is the velocity vector field and τ the viscous stress tensor and R is the calculated Reyn-
olds stresses.
The solution to this Poisson equation is achieved by using the standard Geometric Algebraic
Multi-Grid (GAMG) solver in OpenFOAM. The tolerances and number of correction steps for
this solver are accessible in the 'Advanced' sub-menu of the PfP analysis method:
The solver is applied N (Number of corrections) times, where for each of the correction steps
the number of iterations is controlled by the tolerances as follows:
l  Tolerance: It is the absolute tolerance limit for the global iteration steps. It is enforced at
the end of the number of corrections. The calculation is considered converged after this
limit is reached.
l  Relative tolerance: It is the relative tolerance limit for the intermediate iteration steps. The
calculation is considered converged after this limit is reached in an intermediate iteration
step.
Setting the tolerances very low will allow for more accurate estimations but require longer
processing time. If the solver stops at the maximum number of iterations results may be less
accurate, but it will allow the solver to finish within a reasonable time. The user is adviced that
the calculated results my be dependent on the convergence as a function of these tolerances
limits.
16.70.4 Important considerations
On the solver applicability
Although the solver is built to be very robust, the user must exercise caution when inter-
preting the calculated pressure fields results:
1.   The Poisson equation for the pressure depends strongly on the boundary conditions
provided by the user. These conditions must have physical meaning and be compatible
with the experiment from where the PIV data is obtained.
2.   The Poisson equation for the pressure is constructed in a conservative form, meaning
that variable-density and/or velocity divergence can have an influence on the results. This
problem arises when trying to interpret a 2D PIV velocity field as 2D-planar. It is there-
fore not possible to correctly infer a 2D pressure field from a 2D slice of velocities in a
strongly 3-Dimensional flow.
On custom settings  for advanced users
If the user has prior knowledge of OpenFOAM usage and/or programming, the pressure
solver can be openly customized and tweaked for advanced cases. The following points detail
the location of the source files in the Dynamic Studio installation and how to access the
OpenFOAM executable, templates and raw solution files.
1.   OpenFOAM template files are located in the Dynamic Studio installation folder, usually
'C:\Program Files\Dantec Dynamics\DynamicStudio 7\Misc'. The templates control the com-
plete flow of calculation when using the solver: from the mesh generation, adaptive snap-
ping to objects and surfaces, interpolation schemes, solution control and variable
generation.
2.   OpenFOAM solution and case files are generated for each calculation in a temporal folder.
The user can type '%temp%' in the windows explorer address bar and look for a folder
named '_#ds_openfoam_'. For convenience, the output calculation files are written in plain
ASCII files readable with any text editor software.
OpenFOAMR is a registered trade mark of OpenCFD Limited, producer and distributor of the
OpenFOAMR software via www.openfoam.com.
“This offering is not approved or endorsed by OpenCFD Limited, producer and distributor of the
OpenFOAM software via www.openfoam.com, and owner of the OPENFOAMR and OpenCFDR
trade marks.”
PAGE | 754
PAGE | 755
16.71 Probability Distribution
This method is used to calculate statistics and related histogram or probability density function
inside a region of interest (ROI) and whole field single record. This record can be a calibrated
LIF image (e.g. concentration or temperature image), a filtered calibrated image (i.e. a LIF
image processed using the numerical routines available in the Image Processing Library) or
statistical dataset (i.e. Mean pixel or RMS pixel).
Content:
l  Define and apply a mask
l  Distribution processing
l  More about ROI data analysis
16.71.1 Define and apply a  mask
For detailed information on mask definition and image masking, please refer to the '    Define
mask' and '     masking Image map' help sessions.
16.71.2 Distribution  processing
The method is located in the category 'LIF Signal'. Select the dataset of interest and run the
numerical method 'Distribution processing'. Complete the dialog window that appears as fol-
lows:
1.   Check the 'Use mask' box if a mask need to be used and select it using the 'Select' button
2.   Select the type of distribution to calculate; i.e. Counts (or histogram of values) or Prob-
ability
3.   Set the scaling parameters by defining the 'Bins' values (typically between 25 and 100) and
if necessary the lower/upper limits of data binning.
4.   Press the 'Apply' button to previous the results and then 'OK' to accept it. The data are
stored in the database and easily located with the       icon.
<Distribution processing> dialog window
Using Image ensemble as  mask input
Instead of using a Defined Mask as input, it is possible to select an image ensemble as input.
The Image ensemble most hold the same amount of images as there are datasets in the par-
ent ensemble.
The pixel values of the images in the ensemble indicates if the pixel is masked, here a value of
0 indicates that the pixel is masked. Any other value will indicate not masked.
Using methods like Image Math, Image Arithmetic and Image process Library you can create
any series of mask images that can be used as mask input for the method.
16.71.3   More about ROI data  analysis
In the database, open the 'Distribution' record and right click with the mouse on it to access
the 'Display' options. Click on the 'Display options...' to specify the type of data to graph and
then select the 'Chart type', 'Scatters' and X-/Y-axis scaling options to adjust the view.
PAGE | 756
PAGE | 757
Data display is quickly adjusted with a few mouse clicks.
To further view the raw data in an Excel-like sheet, press the      icon and extract the data of
interest using the 'Copy to clip board' and 'Export as file…' options.
16.72 Profile  plot
The profile plot method allows the user to draw an arbitrary line across an image map, a
scalar map or a vector map, and extract the values along this line to a profile plot.
l  Detailed description
l  A handy shortcut
l  Numerical Values
l  Examples
16.72.1 Detailed description
l  Select an input dataset (Parent dataset)
l  Right click and select "Analyze"
l  On the Analysis Methods tab, pick "Plots" from the list of categories
l  Pick "Profile Plot" from the list of applicable methods
l  The profile plot recipe dialog is displayed.
l  On the "Positions" part of the display, choose start and end point of the line profile to be
extracted. You may directly enter the pixel coordinates into the respective number field
for "Position 1" and "Position 2". Values are separated by comma. You may also activate
"Position 1" or "Position 2" (radio buttons) and use the sliders for X-position and Y-position.
A check mark in the "Snap to vectors" button will lock the target positions to the center
coordinates of the nearest vector (interrogation area).
l  Several lines can be extracted at the same time. Press Add or Remove buttons.
PAGE | 758
PAGE | 759
l  The parent dataset can be displayed and display options can be chosen. A line on the par-
ent dataset indicates where the line profile is extracted. Extracted values are NOT inter-
polated. In images and scalar maps, the pixel closest to the line is extracted. In vector
maps the value from the interrogation area closest to the line is extracted. Small squares
frame the interrogation areas from which values are extracted.
l  Click "Apply" to preview the plot.
l  Click "OK" to generate the plot. Note that all the variables present in the parent dataset
will be extracted along the profile.
l  Double-click on the plot window, or select "Display Options" from the context menu. The
display options dialog is also accessible from the profile plot recipe dialog by pressing the
"Display" button.
l  Any variable can be chosen for x or y-axis. Only one variable at a time can be chosen for x-
axis and multiple variables can be chosen to be plotted against the x-variable.
Right click in the Data column to have the option to Select or Exclude all.
PAGE | 760
PAGE | 761
Note: The way this method is implemented means that you have to re-calculate the result if
the Scale factor or Time between pulses are changed.
16.72.2 A handy shortcut
You may find the following, alternative way for generating a profile plot more handy.
l  Open the input data set (parent data set)
l  In the parent data set draw a line by:
SHIFT-click on the start point of the line,
Hold the mouse button down while moving to the end point of the line
Release the mouse button at the end point of the line
l  Press CTRL-A (with focus still on the parent dataset display window) to open the "Anlalysis"
dialog
l  Pick "Plots" from the list of categories
l  Pick "Profile Plot" from the list of applicable methods
l  Click "Apply" to preview the plot
l  Make changes the plot parameters as desired
l  Click "OK" to generate the plot
16.72.3 Obtaining the numerical values
You can easily display the numerical values of the profile plot:
l  Select the corresponding profile plot dataset in the database window
l  In the "File" menu choose "Open as numeric", or
l  Press CTRL-U, or
l  On the standard toolbar choose click the icon "display as numerical" to display the numer-
ical values in a worksheet.
To use the data directly in another worksheet application simply select, copy and paste the
data.
To export the data to a file:
l  Select the corresponding profile plot data set in the database window
l  In the "File" menu choose "Export"
PAGE | 762
PAGE | 763
16.72.4 Examples
Example 1:
Shear flow phenomenon in a microfluidic Y-coupler. The concentration map shown on the left
is of the type "image map". A vertical cut along the blue line gives the concentration profile
(shown to the right) along that line, here plotted with a logarithmic y-axis to better reveal the
structures at low concentration levels. The coordinates of the start and end points are indic-
ated in the title of the profile plot.
Example 2:
Vortex pair in a water flow. The vorticity plot shown to the left is of the type "scalar map". To
the right a plot of the line profile of the vorticity along the black line.
16.73 Range  Validation
Any vector map can be validated against a user defined expected range of velocities. This is
done by selecting the ensemble containing the vector maps in question and then select the
analysis method 'Range Validation' in the category 'PIV?Signal'. In the recipe you can specify
acceptable vector length as well as min and max values for each of the velocity components of
the vector map:
PAGE | 764
PAGE | 765
-Apart from the in-plane velocity components U &?V?Stereo PIV?analysis will also estimate the
out-of-plane velocity component W and for such a dataset you can of course specify upper and
lower limits for acceptable W-values as well. Suitable limit values are often determined best by
simple trial and error. Try various values, press 'Apply' to see how they affect the vector maps
and when you're satisfied press 'OK' to validate the remaining vector maps in the parent
ensemble.
Here's an example of a vector map that has been validated with the settings above: Inval-
idated vectors are color coded in red, while the rest remain blue to indicate that they are
deemed valid vectors:
Please note:
Range Validation does NOT substitute invalid vectors with an estimated guess for the correct
velocity. To do this you need to apply yet another validation method such as Moving Average
Validation.
16.74 Rayleigh  Thermometry
Rayleigh thermometry strives to determine temperature distribution in e.g. flames and rely
on Rayleigh scattering from the molecules in a gas. Rayleigh scattering is orders of magnitude
weaker than e.g. Mie scattering and generally require an intensified camera to be detected at
all.
16.74.1 Rayleigh  theory
Rayleigh theory describes the scattering of light (or other electromagnetic waves) from
particles significantly smaller than the wavelength. A detailed description of the theory is bey-
ond the scope of this text and will not be covered.
Rayleigh Thermometry depends on light-scattering from the molecules of a gas, where the
number density (molecules/m3) is the governing factor: In general a gas will expand with
increasing temperature, meaning that the number density and thus the intensity of scattered
light will decrease. Measuring the intensity of scattered light at a known temperature and
monitoring how that intensity changes allows us to determine corresponding changes in tem-
perature. The scattering intensity is of course affected by many things other than tem-
perature (e.g. laser wavelength, scattering angle and more) and Rayleigh Thermometry is
generally applied assuming or requiring most of these things to be constant.
The following expression may be used to describe the response to some parameters, which
cannot always be assumed constant:
-where subscript ‘Measure’ refers to the measurement image and ‘Reference’ to a reference
image where the temperature, TReference, is known as is all other parameters that are
expected to change. The various symbols used are: ...
Symbol   Meaning                                                              Unit              Comment
I         (Greyscale) Intensity                                  [ - ]             The “raw” measurement for all pixels.
T        (Absolute) Temperature                       [ Kelvin ]         The intended result for all pixels.
σ        Scattering Cross Section
(describes the scattering effi-
ciency of different gases and
gas mixtures)
[ - ] or [a.u.],
but consistent
Typically relative to N2.
May vary from reference to actual meas-
urement, but remain constant within
each image.
p        (Absolute) Pressure                          [ Pa ] or [a.u.],
but consistent
May vary from reference to actual meas-
urement, but remain constant within
each image.
E        (Laser pulse) Energy                         [ mJ ] or [a.u.],
but consistent
Spatial distribution may vary, but being
unable to detect this we can only com-
pensate for shot to shot variations in
total pulse energy.
PAGE | 766
PAGE | 767
Please note that all parameters except temperature enter as ratios so you need not use any
specific unit as long as you are consistent. Parameters that remain constant need not be
known at all since the ratio will just be 1.
If all but temperature remains constant Rayleigh Thermometry will in principle boil down to
the following very simple formula:
In practice both reference and measurement image will contain gray-value contributions
from other sources than Rayleigh scattering and these may severely disturb results if
ignored.
The two main error sources are Background noise (dark current etc) & Mie scattering from
dust particles and similar contaminants in the measuring area (typically much stronger than
the Rayleigh scattering since the dust particles are much, much bigger than the molecules and
thus scatter much more of the incoming laser light).
Assuming the Background Noise can somehow be measured it can be subtracted and a mod-
ified Rayleigh expression becomes:
The Rayleigh Thermometry recipe supports only background subtraction, while Mie scattering
may be removed using existing image processing techniques in DynamicStudio prior to enter-
ing the Rayleigh Thermometry recipe.?
16.74.2 Rayleigh  Thermometry analysis in  DynamicStudio
The reference image should be stored in the DynamicStudio database as a Calibration image
and be selected for analysis before moving to the ensemble containing measurement images
and from there enter the Rayleigh Thermometry recipe:
-In this example preprocessing has been applied to a series of reference images in order to
get a single reference image. You may also select an ensemble with multiple images in which
case DynamicStudio will compute the mean of all these images and use that as the reference
image.
The simplest Rayleigh calculation is applied by setting all but the temperature to 'Constant' and
attempting no background subtraction:
PAGE | 768
PAGE | 769
Top: Reference image with constant temperature of 300 K.
Bottom: Measurement image with higher flame temperature and particle images in the sur-
roundings.
Top: "Raw" result of the Rayleigh analysis produce extreme temperatures due to Mie scat-
tering from the particles.
Bottom: Rayleigh results clamped to the range 300 K - 1200 K to reduce the effect of Mie scat-
tering while keeping most of the information inside the flame.
Rayleigh analysis  in Region of Interest
The recipe allows you to perform the Rayleigh analysis in a specified region of interest instead
of always processing the entire image:
PAGE | 770
PAGE | 771
To process a subset of the full image select 'Region of interest' and click the button 'Select
ROI...', which becomes active. In the resulting dialog identify the region in which you wish to
perform Rayleigh analysis.
Selecting Frame 1  or 2  from a Double-Frame image
If the input images are double-frame you may also specify whether to process Frame 1 or 2:
-Please note that it is highly unusual and not recommended to use double-frame imaging for
Rayleigh thermometry; Double-frame imaging is normally used in order to investigate the
dynamics of rapidly changing phenomena and the image intensifier required to detect
Rayleigh scattering will typically not be able to support the very short time between the two
images in a double-frame.
Temperature, Scattering cross-section, pressure & Laser pulse energy
The central part of the Rayleigh Thermometry recipe is the most important part where you
set up the analysis:
A reference image has to be pre-selected before you can even enter the recipe. If more than
one image was pre-selected a drop-down button is available allowing you to specify which of
the pre-selected images to use as reference image.
The temperature of the reference image must be specified in Kelvin and is the only man-
datory parameter in Rayleigh thermometry.
By default Scattering cross-section, Pressure and Laser pulse energy will be assumed con-
stant, meaning that they are assumed to be the same in both reference and measurement
image.
If these parameters are not constant you may simply type in a value for the measurement
image. The moment you do so the corresponding parameter will be enabled for the ref-
erence image, allowing you to type in a value there also. Please note that these three
parameters are in arbitrary units since the important thing is the ratios between meas-
urement and reference values, not the values as such. If for example you know that the gas in
the measurement image scatters 5% more light than the gas used when acquiring the ref-
erence image, you may simply enter scattering cross-sections as 1.05 and 1.00 for meas-
urement and reference image respectively.
If you have already added some of the parameters as Custom Properties of the selected
image ensembles there is an optional, and quicker way of filling in the information in the
Rayleigh Thermometry recipe. Simply make sure that the box ‘Use custom properties’ is
checked, and the software will automatically retrieve all the information that you have pre-
viously entered. This option is selected by default. If you need to make adjustmens on some
of these parameters when you do the analysis you can un-check the box again, and you will
have full rights to edit all the values used for the analysis. Should any information be missing,
you are of course also allowed to fill in the final details in the recipe manually to make the table
complete.
For the Scattering Cross-Section you may also look up values from a library of known gas spe-
cies and mixtures by clicking the drop-down box at the top of each column:
Just pick the gas species or mixture you're using and the corresponding scattering cross sec-
tion will be filled in accordingly. Do the same for the reference image, specifying the scat-
tering cross section, either by typing it in directly or by identifying the gas or mixture used.
If the gas species or mixture you're using is not in the list you may type in the scattering cross
section manually or you may enter the library of known gas species and mixtures to create an
entry describing name and properties of your gas. To enter the Species and Mixture Library
press the button labeled '...' in the left hand side of the Rayleigh recipe.
For the Laser pulse energy you may of course choose 'Constant' or enter a nominal value, but
you have two more options, Region Of Interest or Analog Input:
The ROI (Region Of Interest) selection can be used if there is an area within the cameras field
of view in which you know what the temperature is. When you've selected ROI in the drop-
down box the button labeled 'Select ROI...' will be enabled allowing you to identify the region
PAGE | 772
PAGE | 773
in which the temperature is known and constant. The temperature must be entered in the
box to the right of the button. Within the ROI average greyscale values are computed for
both measurement and reference image. Combining these with specified temperatures (and
other parameters) provides an estimate of laser pulse energy which is then used for Rayleigh
analysis in the entire image.
The analog input option (named 'Input #1' in the example above) will be available only if ana-
log waveforms have been preselected, matching the measurement and reference images
respectively. The analog waveforms are assumed to come from a Pulse Energy monitor,
mounted on the laser to measure the energy of each and every laser pulse. In practice the
waveforms are lowpass filtered and the maximum value of the filtered signal is assumed pro-
portional to the laser energy. Comparing analog waveforms for the reference and meas-
urement image we can thus compensate for shot-to-shot variations in the laser pulse energy.
Noise suppresion
Background noise is an important error source in Rayleigh thermometry analysis and remov-
ing or just reducing it may improve the quality of results significantly. DynamicStudio offers
three different methods for background subtraction, all accessible via the group 'Noise sup-
pression':
Apart from 'Not used' you may enter a fixed greyscale value, which will then be subtracted
from all pixels in both reference and measurement image.
You may also choose 'ROI' which will allow you to identify a region of interest, where the cam-
era sees only background and no laser light. The average of all pixels within this ROI will be
computed and subtracted from all pixels in both reference and measurement image.
If a separate background image has been recorded (e.g. with the lens cap on) and stored as
calibration image you may preselect this along with the temperature reference and then pick
the option 'Use image' to subtract background image from both reference and measurement
image. This is the only way to subtract varying greyscale values from pixels in different parts
of the image. Selecting 'Use image' will also enable the drop down list to the right, allowing
you to specify which among multiple images is to be used as background image.
Output temperature limits
A "raw" Rayleigh thermometry analysis can easily generate very high or very low tem-
peratures that are not physical. Mie scattering from particles may for example appear as
bright spots in the image, which Rayleigh analysis will interpret as extremely cold areas. Trun-
cating results to be within physical limits is useful to prevent such extreme values from dis-
turbing the final result. Switching on 'Clamp to limits' will allow you to specify both upper and
lower limits at which results will be truncated so no temperatures will be accepted outside the
specified range:
16.74.3 Species and Mixture Library
DynamicStudio contains a library of known gas species and mixtures with physical properties
such as scattering cross section. A number of predefined gas species and mixtures exist,
which you cannot change, but you can add your own species and mixtures and edit those
freely.
The Species and Mixture Library can be entered from the recipe of Rayleigh Thermometry (or
other analysis methods relying on data from the library) or you can access it directly from
DynamicStudio's Tools menu:
When you first enter the Species and Mixture Library you will see the list of known species and
their scattering cross sections:
If you click the Tab labeled 'Species mixture' you will switch to the library of known gas mix-
tures:
PAGE | 774
PAGE | 775
In the Species library you can see the list of preset species available and their corresponding
scattering cross-sections (for light at 532 nm), relative to that of gaseous nitrogen, N2.
You can add new species to the library by typing the name of the new species in the left hand
side of a new line at the bottom of the list and then type in the scattering cross-section (rel-
ative to that of N2) in the right column. All species in the library are easily accessible from e.g.
the Mixtures library.
In the Mixtures library you can see the available preset gas mixtures, what species they are
composed of and at what mole fractions. You can also create new mixtures and add to the lib-
rary. A mixture is created by clicking ‘New’ and typing a name on the Species Mixture Name
line. Then select the desired species under Mixture Component (these are the species found
in the Species library) and type the mole fraction of the species under Mole Fraction. The rel-
ative scattering cross-section of the gas mixture is updated online, and displayed in the top
right of the dialog. The preset mixtures cannot be changed by the user, but you can make a
copy of a preset mixture by clicking ‘Copy’. In the copy you can then make any changes you
like.
16.75 Reynolds  Flux
Reynolds Flux calculations are performed in two steps. First, instant Reynolds flux maps are
derived and then the user calculates the averaged map. Although no physical information is
contained in the instant Reynolds flux map, this feature is introduced to enable the user to
check convergence by calculating Reynolds flux maps e.g. with the first 250 data, then with
500 data etc.
Statistically 5,000 instant maps are needed to reach a 95% confidence interval. For such heavy
calculations, it is recommended to use distributed analysis (DA). By experience 1,000 instant
maps give a good picture of the Reynolds fluxes, but this depends entirely of the flow prop-
erties.
16.75.1 Image re-sampling
To calculate Reynolds flux, a PIV/LIF set-up is required; i.e. velocity maps and re-sampled
scalar maps. Re-sampling of the scalar map is necessary as this step describes the alignment of
two camera views. (Consult the 'Resampling methods' help for further information.)
16.75.2 Reynolds flux calculations
To calculate Reynolds flux, the average velocity and instant/average re-sampled scalar maps
shall be calculated first. Select all velocity maps (or part of them) and chose the 'Reynolds flux'
method. Complete the dialog window that pops up with the first scalar map and then aver-
aged maps. Click on the 'Apply' button to view the preliminary result and accept with 'OK'.
Multi-select the Reynolds flux maps (or part of them) and using the 'Mean method' in the stat-
istics category, calculate the Reynolds flux map.
16.76 Rigid  Object Tracking  and  Stabilization
16.76.1 Introduction
Image Stabilization is commonly used to remove or reduce ‘shake’ from handheld video cam-
eras, but can also be used to ‘freeze’ moving objects in the field of view of a PIV camera. This
can be used to switch from an Eulerian to a Lagrangian coordinate system if the object in ques-
tion is in fact moving along some trajectory or if it is simply ‘jumping about’ due to mechanical
vibrations (induced or due to aerodynamic forces).
Rigid Object?Stabilization is split into two methods
l  Rigid Object Tracking ? Tracking of the rigid object to get its position, velocity and accel-
eration.
l  Rigid Object Stabilization ? Stabilize the object such that it is in the same position in all the
images.
PAGE | 776
PAGE | 777
16.76.2 Rigid Object Tracking
The Rigid Object Tracking method is used to track a rigid object through an ensemble of
images. It determines the object's position in each of the images and also computes the
object's velocity and acceleration.
To use Rigid Object Tracking, right-click on the image ensemble and select Analyze to select the
method, and pick Rigid Object?Tracking. This will open the method recipe. An preview of the
first image in the image ensemble is shown on the right side of the recipe, along with a selec-
tion box. In the bottom is a slider to browse through the images in the image ensemble. Use
the slider to select an image with a good representation of the rigid object you wish to track
and then move and resize the selection box around the object as shown below.
You can also manually enter the position and size of the object on the left side of the recipe.
The Use preprocessing check box can be used to enable or disable preprocessing of the object.
If preprocessing is enabled, then two 15x15 median filters are applied to the object before
searching for the object in all the images. This is helpful to remove small details which are not
part of the object, such as particles and noise, which may improve tracking.
Manual preprocessing
If you wish to do your own preprocessing of the object before the tracking, this is possible by
preselecting an additional image ensemble containing a single image representing the object.
In the above example, the SpeedSense M310 ensemble contains the images to track the object
in and a ROI?Extract has been used on to extract the object from one of the images. The extrac-
ted object can then be preprocessed using any of the image processing methods in Dynam-
icStudio, such as Image?Processing Library (IPL). Finally, the preprocessed object is
preselected?and then Rigid Object Tracking is applied to the original ensemble, SpeedSense
M310. When this is done, no recipe is shown and the method directly runs since the object has
already been provided.
Note that the object ensemble must have only one image. To apply ROI Extract on only a
single image, right-click the image ensemble and select?Show Contents List, then right-click on
the image from the expanded list and select Analyze to apply ROI?Extract to only that image.
This will produce an image ensemble with only one image with the object.
16.76.3 Rigid Object Stabilization
The Rigid Object Stabilization method can be used to stabilize an object, such that it has the
same position in all the images in the output image ensemble.
Rigid Object Stabilization requires two input ensembles, an image ensemble and a matching
Rigid Object Tracking ensemble. To use Rigid Object Stabilization, first apply Rigid Object Track-
ing on the image ensemble to generate an ensemble with the object track. Then preselect
the?Rigid Object Tracking ensemble and right-click on the image ensemble, select Analyze and
pick?Rigid Object Stabilization.
This will open the Rigid Object Stabilization recipe, which looks quite similar to the tracking
recipe. An preview of the output is shown on the right side of the recipe, with a slider in the
bottom which can be used to browse through the different output images. In the output
images, the object has been shifted to the mean position to provide as much as possible valid
data around the object. Due to the shift of the images, the borders are replicated when no
valid data is available as shown below.
PAGE | 778
PAGE | 779
On the left side of the recipe, it is possible to enable an ROI?extraction which enables a selec-
tion box in the image preview. If ROI extraction is enabled, only the part inside of the selec-
tion box is outputted. If the check box Limit to common FoV is enabled, then the ROI?extraction
selection box is limited to the region where only valid data is available for all the images in the
ensemble.
16.77 Region  of Interest (ROI) Extract
ROI Extract method can be used to extract a region of an image. The region to extract is spe-
cified by a rectangle as shown in the screenshot below and the resulting image will have the
same height and width as the rectangle specified. The region is extracted using the specified
interpolation method, 'Nearest neighbor', 'Linear' or 'Cubic'.
The ROI rectangle can be manipulated either by mouse interaction or by text input in its prop-
erty dialog.
16.77.1 Manipulating the ROI rectangle using the mouse.
The position and size of the ROI rectangle can be changed by mouse interaction. The entire
ROI is moved by using the left mouse button to drag the ROI to its new location. When the ROI
is selected or while hovering the mouse over the ROI rectangle its manipulation handles is dis-
played as illustrated in the image below. By using the mouse to drag these manipulation
handles it is possible to change the ROI size. If the shift button is pressed while dragging a
corner manipulator the ROI size is changed symmetrically while keeping the center of the ROI
at its current location.
PAGE | 780
PAGE | 781
16.77.2 Setting the ROI rectangle using the property dialog.
Right clicking the ROI rectangle its context menu will appear (left image below). When select-
ing the ‘Properties…’ menu item the property dialog is shown (right image below):
In the property dialog entering the desired values of the position and size can alter the ROI
rectangle.
Hint: If the interpolation method is 'Nearest Neighbor' the ROI extract can be used as a simple
cropping tool.
16.77.3   Using the image view.
The view of the image and ROI rectangle can be controlled through the context menu that
appears when right clicking inside the image. In this popup menu the zoom level, the active
frame (for a double frame exposure) and visual appearance (Colormap and histogram) can be
adjusted.
The zoom can also be adjusted by scrolling the mouse wheel button or by dragging a rect-
angle around the desired area to view. If holding the control key while dragging inside the
image the view area can be moved (panned) around.
16.78 Scalar  Conversion
This method is used to transform experimental maps according to a user-defined polynomial
function and thereby gain additional empirical information on the process investigated. (Typ-
ical example is the transformation of [OH] density maps to Temperature maps estimates; see
related information in the Combustion-LIF application manual.) The order of this polynomial is
set by the operator and can vary between m = 0 and 5.
To use this method, select the re-sampled map(s) of interest and call the numerical method
'Scalar conversion' located in the 'LIF Signal' category. Complete the dialog window with the
value of the coefficients a0, a1, … amof the polynomial and press the 'Apply' / 'Display' buttons
to preview the result. Click on 'Ok' to accept the result and create a new      map in the data-
base.
Dialog window for user-defined (empirical) calculations derived from measurements.
PAGE | 782
PAGE | 783
16.79 Scalar  derivatives
This method comprises a number of different derivatives that can be calculated from a Vector
map. How the method calculates the different derivatives is displayed in the recipe. Below an
example is shown:
The first field in the recipe dialog is a drop-down list. The list shows all the possible derivatives
that can be calculated. It is possible to edit the text, in which case all the nearest possible deriv-
atives are listed. If f. ex. the user enters a “d” all the derivatives in the list starting with a d will
be shown.
Below the drop-down selection box a text box is shown. Here is a description of how the result
is calculated.
By checking the check-box "Calculate All available scalars", next to the drop-down selection, it
is possible to have all the available scalar values calculated and saved in to the resulting scalar
map. If this "Calculate All available scalars" is unchecked only the selected scalar value will be
calculated. Calculating only one scalar value can dramatically increase the performance of the
method.
When "Calculate All available scalars" is unchecked, the additional operation and Return oper-
ation becomes available.
Additional operation radio buttons determine what operation (if any) is applied after the cal-
culation. It is possible to negate the values or take the absolute value of the results.
The Return radio buttons select what post-processing (if any) to apply after the calculation has
been done. It is possible to have all values returned or just to keep positive or negative val-
ues.
16.79.1 Calculating the gradients of U,  V and W in  the x and y direction
Vectors in a vector map are in a discrete regular grid, so velocity gradients are estimated by
comparing neighbor vectors to one another.
Whenever possible a central difference scheme is used, but if only one valid neighbor vector
can be found a forward or backward difference scheme will be used instead (this will f.ex.
apply along the edges of the vector map, where only one neighbor will be present in the dir-
ection normal to the edge).
For example the gradient at the point (m,n) of velocity component U in the x-direction will be
calculated as follows:
This is a central difference scheme and the resulting gradient corresponds to the slope of a
2nd order polynomial (i.e. parabolic) fit to 3 neighbor velocities and is thus said to be 2nd
order accurate.
If only 2 valid neighbors can be found a forward or backward difference scheme will be used
instead:
Forward and backward difference schemes corresponds to the slope of 1st order polynomial
(i.e. linear) fits to 2 neighbor velocities and are thus said to be 1st order accurate.
If no valid neighbors can be found velocity gradients are set to zero, but tagged as invalid with
the status code 'Rejected'.
To suppress noise spatial averaging is applied also and the combined effect of a series of con-
volutions corresponds to a 5x5 gradient estimation kernel like this one:
-estimating a horizontal gradient from a 5x5 neighborhood of (valid) neighbor vectors. Ver-
tical gradients are computed with a similar kernel that is simply rotated. The same convolution
is applied to all velocity components to determine the various velocity gradients and if present
uncertainties are propagated through the various convolutions to provide uncertainties of
the estimated gradients as well.
The gradients can be collected in the 3x3 velocity gradient tensor J:
PAGE | 784
PAGE | 785
-from which several other scalar derivatives can be derived.
For planar vector maps spanning the x/y-plane, derivatives in the z-direction cannot be estim-
ated, so the rightmost column of J is commonly treated as zeros. Stereo PIV can estimate the
out of plane velocity component W, but classic PIV cannot in which case the bottom row also
become zeros.
16.79.2 Scalar derivatives that can  be calculated
dU/dx, dU/dy, dV/dx, dV/dy, dW/dx & dW/dy
Gradients of velocity components U, V &?W in the x- & y-direction:
Divergence
Divergence of a 3D vector field such as U is defined as
For planar data gradients in the z-direction cannot be calculated, so it reduces to
For incompressible flows (i.e. liquids or gases at velocities an order of magnitude below Mach
1) the divergence must be zero, since nonzero values indicate local changes in density.
Nonzero divergence values can thus help identifying erroneous measurements. For volu-
metric data nonzero divergence-values can otherwise be used to identify f.ex. shock-fronts
(across which density may change dramatically). For planar data nonzero divergence values
may be used to indicate areas where the flow is not 2-dimensional (i.e. contain significant out-
of-plane velocities).
Shear UV
The velocity shear tensor in a point is derived from velocity gradients:
-where each term describes the shear parallel to the yz-, zx- and xy-planes respectively.
For planar data gradients in the z-direction cannot be calculated, so only shear parallel to the
xy-plane can be determined:
Vorticity  (Z)
Vorticity in a point is defined as the local rotation or curl of the 3D velocity field:
-where each term describes the rotation around the x-, y- and z-axes respectively.
For planar data gradients in the z-direction cannot be calculated, so only rotation around the z-
axis can be determined:
Lambda-2
The Lambda-2 vortex criterion is based on the velocity gradient tensor J. For planar data input
gradients in the z-direction may be set to zero in the following calculations.
Split the gradient tensor in a symmetric and anti-symmetric part S & R, rate of Strain and rate
of Rotation tensors:
from this compute the eigen-values of the symmetric tensor S2+R2.
Since the tensor is real and symmetric all 3 eigen-values will be real and they can be sorted so
λ1?λ2?λ3. If the point under investigation is part of a vortex at least two of these eigen-val-
ues will be negative, corresponding to the Lambda-2 vortex criterion requiring simply λ2<0.
For 2-D data the gradient tensor J becomes a 2x2 matrix and it's smallest eigenvalue can be
computed as:
Local minima of negative-valued Lambda-2 can be used to identify vortex cores, while positive
values indicates areas of the flow, where shear may be present, but no swirling motion.
Reference:
Jeong & Hussain (1995)
“On the identification of a vortex”
J. Fluid Mech. (1995), vol. 285, pp. 69-94
Swirl Strength
Swirl strength is defined as the imaginary part of the complex eigenvalue of the velocity gradi-
ent tensor J.
For planar data gradients in the z-direction cannot be calculated, and setting them to zero sim-
plifies eigenvalue calculation, so the square of the imaginary part can be computed as:
-which is the figure returned as the swirl strength.
PAGE | 786
PAGE | 787
Please note that the corresponding eigenvalues will only be complex if this number is neg-
ative!
Local minima of negative-valued swirling strength can be used to identify vortex cores, while
positive values indicates areas of the flow, where shear may be present, but no swirling
motion.
Reference:
Adrian, Christensen & Liu (2000)
“Analysis and interpretation of instantaneous turbulent velocity fields”
Exp in Fluids, 29/3, p. 275-290
Q-Criterion
The 2nd invariant Q of the 3x3 velocity gradient matrix J may also be used to identify vortices.
The second invariant, Q, of this 3x3 matrix is:
In the immediate vicinity of a vortex Q will be positive and have a maximum at the vortex core.
For planar data gradients in the z-direction cannot be computed, and setting them to zero the
expression above simplifies to the determinant of the 2x2 gradient matrix:
-which will be the same for both 2D and 3D vector input.
Local maxima of positive Q can be used to identify vortex cores, while negative values indic-
ates areas of the flow, where shear may be present, but no swirling motion
References:
Hunt, Wray & Moin (1988)
“Eddies, stream, and convergence zones in turbulent flows.”
Center for Turbulence Research Report CTR-S88, p. 193
Chong, Perry & Cantwell (1990)
”A general classification of three-dimensional flow fields”
Phys. Fluids A 2, 765
16.79.3 Propagating uncertainties
If the parent vector map contain uncertainties for the displacements and velocities U, V & W,
those uncertainties are propagated through the calculation of scalar derivatives to estimate
uncertainties of those as well.
Uncertainty propagation is based on the variance formula that quantify the uncertainty of a
quantity f when it is a function of a number of other quantities, u, v, w, each with their own
uncertainties:
Please note that the variance formula assumes that parameters u, v & w as well as their uncer-
tainties are mutually independent. By default, we use a central difference scheme to estimate
velocity gradients, meaning that the two velocities we subtract are two steps apart in the vec-
tor grid, so the Interrogation Areas from which they originate will be completely separated
even with 50% overlap between neighboring IA’s. With more than 50% overlap between
neighboring IA's or any overlap at all if/when we compare immediate neighbors, velocities will
originate from partially overlapping IA's and the assumption of independence does not hold!
Overlapping IA's will lead to positive correlation between neighbor vectors so if one vector
overshoots the true value the neighbor will tend to overshoot as well and the difference
between the two will in fact be closer to the truth than the computed uncertainty would pre-
dict. In this case assuming independence can thus be considered a worst case scenario.
The estimated uncertainties will typically come from "PIV Uncertainty" on page?707, that offer
different methods to estimate the uncertainty of computed velocities. Some of these meth-
ods estimate uncertainties for each individual velocity component, others provide a single
common radial uncertainty that covers them all. The Scalar Derivatives depends on individual
velocity gradients and likewise the uncertainty of those derivatives depends on uncertainty of
the individual gradients. Methods that provide uncertainty for individual velocity components
are thus the preferred and recommended input, but if radial uncertainty is the only one avail-
able it will of course be used instead.
16.80 Scalar  Map
The 'Scalar map' display function is used for on-screen display of a number of data-types,
including e.g. Vorticity and re-sampled LIF data representing concentration or temperature.
It is also used for display of 3D-PIV vector maps, where the scalar display is used to show the
out-of-plane velocity component, while in-plane components are shown using a traditional vec-
tor plot.
The numerical method 'Scalar Map' is used to extract a scalar quantity from a dataset with mul-
tiple values, and the parent dataset will thus determine the contents of the recipe with
respect to the options available.
Select the dataset from which you want to extract scalar data and from 'New data set' select
'Scalar Map' in the category 'Plot'.
The recipe will show you what scalar quantities are available from the parent dataset, pick the
one you want:
PAGE | 788
PAGE | 789
Extracting a Scalar map from a Vector map.
Peak heights, Peak height ratio and Peak widths for the 1st and 2nd highest peak in the cor-
relation plane can be used to evaluate the overall quality of your vector map. For high quality
data, peak 1 should be significantly higher than peak 2, i.e. the peak height ratio should be
higher than one.
Peak widths should normally be in the range 3-6 for good quality data. Narrow peaks normally
indicate that particle images are very small, introducing risk of pixel locking in the resulting
vector maps. Broad peaks can be the result of large (poorly focused) particle images, or
caused by strong flow gradients within the interrogation area used.
The last entries are used to extract horizontal and/or vertical velocities or to extract the velo-
city magnitude of the vectors (the length).
Press the 'Apply' button to calculate the scalar map and view the results (Click on the 'Display'
button to access further visualization methods) and 'OK' to accept the calculation and visu-
alization settings. When needed, raw data can also accessed via the 'Open as numeric...'
option.
Example: Cylinder wake, where mean flow is shown as a vector map overlayed on a scalar map
of Var{U} + Var{V} (~turbulent energy).
All scalar maps (other than re-sampled scalar maps) are tagged with the icon      for simple iden-
tification with further processing, e.g. with batch export via the Tecplot Loader or the
MATLAB Link.
16.80.1 Visualization  methods…
With the mouse, double-click on the scalar map (or Right click on the map with the mouse and
select 'Display option'): scaling options, color codes and other advanced data representations
are now available.
PAGE | 790
PAGE | 791
The recipe tab 'Levels and Range'
'Levels' determine how many different shades and/or colors should be used for the scalar
map display.
'Minimum' and 'Maximum' determines the upper and lower limits for the scalar values shown,
where the default 'Use full range' will set these automatically based on the numerical values
present in the scalar map.
The recipe tab 'Style'
'Drawing style' determine how the scalar map is shown on the screen. The default is 'Follow
contours', where the display will interpolate between discrete scalar values to estimate the
continuous variation of the scalar quantity.
The style 'Discrete' will not interpolate, but simply show a colored rectangle for each point in
the scalar map, thus producing a display of colored 'tiles'.
For the contour plot you may choose to add lines for each change in contour level, or show
these lines only.
'Color use' determines the color palette used to display scalar values. The default is 'Rainbow',
going from magenta over blue, cyan, green and yellow ending in red. From the drop down list
you can choose a number of other palettes, including simple grayscale shading.
The recipe tab 'Interpolation'
This is relevant only when showing contours and/or contour lines, not when using the drawing
style 'Discrete'.
For every pixel in the display interpolation is performed by calculating a weighted average of
neighboring discrete scalar values. The size of the averaging neighborhood is determined by
the 'Integration step size', where large areas produce smoother displays than small areas.
The checkbox 'Mask out invalid regions' provides the option of not showing regions that are
invalid. For example previous masking of a vector map may have tagged some of the vectors
as being outside the flow (i.e. inside a wall or similar).
16.81   Scalar  statistics
For an ensemble of scalar maps three statistical analyses can be performed, Scalar Sum, Scalar
Mean & Scalar RMS.
As the names imply, the analysis methods will compute respectively the sum, the mean or the
rms of scalar values and produce a single scalar map from an ensemble containing multiple
scalar maps.
To actually perform the analysis right-click the scalar map ensemble in the database window or
right-click inside an open display of one of the scalar maps in the ensemble. From the resulting
context menu select 'Analyze...' and then select the analysis category 'Statistics' or 'LIF?Signal':
PAGE | 792
PAGE | 793
Pick the desired analysis method and click OK.
Traveling vortices in the boundary layer of an acoustically
excited jet appear as local minima and maxima in a scalar
map of vorticity (vector map overlaid for clarification).
Scalar mean of a series of vorticity maps illustrate how overall
vortex strength (vorticity) decay downstream from the jet exit.
PAGE | 794
PAGE | 795
Scalar RMS of a series of vorticity maps indicate that the top
vortices appear more stable than the bottom ones, but both
spread out downstream from the jet exit.
Computing the Scalar Sum (not shown) would produce results corresponding to the Scalar
Mean multiplied by the number of scalar maps in the parent ensemble.
16.82 Shadow Combine
Shadow combine is to combine sequences of shadow data. Later the user can extract shadow
statistics from the whole sequences of shadow data by using shadoe histogram, diameter stat-
istics, and etc.
Click OK or Apply to start the Shadow Combine.
16.83 Shadow Histogram
The histogram display refers directly to the underlying histogram dataset, which is a subset of
the Shadow dataset.
.
When selecting Shadow Histogram, the following window appears.
PAGE | 796
PAGE | 797
16.83.1 Variable to process
Select the variable from the list.
Equivalent Diameter is the diameter of the spherical particle having the same surface as the
measured object.
Area is the area of the measured object
Perimeter is the perimeter the measured object
Eccentricity expresses the elongation of the object. For a circle, Ecc is 1 and for a line it is
greater than one. The eccentricity is defined as
where the central moments are defined as
,
and where the central moments for an image are calculated as
and the centroid as
and
Orientation theta can be defined as the angle between the X axis and the principal axis
Shape factor is a measure of the circularity or compactness of the shape
where P is the perimeter and S the surface of the shape.
Major and Minor axis  length are the major and minor radii of the ellipsis calculated as
U and V  velocity are define along the horizontal and vertical axis respectively.
Equivalent volume are calculated based on the assumption that the particle has a symmetric
structure along its long axis. It first slice the particle along its long axis by a height of 1 pixel,
calculate the volume of each slice, and finally sum up all the volume to represent the whole
particle volume.
Speed is the magnitude of velocity
16.83.2 Process data  from
In double frame, mode, the data can be processed from the first frame (check Image A), from
the second frame (check B) or from both
16.83.3 Display
Select the histogram type between count, percent of the total count or cumulative.
16.83.4 Scaling
Check autoscale checkbox so that the X axis scale is automatically adapted to the range of the
measured particles or uncheck it and type the minimum and maximum value. The number of
bins or classes can be changed from the "Bins" field.
PAGE | 798
PAGE | 799
16.83.5 Region
The histogram can be calculated from a specific region of the looking area. Uncheck "use
entire area" and type the X and Y coordinates.
16.83.6 Histogram  display properties
In a right clicking on the graph, it is possible to change the display options and the data selec-
tion (see window below).
For further information about the XY display  setup, please refer to XY display in display sec-
tion of the online help>
To copy  the graph to the clipboard, press "clip clipboard" and paste the graph in a third part
software.
16.84 Shadow Processing  (Legacy method)
This analysis method is obsolete and included for backward compatibility only.
It remains functional, but no further development or maintenance will be made
and it may be removed completely in future releases of DynamicStudio.
We recommend using "Shadow Tracking" on page?813 instead.
This method is used to extract information such as the size, the position, the shape, the velo-
city, etc. of droplets, bubbles or particles imaged according to shadow principles. Due to the
nature of the technique, there are no limitations on the size and shape of the droplets, etc.
and it can be used both with transparent and opaque droplets/particles as well.
The Data base should include single- or double-frame image(s) and calibration image.
PAGE | 800
PAGE | 801
16.84.1 Content
Field of view and calibration
Recipe <Shadow processing> dialog window
l  Assistant
l  Threshold levels and validation criteria
l  Region of interest (ROI) processing
l  Reject  non-closed contours
Data visualization section
16.84.2 Field of view  and calibration
The field-of-view is defined (i) either using a calibration target and dewarping shadow images
accordingly (see related help files from the category 'Coordinates') or(ii)?using the spatial
information contained in the "Field of view"?property?of?the?calibration? menu (default set-
tings). When using the first methodology, press the 'Select' button and look for the calibration
to use.  The scale factor (second methods) can be measured from the calibration image by
doing a right mouse click on the image and by selecting
Field of view property
Measure scale factor from calibration image
When using the measuring scale factor the origin point displaces the found particles by its off-
set to the image origin. If no displacement is needed, the origin point must be placed in the
lower left corner.
16.84.3 Recipe <Shadow  Sizing>
To use this numerical method, select the single- or double-frame image(s) of interest, call the
method 'Shadow processing' located in the 'Particle characterization' category and complete
the following dialog window (recipe) as described below.
PAGE | 802
PAGE | 803
Shadow processing dialog window
l  Assistant
The shadow assistant is a setup wizard which assists the user in determining the parameters
that best capture the particle image characteristics of acquired images. This is done by select-
ing a particle on one or two images and analysing the outcome. The Assistant displays the gray
level characteristics, including particle pixel depth and edge gradients.
The results are shown in 2 windows, the first displaying the particle contour with statistical
information, the second displaying the gray level characteristics along the minima and maxima
lines.
A general procedure is as follows: ・
l  Select region about one particle in frame 1. ・
l  The software automatically determines the optimum local threshold and processes the
image.
l  The user can then adjust the local threshold manually and reprocess. ・ To go back to the
suggested threshold press Auto select
l  Select frame 2 (if double-imaged) and press select to pick one particle. ・
PAGE | 804
PAGE | 805
l  Once complete the user can select OK and the parameters are saved to the analysis setup
for processing of the entire image. The local threshold is used to calculate validation cri-
teria (Edge height and edge slope validations). A small difference might be observed
between the local threshold suggested by the assistant and the threshold level in the
recipe window. A safety margin is applied so that the local threshold can be used for the
other particles without rejecting too many of them. In case of 2 image frames, the softest
parameters of the 2 frames will be used in the recipe window (minus a safety margin).
Note:
If several particles are selected simultaneously, the assistant will process only one.
It is a good idea to test several particles in the same image when optimizing validation, also to
take into account variations. The parameter suggested from the wizard should be considered
as starting point: it might be necessary to manually fine tune the validation parameters to
improve detection rate.
l  threshold value and validation criteria
The threshold level is a main parameter that helps define the contours of the droplets,
bubbles or particles on the images. This parameter expresses the threshold level in % of the
maximum gray level of the camera. The higher this value is, the easier is becomes to identify
'large' structures.
Auto-compensate double images:?In double frame mode,?check this check box to equalize
first and second frames relatively to their mean, maximum and minimum gray levels.
l  Edge height/Slope validation
The edge height validation is the acceptance of particles whose edge depth as a pro-
portion of total image resolution satisfies a user defined value. In many situations the
visibility of particles is limited, either due to scattering of the medium or poor light
access. In these cases the edge height will be limited.
Examples
In the above example a particle shows a rather soft edge and utilizing a small portion
of the entire image range. An even weaker shadow image is shown below:
PAGE | 806
PAGE | 807
In this example there is no distinct edge, but rather a soft featureless (probably defo-
cused) particle. The edge slope is calculated from a least squares fit of a line (made up
of segments) that crosses the threshold cutoff. For a particular image the threshold
cutoff is determined at the midpoint between the minimum and maximum edge
height.
In the above case the edge depth is so small that the slope has little meaning and
should be rejected outright as a suitable particle measurement. The slope is high
because of a noise element in the profile near the threshold. Therefore, both edge
height and slope need to be taken into consideration when selecting validation limits.
A fine example of a focused particle in a weak light environment is shown below.
PAGE | 808
PAGE | 809
Often, in double images, one of the images of a particular particle may be out of
focus. It may help to step back any restrictions to allow for particle matching (in cases
where velocity is needed). The auto-selection feature in the shadow assistant will nor-
mally step back 5 % when determining suitable limits.
l  Area validation
Enable the 'Area validation' and set the minimum size (in pixels) to remove the noise when
needed. The maximum area (in pixels) is typically not used and is generally set to any high
value.
l  Region of interest (ROI)
To limit image analysis to a given region of the image, uncheck the 'Use entire region' check-
box, press the 'Region' button and select (with the mouse) the area of interest. If required,
fine-tune the (X, Y) positions of the ROI manually by adjusting the (X1, Y1) and (X2, Y2) values.
Press the 'OK' button to close the ROI dialog window and jump back to the 'Shadow pro-
cessing' main window.
Region of Interest processing is defined according to user needs.
l  Options
Merge open contours; attempts to merge nearby open contour segments, and thereby close
the contours. The search radius of the merge operation is given by the Merge distance input.
If any open structures remain, remove them by check the 'Reject non-closed contours' option
(if required).
Press the 'Apply' button to preview the results and accept/extend the processing to the other selec-
ted images.
16.84.4 Data  visualization
The resulting image is labeled with the icon       to facilitate its location in the database. To edit
this result, press the 'Open as numeric' shortcut       and a spreadsheet containing information
on droplets, bubbles or particles location, center position, eccentricity value, equivalent dia-
meter, etc. is open. This information may then be copied or sent to the MATLAB Link for fur-
ther data analysis.
PAGE | 810
PAGE | 811
To enhance selected information on the image (e.g. axis and contours), open the 'Display'
option of the shadow result image and modify the color codes and the scaling of the velocity
vectors.
Measured parameters such as eccentricity, orientation, shape factor, major and minor axis
length, velocity are described in the Shadow spatial histogram (Analysis method). For more
information see "Shadow Histogram" on page?796
Information gained from shadow processing can be enhanced and accessed for individual
droplet, bubble or particle.
16.85 Shadow Resample
Shadow resample generates a vector map based on the displacements of the particles in the
shadow/particle dataset.
Number of cells:
Defines how many vectors will be generated in x, y and z.
Scaling:
Defines the area/volume where vectors will be generated.
If Autoscale is selected, the maximum extend for any dimension will be used.
16.86 Shadow Spatial  Histogram
The histogram display refers directly to the underlying histogram dataset, which is a subset of
the Shadow dataset
?When selecting Shadow Histogram, the following window appears.
PAGE | 812
PAGE | 813
16.86.1 Calculation
Select the histogram type from the?drop?down?list. Histogram type can be selected between
Particle?count,?Equivalent?diameters,?Area?mean,?Perimeter?mean?Average?orientation,?Aver-
age?eccentricity,?Average?shape?factor,?U?mean?and?V?mean. Concerning the definition of
these parameters, please see "Shadow Histogram" on page?796.
16.86.2 Number of cells
The image is divided in spatial cells. The mean value will be calculated from the particles detec-
ted in each cell.
16.86.3 Process
In double frame, mode, the data can be processed from the first frame (check Image A),?from
the second frame (check B) or from both.?
16.87 Shadow Tracking
Shadow tracking detects particles from their shadow image and extracts information such as
the size, the position, the shape etc. It also tracks the movement of particles shadow between
two frames and extracts velocity information of the particles found in the images.
The input needed is an ensemble of images containing shadows of particles. The user can
optionally supply an ensemble of vector maps that will be used to assist in the tracking process
of the particles.
16.87.1 General description  of the algorithm
This algorithm firstly detects particles based on a global threshold (user selectable). Then each
particle is segmented from the global image with a bounding box slightly larger than itself for
further process.
After the particle is segmented from global image, a local threshold will be applied to the
bounding box to extract the particle. The local threshold is calculated by:
Where imin and imax are the minimal and maximum gray value of the segmented single
particle image. Segmented single particle image uses this threshold to be binarized and
particle will be extracted for characterization.
If velocity evaluation is requested, an advanced Particle Tracking Velocimetry will be used to
process particle images based on centroid of each identified particle.
16.87.2 Dialog window  (recipe)
To use this method, select the images to be processed, and then select the analysis method
"Shadow Tracking" located in the "Particle characterization" category and complete the dialog
window (recipe) as described below.
PAGE | 814
PAGE | 815
The dialog window for shadow tracking is composed mainly by a setting panel and a window to
show both raw images and detected particles. Only particles detected in the area selected by
the user are shown as white, while the rejected particles are shown as gray and the rest are
shown as black. The results will be updated immediately after the user changes the settings in
the setting panel, which are described below:
Process  only  selected area:
The analysis method only processes particles inside the area selected by the user using the
ROI box.
Apply  Mexican Hat Filter:
If this is checked, a 2D Mexican hat filter will be used to convolve the raw particle image. The
result is a 2D map of intensity (gray value) gradient, where the edge is highlighted because
gradient of intensity (gray value) is maximum at the edge of particles.The result image of
intensity gradient will be used for particle segmentation.
If the back illumination for shadow measurement is non-uniform, this option is recom-
mended.
Particle Edge Gradient /  Particle Intensity:
To select the threshold for particles segmentation from globe image. 100% is the Otsu
threshold calculated from intensity (gray value) histogram based on Otsu algorithm.
When Mexican hat filter is used, instead of segmenting particles from raw images, particles
are segmented from intensity gradient image. And 100% is the Otsu threshold calculated
from histogram of intensity gradient, also based on Otsu algorithm.
Particle Eccentricity:
The analysis method will reject particles that are more eccentric than user-selected ratio
MaxAxis / MinAxis. If the particle is perfect round, then the ratio MaxAxis/MinAxis will be one.
Particle roundness  (Normalized Particle Momentum of Inertia):
The analysis method will set up a validation criteria based on particle's roundness. This para-
meter is also called normalized momentum of inertial, made to measure the spread of points
around the center of mass. It can be calculated by:
Where x and y are coordinates of pixels within the particle, x and y are the centroid of this
particle. The table below shows roundness of some particles with typically shapes:
Particle separation:
This is an embedded process to separate overlapping particles by eroding the shadow, sep-
arating the shadow into its sub components and expanding the separated sub components by
the same amount as the erosion. The process is illustrated below:
Both the erosion and expansion of the shadow is done by a percentage of the radius circle of
an equivalent area to a given particle, selected by the user using the slide bar .
Minimum/Maximum Particle Size:
Specify the lower and upper limit of accepted particle area (in pixels).
Enable Tracking:
Check this box for particle tracking velocimetry (PTV) analysis. If only particle detection and
characterization are needed, the tracking can be disabled in order to speed up the pro-
cessing.
The PTV algorithm here is similar to 2-Frame 2-D PTV in Dynamic Studio. The user is encour-
aged to perform an adaptive PIV first and then use the result of adaptive PIV analysis as a first
PAGE | 816
PAGE | 817
input for the PTV analysis, for better accuracy. Please refer to "2-Frame 2D PTV" on page?424
for more details.
Search Radius:
If no adaptive PIV vector map is selected as an input, the software will search similar particle
around the center of the particle (for velocity evaluation) within an area specified here.
If there is an adaptive PIV vector map selected as an input, the software will search similar
particle around the predicated center of the particle (for velocity evaluation) in the second
frame - estimated from the input vector map, within an area specified here.
Current Image:
Browse through the images to verify the settings across all images.
16.87.3 Data  Visualization
The resulting image is labeled with the icon       to facilitate its location in the database. To edit
this result, press the 'Open as numeric' shortcut       and a spreadsheet containing information
on droplets, bubbles or particles location, center position, eccentricity value, equivalent dia-
meter, etc. is open. This information may then be copied or sent to the MATLAB Link for fur-
ther data analysis.
To enhance selected information on the image (e.g. axis and contours), open the 'Display'
option of the shadow result image and modify the color codes and the scaling of the velocity
vectors.
Measured parameters such as eccentricity, orientation, shape factor, major and minor axis
length, velocity are described in the Shadow spatial histogram (Analysis method). For more
information see "Shadow Histogram" on page?796
16.88 Shadow Validation
Check the validation criteria to be applied and type the range of each parameter.
For the parameter definition, please see "Shadow Histogram" on page?796.
16.89 Size-velocity correlation
The size-velocity correlation is a 2D histogram that bins velocity data against to diameter
classes. Each particle in the input data is mapped according to its velocity and size. The map-
ping can be direct, as in point data, or binned further by the user to average the results.
PAGE | 818
PAGE | 819
Select component      Select which velocity component or velocity mag-
nitude to compare with diameter
Scaling                          Auto-scaling or user selectable
Velocity; min, max      User selectable limits for velocity
Velocity bins                Number of bins to use for velocity (only for binned
processing)
Diameter; min, max   User selectable limits for diameter
Diameter bins             Number of bins to use for diameter (only for
binned processing)
Output                         Point data ? direct output as X-Y data binned ? aver-
aged results as colored scalar plot
Region                         Select region of interest
- Use entire area-
- User limits: x min, x max, y min, y max
Output as  point data
Output as  binned data
16.90 Spectrum
The analysis method Spectrum makes an estimate of power spectral density based on an
extract from multiple datasets in selected points (i, j). This is particularly useful in connection
with for example time resolved data, but can also be used in other connections.
PAGE | 820
PAGE | 821
Input to the Spectrum calculation should be a multi-selection including multiple scalar or vec-
tor maps (2D or 3D). From the input datasets the user must select one or more points in
which to perform the calculation, and also specify the quantities to be used for the analysis
(for scalar maps this will be the scalar quantity, while for vector maps it can be one of the velo-
city components measured).
The results of a spectrum calculation can be shown as graphics or opened as Numeric in a
spreadsheet display from where it can be exported or copied via the clipboard to e.g. MS
Excel.
16.90.1 Example:  Spectrum  in  the wake of a  cylinder
In the wake of a cylindrical obstacle vortices are being generated at a fixed rate depending on
the free stream velocity and the size of the obstacle. Statistical analysis of a time resolved PIV
data series produces both a mean flow field and a map of the variance for U- and V-velocity
components. In the figure below the mean flow field is shown superimposed on a map of the
variance of the V-component. The strongest fluctuations appear to be in a point about 2 dia-
meters downstream of the cylinder.
In this point we can extract a time series of the V-component to confirm the periodic oscil-
lations (See help files for XY Plot regarding time series plot):
To perform a spectrum calculation on the basis of this signal you must first select the
ensemble containing the vector maps. (The spectrum is calculated using FFT, but the number
of vector maps need not be a power of two. If it's not the algorithm will automatically zero-pad
your data to the nearest higher power of two).
For the first dataset in the multi-selection select Spectrum in the New Dataset dialog:
In the resulting recipe you must now select the quantities on which you wish to do the analysis
(in this case the V-component of the velocities measured)
PAGE | 822
PAGE | 823
You must also identify the point(s) in which you want to perform the calculation.
Points in the vector map are identified with index numbers, where position (0,0) is the lower
left corner of your vector map. Please note the option to open the parent dataset to see an
example of where the point(s) selected are in the flow-field:
PAGE | 824
PAGE | 825
If you wish to calculate the spectrum in several points you can either add several points to the
list here, or repeat the entire calculation for each point. Calculating several spectra sim-
ultaneously will store the results together, which may be convenient for numerical overview.
In graphs the data will be shown as differently colored curves in the same figure window. If
you have more than a few spectra it may be more convenient to perform a separate analysis
for each of them in order to get the plots shown in separate windows.
If you wish to show the spectra using logarithmic axes, just right click the display and select
Logarithmic from the context menu:
16.91 Spray Geometry
16.91.1 Introduction
Spray geometry determines the spatial geometry of a spray nozzle with one or more nozzle
exits. The analysis can characterize the geometry of a spray seen as a plume ("Spray Geo-
metry" above) or seen from below or above as a pattern ("Spray Pattern" (on page?832)). Of
interest is the number of plumes (cones) found as well as their geometry and orientation. Typ-
ically the geometry of interest occurs at a specific time delay after injection. The user may be
interested in the temporal development of the spray at different time delays, or time aver-
aged.
The analysis of the spray geometry requires the following steps:
n Determination of the number of cones (plume).
n The orientation of each cone.
n The angular spread of each cone.
n The penetration length of each cone.
Prior to analysis the user must:
PAGE | 826
PAGE | 827
n Enter the nozzle coordinates and direction:?"Spray nozzle properties " on the next page
n Select the region-of-interest (bounding area of spray): "Region of interest" on page?830
n Adjust the image threshold: "Cone Geometry - Plume geometry" on page?830
n Optionally adjust the radial scan limits: "Cone Geometry - Plume geometry" on page?830
16.91.2 Setup window
Spray settings                   Select nozzle origin, direction and search area using the mouse in
the select Nozzle dialog or entering numeric values
Flat spray                           Select whether the spray is seen primarily from the side or from
above/underneath (Flat spray checked). If Flat spray is checked the
method searches 360 degrees around the nozzle point, otherwise
180 degrees will be searched.
Radial limits                       Set Minimum and maximum R(adius) to sweep during analysis. R?is
defined from the nozzle origin using the mouse in the select Nozzle
dialog or entering numeric values
Invert image                     Invert the image gray-scale to process images where background
has a higher gray value than the spray
Mean filter size                 A NxN mean operator smoothens the image.
Threshold filter                 Percent of total gray-level range of the camera to apply as
threshold value. The corresponding gray scale level is displayed.
Updating the slider while the threshold image is being shown, will
update the threshold image in real time.
Select ROI                          Select region of interest. Only the area selected will be analyzed.
Select frame                      Select image frame to process.
Output                               Select plume or pattern output
Z (height from the
nozzle)
Vertical distance from the spray origin used to calculate cone angle
from spray pattern images
16.91.3 Spray nozzle properties
To define the nozzle origin, type in the X and Y coordinates (in pixel) or press Select Nozzle
The following window appears.
- Select Frame 1 or Frame 2 (bottom of the window)
- With a right mouse click, you can zoom in and adjust image contrast. Thresholding can be set
from the main dialog. Mouse moves can be reset by pressing Esc, at any time.
- Set the nozzle properties by:
PAGE | 828
PAGE | 829
- Press the left mouse button to identify the spray origin (red cross).
- Drag the mouse to define the spray direction and the outer radius of the search area.
Release to finish the selection
-Click the image to define the inner radius of the search area
If the Flat spray is selected, the spray radius indicator will be circular instead.
16.91.4 Region  of interest
To define the Region Of Interest (ROI), press the ROI?button. The region to be used is delim-
itated by a red rectangle. With the mouse, it can be moved, expended or reduced from the
small square on the corner. A right mouse click on the image allows to zoom in and out and
adjust contrast image display.
16.91.5 Cone Geometry - Plume geometry
The cone geometry analysis is as follows:
n A NxN mean operator smoothens the image. The size of the mean filter is user-specified.
A large N will effectively dilate the image (enlarge the spray area on the image)
n A user-supplied threshold operator filters out all pixels below the specified value. Adjust-
ing the threshold affects the outcome of the cone angles as well as penetration length.
n A radial sweep (originating at the user-specified nozzle position) is conducted to determ-
ine the number of cones. A histogram of results is accumulated whereby the maximum
number of counts determines the number of cones. The user can manually adjust the
radial sweep length to better suit the image inputs.
n Once the number of cones is determined, a radial sweep is conducted once more to
determine the cone edges. Since the edges vary along the length of the cone an average
value is calculated. Here the radial sweep length will play a part in the cone spread angle
since a long sweep will often result in a narrow cone, a short a sweep a wide angle.
PAGE | 830
PAGE | 831
n The penetration length is then evaluated by taking roughly 25 % of the far spray edge and
averaging the length.
n The cone angle is measured clockwise relatively to the positive horizontal axis.
The measured parameters are displayed in the info box of the dataset. If the info box is not
available, right click on the spray geometry dataset and check the info box option.
The results are also available as a separate table. From the tool bar of DynamicStudio, click on
the "open as numeric" icon:
16.91.6 Spray Pattern
The spray pattern analysis determines the position and shape of the each spray cone cross-
section at different distances from the nozzle. The analysis follows closely with the plume geo-
metry in that:
PAGE | 832
PAGE | 833
n The input image is smoothened by a NxN mean operator.
n A threshold is applied
n The number cone objects with a cross-section above a certain size are determined
through Canny edge analysis.
n The centroid position is determined.
n The pattern perimeter and spatial statistics are calculated
The measured parameters are displayed in the info box of the dataset. If the info box is not
available, right click on the spray geometry dataset and check the info box option. The para-
meters are also available in a separate table (click on the "open as numeric" button from
DynamicStudio tool bar)
Parameters:
Centroid X, Y          center of the object
Area                        area of the measured object
Perimeter              the perimeter the measured object
Shape factor          a measure of the circularity or compactness of the shape. Furhter inform-
ation can be found in "Shadow Histogram" on page?796
Alpha                      Angle between the cone axis and nozzle axis. Requires the distance
between the spray origin and the measurement plane to be documented
Liquid content       Estimation of the relative quantity of liquid calculated as follow:
16.91.7 Spray geometry processing - Temporal evolution
The Spray geometry processing allows plotting geometrical parameters versus a user defined
variable such as a time delay after injection. This routine can be used to plot temporal evol-
ution of the penetration length for example.
Select the spray geometry datasets to be used for the plot as fixed input (press bar space)as
shown below:
PAGE | 834
PAGE | 835
Do a right click on the first spray geometry dataset and select Spray  Geometry  Processing.
Time variable                   Select the variable for the X axis. This variable must be available for
each spray geometry ensemble to be used for the plot. see below
Select cone                       Select the cone to be studied in each ensemble
Select variable to
include
Select the variable to be plotted (Y-axis)
How to add a Time variable to the Spray  Geometry  ensemble
Prior to use the Spray Geometry Processing, it is necessary to add a time variable to each
Spray Geometry ensemble. Right click on the Spray Geometry icon and select "Custom prop-
erties".
Add a property from the window below by clicking on "add" button and by typing in a name
(delay in this example). Press OK.
PAGE | 836
PAGE | 837
The new property is now added to the record properties window and can be documented:
16.91.8 Trouble shooting
The following error message appears when the software cannot find any cones.
To solve the problem, make sure that
n The origin of the spray is positioned correctly
n The Region of Interest (ROI) covers the cones to study
n The Radii have ben set correctly
n The output (Plume geometry or Spray pattern)?is set correctly
n The threshold is not too high
n The spray is not too noisy, try increasing the blur filter size.
16.92 Stereo-PIV
The Stereo PIV processing method computes 3C velocity vectors in a 2D?plane (a light sheet)
by combining data from two cameras, each providing double-frame particle images from the
light sheet as seen from different viewpoints. From each of the two cameras the method
requires a 2D?vector map and a camera calibration ("Imaging model fit (Legacy Method)" on
page?556), describing how points in object space map to points in the image plane of the cam-
era in question.
When looking at the light sheet at an angle instead of head on, the lens and image plane need
to be tilted as illustrated below. This is known as the Scheimpflug condition and ensures
proper focusing :
PAGE | 838
PAGE | 839
Due to perspective out-of-plane motion will be perceived differently from each of the two
cameras and this difference is exploited to infer the third velocity component. This is illus-
trated above where the true displacement is shown by the blue vector, whereas the green
and red vectors illustrate what this looks like from Camera 1 and 2 respectively (projected
onto the light sheet/object plane).
16.92.1 Method and formulas
An imaging model F describes the mapping of a point X=[X Y Z]T in object space to the cor-
responding point x=[x y]T in the image plane of a camera:
… or splitting the vector function F(X) into separate scalar functions f & g for x &?y respect-
ively:
… the exact behavior of functions f & g is determined by the imaging model chosen
With two cameras there will be two such imaging models and two sets of image coordinates
from the same point in object space.
Using subscripts A and B to identify each of the two cameras we get:
…or…
If we know (or assume) that two image points (x,y)A and (x,y)B represent the same point in
object space, the expressions above create 4 equations with 3 unknowns from which we can
estimate the corresponding object space location (X,Y,Z).
Differentiating with respect to time the point-to-point mappings become mappings of velocity
instead:
…or equivalently in a matrix formulation:
Multiplying with the time between the two exposures in the double-frame image we find the
mapping of displacements for a single camera:
…introducing again x=f(X,Y,Z) and y=g(X,Y,Z).
Please note that the partial derivatives above will themselves typically depend on object space
location (X,Y,Z).
Once more we combine data from two cameras to obtain a system of 4 equations with 3
unknowns (as before subscripts A &?B identify the cameras):
Solving this equation system in a least squares sense leads to an estimate of the object space
displacement [ΔX, ΔY,ΔZ]T.
As a quality check we can project the solution back through the gradient matrix and compare
to the image plane displacements from which the solution was found:
PAGE | 840
PAGE | 841
The total reprojection error ε should be below 0.5-1.0 pixels for decent quality PIV?images and
reasonably accurate camera calibrations. If it is not one (or both) of the 2D vectors from cam-
era A &?B?may be erroneous and the algorithm may reprocess with each vector replaced with
the average of its immediate spatial neighbors.
16.92.2 Input Required
Stereo PIV requires an imaging model fit and a 2D?vector map from each of the two cameras
used.
Either of the vector maps can be the parent, if the images from which the vector maps have
been derived aren't dewarped, camera calibrations needs to be part of the user selection that
need to be made before entering the Stereo PIV?recipe (See "Working with the Database" on
page?93 and/or "Selection (Input to Analysis)" (on page?419) ). If dewarped images are used,
the camera calibrations are automatically selected based on the corresponding cameras in the
input.
The vector maps can be derived from raw images or from images that have been dewarped
("Image Dewarping" on page?501):
The two vector maps must share a common grid of vector locations, which is easiest accom-
plished by dewarping the images to a common user defined grid and applying the same pro-
cessing to get the vectors. The images can be dewarped to the same grid by selecting both
image ensembles and calibrations when dewarping the images. This will make sure the images
are dewarped to the same coordinate system.
The two vector maps do not have to be from the same level, i.e. one can be a raw vector map
and the other can be a validated vector map.
16.92.3 Recipe for Stereo PIV?processing
Different parts of the recipe become active or inactive depending on the input chosen.
If the 2D vector maps are derived from raw images you may choose to define your own mesh
(/grid) or have DynamicStudio generate one for you. The auto-generated grid will by default
attempt to match the vector density of the parent 2D?vector maps, but setting oversampling
factors larger than one will increase the density, while oversampling factors smaller than one
will reduce it:
If the vector maps are derived from dewarped images, the resulting 3D?vectors will inherit
the grid from their 2D?parents (you will get an error message if the two 2D?vector maps do
not share a common grid).
Since the grid is given by the parents, oversampling or user defined mesh is not an option and
thus disabled in the recipe. You can however specify a max accepted reconstruction error (the
ε described above). For historical reasons this is not supported when 2D parent vector maps
are derived from raw images:
PAGE | 842
PAGE | 843
If the reconstruction error ε exceeds the specified limit, DynamicStudio will assume that one
or both 2D?vectors are erroneous and try to replace them with the average of the 8 nearest
neighbors. This leads to three new stereo reconstructions;
l  One where only the vector from Camera 1 has been replaced.
l  One where only the vector from Camera 2 has been replaced.
l  One where vectors from both cameras have been replaced.
If either of these calculations brings the reconstruction error below the accepted limit,
Dynamic Studio will pick the solution with the smallest ε (if possible replacing only one of the
vectors).
If the reconstruction error remains too high even when replacing both vectors with the aver-
age of their neighbors, the system will keep the initial result, but tag it as invalid.
16.92.4 Displaying results
In-plane velocity components U &?V are typically shown as a conventional vector map, while
the Out-of-plane velocity component W?can be shown as a scalar map underneath:
You can also overlay images and vector maps, but the image needs to be dewarped so both
datasets are represented in a common object space (metric) coordinate system rather than
an image plane (pixel) coordinate system. Please note that the image dewarping simply maps
the image plane onto the light sheet plane. This means that objects in front of or behind the
light sheet will be more or less displaced and may also appear distorted depending on the
viewing angle and the distance from the light sheet.
To see the image you may need to turn off the scalar map display of the out-of-plane velocity
W, make the scalar map transparent or put the image on top, adjust the lookup table and
make the image transparent:
PAGE | 844
PAGE | 845
The example above shows the flow in a horizontal plane above a magnetic stirrer, spinning at
the bottom of a tank. The stirrer can be seen, but is probably slightly displaced due to per-
spective effects. Even so the out-of-plane component shows clearly how water is pushed up in
front of the stirrer tips and sucked down behind them. In this case the image is shown on top
of the vector map and the image uses a transparent and inverted color map from white to
gray so both in- and out-of-plane velocity components can be seen.
16.93 Subpixel  Analysis
Subpixel analysis takes as input a vector map and creates a histogram of pixel displacements.
This may be done over the full range of displacements in the vector map or on the subpixel
part alone. The presence of peaks near integer pixel values may indicate pixel locking in the
data. Pixel locking can be caused by particle images being smaller than 2 pixels in diameter
meaning that the Nyquist sampling criterion was violated already when images were acquired.
In this case the data is undersampled and there is very little you can do to recover the inform-
ation lost during image exposure.
If particle images are too small the best solution will be to acquire new images where particle
images are bigger. This can be accomplished by reducing lens aperture and/or defocusing the
lens slightly to make particle images blurry. Both will reduce particle image intensity and if it
was low already particles may no longer be detectable, so correlation becomes impossible.
You may be tempted to increase particle image sizes by low-pass filtering the undersampled
images before you correlate, but this will increase particle image sizes symmetrically around
the already biased positions and thus not remove the bias towards integer pixel positions.
Instead you can try to correlate using larger interrogation areas, thereby including more
particles in the calculation of each vector. Averaging over a larger area can mitigate the prob-
lem of pixel locking at the price of reduced spatial resolution.
Below is an example of a situation where pixel locking is present; To the left the full range his-
togram shows particle displacements from just below 7 to just above 9 pixels, but with a very
distinct peak at 8 pixels and two less distinct peaks near 7 and 9. To the right the same vector
map has been processed looking at the subpixel part and a clear bias towards zero can be
seen.
Please note that a peak near integer pixel values does not necessarily indicate pixel locking! If
the flow being measured has a very narrow velocity distribution the peak may actually
describe the physics of your flow correctly. To test for pixel locking in this case try to make a
new acquisition, where time between pulses has been increased or decreased slightly. This
will mean that particle displacements should increase or decrease accordingly, so the peak
should move if it represents the physics of your flow, but remain at or near integer pixel val-
ues if pixel locking is present.
The example below shows a situation similar to the one above, but without pixel locking: The
full range histogram on the left indicate that the majority of displacements are in the range 8-
PAGE | 846
PAGE | 847
9 pixels, while the subpixel part on the right shows a reasonably flat distribution also indicating
no pixel locking problems.
16.94 Temporal  Smoothing
The temporal smoothing method can be used to smooth scalars and vectors using its tem-
poral history. The method supports multiple different smoothing functions, such as poly-
nomial fit and Gaussian window.
Fig. 1 Graphical user interface for the Temporal Smoothing with vector input using polynomial
fit.
16.94.1 A) Smoothing settings
Kernel size: The number of datasets used for the temporal smoothing, centered around the
input dataset. For instance, a selection of kernel size 5 will use the datasets and its two two pre-
ceeding and two following datasets for smoothing.
Smoothing function: The smoothing function to apply to each of the input vectors /?scalars.
There are currently four different options to choose from:
l  Polynomial fit:?This smoothing function fits a polynomial at the requested polynomial
order. The fitted polynomial is then evaluated to find the smoothed value.
l  Box: Box window function, which is an even averaging of the input values.
l  Gaussian: Gaussian window function with the specified sigma.
l  Hamming:Hamming window function.
16.94.2 B) Values to smooth
The method will only process and smooth the values selected in this list. The number
of options in this list depends on the input data. At least one option must be selected
for the method to be applied.
16.94.3 C) Preview
The preview section can be used to inspect the input values and the smoothed output
of a selected vector/scalar. The settings in this area has no effect on the output of the
method, but can be used as an assistance for selecting the optimal smoothing set-
tings. The input values are shown as blue squares in the graph, while the smoothed
output value is shown as a green circle. The smoothing function is also shown as a red
dashed line.
Dataset slider: This slider is located under the input dataset and can be used to selected a
specific dataset to inspect. Note that datasets which do not have sufficient number of pre-
ceeding or following datasets (such as dataset #2, if kernel size is 5) will not be smoothed.
These are passed through without modification.
X and Y?slider: These sliders can be used to select a specific scalar/vector in the selected data-
set to inspect. The selected scalar/vector will be highlighted in a red box in the preview of the
input dataset.
Z?slider: For 3D data, the preview is still shown in 2D. This slider can be used to navigate
through the different XY-planes.
Component to preview: Selects the component to preview in the graph.
16.95 Tomographic  Particle  Tracking  Velocimetry
PTV (Particle Tracking Velocimetry) analysis methods can perform tracking of particles in
either a 2D plane or a 3D volume. The PTV methods are used for calculating the tracks of indi-
vidual particles in the measurement volume.
PAGE | 848
PAGE | 849
16.95.1 Tomographic PTV
The Tomographic PTV method uses double-frames or time-resolved particle images from mul-
tiple cameras observing the measurement volume from different viewpoints. For small and
very sparsely seeded volumes 2 cameras might suffice, but in most cases 3 or 4 cameras are
used. The cameras must be calibrated to a common reference coordinate system. This cal-
ibration can be performed by using the camera calibration method of Dynamic Studio (See
"Multi Camera Calibration" on page?585 or "Imaging model fit (Legacy Method)" on
page?556for more information on camera calibration.) Based on the camera images and the
corresponding calibrations, a voxel volume is reconstructed from each time-step. Within
these voxel volumes the particles are identified and subsequently tracked from frame to
frame over the entire acquisition.
The pictures below show the recipe dialog for the Tomographic PTV method when applied to
an image set. If the Tomographic PTV analysis method is applied to a voxel volume ensemble,
the reconstruction tab will not be shown.
The Tomographic PTV can be applied to time resolved data, meaning single frame images and
double frame data. In case of the time resolved images the result is one big data file con-
taining all tracks over time. In case of double images as input, the result is a single data set
with vectors for each single time step
PAGE | 850
PAGE | 851
Several parameters can be adjusted inside the recipe. The parameters are grouped into 3
panes.
The first pane is used for setting up the Voxel Reconstruction
The group 'Voxel space setup' is used to setup which part of the measurement volume to
reconstruct and analyze. The 'Center' is the metric position of the center of the reconstructed
volume. From here the volume spanseaqually half the distance of the specified size in
+x,y,and z- direction.'Voxel space' is the metric size of the axis aligned volume. 'Voxel res-
olution' is specifying the scaling factor between voxels and metric unit (mm). The voxel res-
olution is the same for all axis (aspect ratio is one). All metric values must be given in the
common reference coordinate system that is establish by the calibrations. The 0|0|0 location
is defined by the center of the volume if this coincides with the calibration
The group Reconstruction setup is used for setting up the different reconstruction meth-
ods MLOS, SMART and MTE.("Voxel Reconstruction" on page?926)
PAGE | 852
PAGE | 853
The second pane is used for Volume Pre-processing the voxel volume before calculating
particle positions in the PTV.
Here one can access the setting for the amount of blurring of the volume. This is achieved by
several iterations of blur by a 3x3x3 gauss kernel.
The third pane is dedicated to the PTV  processing.
Particle
threshold
Defines the amount of grey values cut away from the reconstructed voxel space 1%
means, that the darkest 1% of the grey values in the voxel-space is set to 0
Approximate
mm per
pixel:
Gives the ideal ratio for each voxel, so that the used estimate the particle dis-
placement in mm.
X-Y-Zsearch    Defines a search radius in mm around the location of a particle centroid in time step 1.
The location of the same particle in time step 2 must be within in this search radius
Min. particle
volume
(voxels)
Defines the size of used voxels for a single reconstructed particle that will be detected
as a valid particle. Note that if a particle image is 3x3 pixels, its size in voxel will be at
least 3x3x3 voxels, depending on your camera amount and orientation. The more
voxels the smaller the possibility of detecting ghost particles as true particles, since
they are normally smaller in size
Minimum
time steps
per track
Defines the duration of time steps the software tracks one distinct particle. The larger
the number is, the less likely the tracked particles will be ghost intensities. Because
the usually disappear after 3-6 time steps.
Track display showing only the tracks.
PAGE | 854
PAGE | 855
Track display showing only the vectors
Track display showing vectors of absolute size
Track display showing vectors scaled by the vector velocity
The display options for the 3D display:
PAGE | 856
PAGE | 857
From the display options one can enable or disable, as well as set the thickness / size of the
tracks and arrowheads.
The track representation can be chosen from the drop down menu, as either line, ribbon or
tube representation
Using the Relative Arrowhead scale sets the size of the arrowhead proportional to the velocity
of the particle in the track. Use this to suppress particles that are slow moving or stationary.
If one wishes to have to view only a sub section of the tracks, the subsection can be set by
choosing the start frame as well as the track length. When animating the tracks, the track
length will be used as the maximum track length of the animation. Please note that in case of
double images, the path length cannot be selected, due to the fact that only one timestep is
available.
Under the color options one can set the information to display by use of the color of arrow-
heads and track, one can also choose to set a fixed user defined color.
16.95.2 Working principle:
The tomographic PTV first searches for particles within a voxel space, these found particles
centroid positions are stored in a particle list for all time steps. Now the matching of the
particles start, during this phase the particles from the 1st time step are trying to be matched
with their consecutive positions in the next time steps.
Therefore, the so called nearest neighbor approach searches the closest position of a
centroid in the next frame. This is considered to be the position of the particle in the next time
step. This fact also defines the maximum seeding density where this technique can be used.
Because it is crucial, that distance between the particles is larger than their displacement.
Otherwise the nearest neighbor criteria will not work properly.
For the 3rd time step now one can calculate a position where the particle moved. This cal-
culation is based on the velocity information. Of course this area is only an estimation, and the
software searches within a given radius around this location. For the 4th time step, it is now
possible to even take the acceleration into account to calculate location of the particle. This is
why the procedure is called 4-frame approach.
(See J. Kitzhofer, C. Brucker and O. Pust. Tomo PTV using 3D Scanning Illumination and Tele-
centric Imaging. In: Proceedings of the 8th International Symposium on Particle Image Veloci-
metry - PIV09. Melbourne, Victoria, Australia, August 25-28, 2009.)
16.96 Track Smoothing
Similar to PIV, PTV algorithms need some form of outlier detection and data smoothing. There-
fore the function “Track Smoothing” can be used for time resolved tracking data in 2D and 3D.
3D Tomo PTV result in a silicon model of an aortic root:
Before smoothing/filtering                                   After smoothing/filtering
16.96.1 Inputs
The Track smoothing function can be applied on any time resolved tracking data that was
recorded in single frame mode. It is a smoothing function for 2D tracks as well as 3D tracks cal-
culated from the methods:
PAGE | 858
PAGE | 859
l  Time-resolved PTV (2D Method) [Link to help File]
l  3D PTV (3D Method) [Described in "Volumetric Velocimetry (Legacy method)" on
page?925]
l            3D Tomographic Particle Tracking Velocimetry (3D Method) [See "Tomographic
Particle Tracking Velocimetry" on page?848]
16.96.2 Outputs
The output dataset is a PTV data set with filtered and smoothed tracks.
16.96.3 Settings and how  to
The function Track smoothing filters tracks in terms of their length and it can apply a B-Spline
or an Overhauser spline filter.
Minimum Track length: Defines the number of time steps each specific particle is tracked. In
this example each specific particles is tracked over at least 17 time steps. Every track that is
shorter will be removed.
Algorithm: Defines the spline function used for the filtering. Here the Overhauser spline is
used.
Blend: Determines the number of sampling points used for the smoothing.
16.97 Two-Color  Pyrometry
16.97.1 Introduction
Two-Color Pyrometry is a technique to measure temperature, where image pairs are
acquired with two different band-pass filters in front of the two camera views so that a dif-
ferent spectral range is imaged on each of the two images. A temperature dependent prop-
erty can then be created by calculating the ratios of the individual pixel values between the
two recorded signals.
To make a correct ratio, it is important to achieve pixel-to-pixel overlap of the fields-of-view of
the two image sets. This is achieved by spatial calibration with a calibration target, followed by
image dewarping.
To perform a temperature calibration, one must first acquire a set of images of an object at
different temperatures, where the temperatures are well known (and ideally homogeneous
over the field-of-view of interest). The temperature values need to be added as a Custom
PAGE | 860
PAGE | 861
Property to each of the data sets. The ratio between the two signals is dependent on tem-
perature.
16.97.2 Overview  of the procedure
In this section the different steps for a successful Two-Color Pyrometry measurement are lis-
ted.
Spatial calibration step:
l  Spatial calibration based images of a calibration target. This step is performed separately
and using the Multi Camera Calibration
Two-Color Pyrometry Calibration (Temperature) steps:
l  Image dewarping ? of the image pairs for temperature calibration
l  Image ratios ? of the dewarped image pairs for temperature calibration
l  Temperature calibration ? using the ratio images and the temperature information in Cus-
tom Properties to create a calibration curve
Two-Color Pyrometry Processing steps:
l  Image dewarping ? of the image pairs of experimental data, using the above spatial cal-
ibration
l  Image ratios ? of the dewarped image pairs of experimental data
l  Temperature processing ? using the ratio images as well as the temperature calibration
above, to calculate the final temperature result
The different steps for Temperature Calibration and Processing respectively, are done in dif-
ferent tabs, where each tab also has a preview of the images showing an example of the res-
ult in each step.
NOTE: The image pairs can be acquired using either two cameras and image intensifiers or a
single camera and intensifier in combined with a DualScope from Dantec Dynamics.
16.97.3 Spatial calibration
As input to the Two-Color Pyrometry Calibration, which has a built-in dewarping function in
order to ensure pixel-to-pixel overlap of the two camera views, a spatial calibration is
required. So for each of the two camera views, acquire a number of images of a calibration tar-
get and save them for calibration. To perform the camera calibration, use the Multi Camera
Calibration for each of the two image views individually. Automatic calibration should be used
if possible, but also Semi-automatic and Manual options should be available.
For further information on the spatial calibration, please see the help text for Multi Camera
Calibration.
16.97.4 Two-Color Pyrometry Calibration
To perform a full temperature calibration using the Two-Color Pyrometry approach, the user
needs to acquire a number of image ensembles (minimum 2) with well-known temperature,
for each of the two camera views. These should be saved for calibration. Once the data is
saved for calibration, the user needs to right click on the image ensembles, select Customer
Property and add Temperature to the ensembles. Now Temperature will be found in the
Record Properties for each ensemble, and here the correct temperature for each ensemble
needs to be typed in.
Once the data is prepared as described above, everything needed to enter the Two-Color
Pyrometry Calibration recipe is now available. Preselect the two spatial calibration files from
the Multi Camera Calibration (one for each of the two camera views), and the temperature cal-
ibration image ensembles that include temperature information in customer properties.
Note that you need to select ensemble pairs (one ensemble for each camera view), and thus
an even number of ensembles is required.
PAGE | 862
PAGE | 863
Right-click on one of the image ensembles and select Calibrate to select calibration method,
and pick Two-Color Pyrometry Calibration. When you enter the recipe, you will see the three
tabs for Image Dewarping, Image Ratio and Temperature Calibration. The functions in each of
the tabs are explained below.
1. Image Dewarping
In the Image Dewarping tab the two camera calibrations are used to dewarp the temperature
calibration images to achieve pixel-to-pixel overlap between the two camera views. In the Pre-
view area, an example of the resulting dewarped images are displayed overlaying one
another and half transparent. If there are any static structures in the images, this preview can
also serve as a check of whether the pixel-to-pixel overlap has been achieved. By using the
slider bar at the bottom of the Preview section, the user can select from which temperature
ensemble the preview should be shown.
Note that only the area where the image pairs overlap are taken into account in the cal-
ibration. Where the image views do not overlap, according to the spatial calibration files,
there will only be pixels from one of the camera views, and not from both camera views. Thus
calibration is not possible, and this area is disregarded.
As default, the entire image region is calibrated pixel-by-pixel. However, the user also has the
option of selecting a smaller region to be calibrated. This is done by clicking Enable in Cal-
ibration Region, and then typing in the pixels for corner position, width and height of the cal-
ibration region, or alternatively use the mouse in the preview area to move the rectangle
around that defines the calibration region.
PAGE | 864
PAGE | 865
If a minor error in the dewarping is noticed, ideally the Multi Camera Calibration should be car-
ried out once again. However, it is also possible to correct for a small displacement also in the
Two-Color Pyrometry Calibration. This is achieved by using enabling Pixel Shift in the Image
Dewarping tab. This allows the user to define a small shift of one of the two image sets in
order to correct this error.
It should be noted that if Pixel Shift is enabled, the calibration will no longer be done for each
individual pixel pair, but rather on the average pixel values in each of the images in an image
pair.
2. Image Ratios
In the Image Ratio tab, the next step in the temperature calibration procedure is taken,
namely calculating the ratio, pixel by pixel, between the two images in each image pair. In the
Preview area the resulting ratio image is now displayed, for the region defined in the pre-
vious step. As in the previous tab, by using the slider bar at the bottom of the Preview section,
the user can select from which temperature ensemble the preview should be shown. It’s also
possible to selection what to show in the Preview area, using the drop down menu at the
lower right corner. Either the Ratio, of the image from Camera 1 or Camera 2 can be selected.
Divisor camera
It’s possible to switch between calculating the ratios “Camera 1/Camera 2” and “Camera 2/Cam-
era 1”. This is done by simply selecting which of the two cameras should be the divisor, by pick-
ing that camera from the drop-down list.
Median filter
It’s also possible to apply a Median filter to the images before calculating the ratio. This can
reduce the noise in the images, providing a cleaner result, if the images suffer from noise.
The size of the Median filter kernel is selected in the drop-down list.
Region-of-interest
If it is not possible during the calibration procedure, to achieve a well-known calibration tem-
perature that fills the imaged area entirely, then the user has the option of defining a Region-
of-Interest within the imaged area in which the temperature is well-known. From this region,
the pixel values are then averaged and new temperature calibration images are created
where all the pixels have this average value. This means that even though the region-of-
interest with well-known temperature may be much smaller than the Calibration Region
defined in the previous tab, the calibration will still be done for the entire Calibration Region.
This also means that since there is no calibration temperature information outside the Region-
of-interest, the calibration will be done using the average pixel value within the region-of-
interest for all pixels in the Calibration Region. Thus the pixel-to-pixel calibration will be lost
and may result in less accurate calibration. However, this function should be seen as a help
when it is not possible to achieve a homogeneous and well-known temperature in the entire
Calibration Region of the images.
To use this function, the user first needs to Enable it, and then define the region where the
relevant temperature information is found. This is done either by typing in the position in the
table, or using the mouse to drag the region to the correct size and location.
3. Temperature calibration
In the Temperature Calibration tab the average calibration curve is displayed, relating the
gray level values in the ratio images to temperature.
Image mean
It’s possible to select whether the final result should be calculated by using the image means
for the different temperatures only, creating a calibration curve as seen in the preview, or
whether the final result should also take the information in the individual temperature images
into account so that the final result also includes error bars showing the spread of the fluc-
tuation from image to image.
Scalar range
As default, temperature calibration is carried out over the entire temperature range in which
there is input data. However, it’s possible for the user to change the “Scalar range”, so that
the calibration curve includes a wider or narrower range.
It’s possible to go back to previous tabs in order to alter the settings if needed. The displayed
calibration curve will be updated accordingly. Once all the settings are made, click on OK or
Apply to carry out the calibration.
PAGE | 866
PAGE | 867
16.97.5 Two-Color Pyrometry Processing
Using a Two-Color Pyrometry Calibration dataset together with Two-Color Pyrometry exper-
imental images allows you to determine the spatial distribution of the temperature in the
experiment.
When performing Two-Color Pyrometry experiments most of the work lies in careful cal-
ibration. When performing the actual measurements you need to make sure to have a valid
Two-Color Pyrometry temperature calibration. Once the calibration is taken care of, the actual
processing of the acquired images is relatively straightforward.
Preselect the Two-Color Pyrometry Calibration dataset and then select the ensemble pairs
you which to process, containing the two camera views of the experiment, with images at the
two different spectral ranges. Right click on any of the data sets, select Analyze, choose the
category Two-Color Pyrometry, and then select Two-Color Pyrometry Processing and click OK.
When entering the processing recipe, three tabs are found just like in the calibration recipe.
1. Image Dewarping
In the Image Dewarping tab the two camera views are dewarped using the same Multi Cam-
era Calibration files as used for dewarping the temperature calibration images, again to
achieve pixel-to-pixel overlap between the two camera views. In the Preview area, an
example of the resulting dewarped images are displayed overlaying one another and half
transparent. By using the slider bar at the bottom of the Preview section, the user can select
from which temperature ensemble the preview should be shown.
PAGE | 868
PAGE | 869
NOTE: The Multi Camera Calibrations are not needed to be preselected as input for this pro-
cessing step. This information is instead part of the Two-Color Pyrometry Calibration.
Now the image area for which the calibration was carried out is also shown in the preview dis-
play as a rectangle. As default the user can’t change this area in the Processing recipe, since
the calibration is carried out pixel-by-pixel, and are therefore only valid for the pixels within
the calibrated area in the rectangle. In most cases this will also correspond to the area in
which you have the experimental data, and the user can simply go on to the next tab without
making any changes in the Image Dewarping tab.
In this example however, there is a discrepancy between the calibrated area and the location
of the experimental data of interest. As can be seen in the image above, there is also a slight
horizontal shift between the two camera views. The most accurate way of dealing with these
issues is to go back and to the Multi Camera Calibration and Two-Color Pyrometry Calibration
all over again. However, there is a faster way to process, and this will be explained below.
As default, the calibration is made on an individual level for each pixel. This means that each
pixel has its own individual calibration curve. This in order to make a very detailed calibration,
taking into account the slight differences in light sensitivity and noise for each pixel. This also
means that, as the calibration is specific to the pixels in the calibration region, it is not mean-
ingful to apply the calibration to a different part of the camera sensor since then the pixel cor-
respondence is lost. Therefore, as default, the processing can only be done for the same
region as defined in the calibration.
However, by enabling “Use pixel mean calibration” all the pixel values in the calibration region
are averaged, so that only the mean value is used for the calibration. The pixel specific inform-
ation is then lost, but on the other hand it makes the calibration less noise sensitive. As the
pixel specific information is no longer included, this allows for the calibration to be applied to
any user-defined part of the camera sensor / image view, and the user now has the option of
enabling two new functions to do this.
Calibration region
When enabling Calibration region, the user can define a new region in which the processing
will be made, and thus to which the pixel mean calibration will be applied. This is done by typ-
ing in the pixels for corner position, width and height of the calibration region, or alternatively
use the mouse in the preview area to move the rectangle around that defines the region.
Pixel shift
In this example there is also a horizontal shift by 10 pixels between the two camera views. As
Two-Color Pyrometry is a ratiometric technique, a correct pixel-to-pixel overlap between the
two camera views is crucial. With the Pixel shift function, the user can adjust the shift, vertically
and horizontally, to correct for slight misalignment.
PAGE | 870
PAGE | 871
2. Image Ratios
In the Image Ratio tab, the Image ratio is calculated, pixel-by-pixel, between the two meas-
urement images in each image pair. In the Preview area the resulting ratio image is now dis-
played, for the region defined in the previous step. As in the previous tab, by using the slider
bar at the bottom of the Preview section, the user can select from which temperature
ensemble the preview should be shown. It is also possible to select what to show in the Pre-
view area, using the drop-down menu at the lower right corner. Either the Ratio, of the image
from Camera 1 or Camera 2 can be selected.
Divisor camera
Here it is shown which camera is the divisor in the ratio calculation. This has been chosen dur-
ing the calibration procedure, and can’t be changed during the processing.
Median filter
It is possible to apply a Median filter to the images before calculating the ratio. This can reduce
the noise in the images, providing a cleaner result, if the images suffer from noise. The size of
the Median filter kernel is selected in the drop-down list.
3. Temperature Processing
In the Temperature Processing tab the resulting temperature map is displayed, relating the
gray level values in the ratio images to temperature.
Clamp to calibration limits
As a default, the temperature result shows the entire temperature range it which there is
input data. However, it is possible for the user to clamp the result to the calibration limits. If
enabled, temperatures above and below the calibration limits will be set to the maximum and
minimum temperature, respectively.
It is possible to go back to previous tabs in order to alter the settings if needed. The displayed
result will be updated accordingly. Once all the settings are made, click on OK or Apply to carry
out the full processing of all preselected images.
PAGE | 872
PAGE | 873
16.98 Universal  Outlier  Detection
The Universal Outlier Detection analysis is used to detect and optionally substitute false vec-
tors based on a normalized median test using the surrounding vectors. The technique is well
known as the Universal Outlier Detection for PIV data, and was presented by Westerweel &
Scarano in Experiments in Fluids 2005. It was proven that a small adaptation to the standard
algorithm for the median test by introducing a single threshold to the normalized vector resid-
uals makes the algorithm tolerant (universal) to a variety of different flow conditions and char-
acteristics.
The adaptation to DynamicStudio provides a rectangular vector neighborhood defined by a
uneven MxN number of vectors. The number of surrounding vectors included in the
algorithm depends on the location of the displacement vector in the data set. In the 3 fol-
lowing examples, using a 5x5 vector neighborhood, the displacement vector is in the corner
of, on the edge of, and well inside the data set respectively.
a)                                                   b)                                                   c)
As it can be seen when the displacement vector is in a corner a)?only 8 neighboring vectors
are included in the median calculation. Whereas if the displacement vector is on the edge of
the data set b)14 surrounding vectors are used, and when inside the data set c) 24 = 5x5 - 1
are used. Note that in none of the cases the displacement vector itself is used in the cal-
culation.
The algorithm also ignores previously rejected vectors. If the displacement vector is a rejec-
ted vector, it is left unchanged. If one or more of the neighboring vectors are rejected vec-
tors, these are ignored when calculating the median. If no surrounding vectors can be used in
the median calculation, the displacement vector is left unchanged.
The normalized vector residuals can be calculated as:
, where
is the displacement vector,
is the median vector calculated using the neighborhood vectors:
,
is the median residual calculated using the neighborhood residuals:
, where
for
and
is the minimum normalization level.
If the normalized vector residual       is above the acceptance threshold, the displacement vec-
tor        is considered invalid. If the user has chosen to replace invalid vectors the software will
first look at the second, third and fourth highest peaks in the correlation map. If any of these
match the neighbors the vector will be changed accordingly and remain valid. If not the vector
is substituted by the neighborhood median vector         and flagged as 'Substituted'.
In the recipe dialog for the Universal Outlier Detection analysis the neighborhood size, the
detection threshold and the minimum normalization level can be specified. The output can
either be validated and the invalid vectors rejected or the invalid vectors can be substituted
by the median vector calculated using the neighborhood vectors.
PAGE | 874
PAGE | 875
Example of the result of a Universal Outlier Detection analysis:
a)?Standard cross-correlation PIV output. From the image it can be clearly seen that some of the
vector are expected to be invalid.
b) The same data with the Universal Outlier Detection analysis applied. The red vectors indicates
that the vector is rejected by the filter. Using the default settings of the Universal Outlier Detection
most of the expected invalid vectors are found.
PAGE | 876
PAGE | 877
c)?The same data with the Universal Outlier Detection analysis applied. The green vectors indic-
ates that the vector is substituted by the filter. The same vectors which are invalidated are now
replaced by the median vector calculated using the neighborhood vectors.
Universal Outlier Detection can be applied to a vector map as a separate analysis method, but it is
also used in Adaptive PIV to validate vector maps between each pass in the iteration.
16.99 UV  Scatter  plot Range  Validation
Any vector map can be validated against a user defined expected range of velocities. This is
done by selecting the ensemble containing the vector maps in question and then select the
analysis method 'UV Scatter plot Range Validation' in the category 'PIV?Signal'.
The recipe of UV Scatter plot Range validation plots the vectors in a vector map in a XY scatter
plot, where the X Axis is U?component (pixels) and the Y Axis is the V component of the vec-
tors.
Suitable limits are often determined best by simple trial and error. Try various values, press
'Apply' to see how they affect the vector maps and when you're satisfied press 'OK' to validate
the remaining vector maps in the parent ensemble.
It is possible set the limits either by typing in the values or by dragging a rectangle in the plot.
Here's an example of a vector map that has been validated with the settings above: Inval-
idated vectors are color coded in red, while the rest remain blue to indicate that they are
deemed valid vectors:
Please note:
UV Scatter plot Range Validation does NOT substitute invalid vectors with an estimated guess
for the correct velocity. To do this you need to apply yet another validation method such as
Moving Average Validation.
16.100 Vector  Arithmetic
As indicated by the name, the anlysis method 'Vector Arithmetic' enables the user to e.g. sub-
tract a velocity vector from a velocity vector map. The result is another vector map, which can
be examined and/or used for further processing. The analysis method can be applied to both
3-D and 2-D?vector maps as well as Scalar maps (which can be considered 1D-vector maps).
You can add, subtract, multiply or divide a fixed vector from all vectors in your vector map(s)
or you can apply the analysis with another dataset as operand. The latter is commonly used to
f.ex. subtract the mean vectors from instantaneous vectors in order to reveal f.ex. vortices
conveyed with a bulk flow...
This requires that you first identify the (mean) vector map that you wish to subtract from the
other vector map(s):
To do this you must of course calculate the mean velocity vector using 'Vector Statistics'. Hav-
ing done that right-click the Vector Statistics and choose 'Select' from the context menu.
Alternatively left-click the statistics while pressing the Ctrl-key. Either way a small checkmark
will appear beside the vector statistics icon indicating that this dataset has now been selected
for use in an upcoming analysis.
PAGE | 878
PAGE | 879
You can now return to the ensemble containing the vector maps from which you wish to sub-
tract the mean. Right-click it, select 'Analyze...', and in the resulting dialog choose 'Vector Arith-
metic' in the category 'Vectors and derivatives':
In the resulting analysis recipe you can choose to either subtract a fixed vector or subtract
another vector map. If you've previously selected another vector map such as the mean vec-
tor map as outlined above this option will be chosen by default and the name of the ensemble
will be listed as shown below
If you did not pre-select another vector map for subtraction you will only have the option of
subtracting a fixed value, which you can enter in the lower half of the recipe.
Performing the vector subtraction will give you a new ensemble of vector maps all calculated
as the original vector maps minus either a fixed vector or a chosen vector map.
As you can see from the recipe, Vector Arithmetic can do more than subtracting values, it can
also add, multiply or divide and combine with other datasets and/or constant values as oper-
ands.
Not all combinations of input (parent) data and operator are possible or meaningful. Possible
combinations are listed in the table below:
Input (Parent)?dataset
Scalar                2D Vector             3D Vector
Operand
(constant or
other data-
set)
Scalar              +       -       *      /      +       -       *      /      +       -       *      /
2D Vector         +       -       *      /      +       -       *      /      +       -       *      /
3D Vector         +       -       *      /      +       -       *      /      +       -       *      /
Operations with a green background color are possible, while operations with a red back-
ground color are not allowed. Arithmetic operations with a yellow background color are pos-
sible, but not always meaningful. Often they will only be relevant with constant (i.e. user
defined) operator values, while using another dataset as operator in these cases will typically
be of very limited use.
Resulting datasets will generally be of the same type as the parent (i.e. Scalar - Scalar, 2D - 2D
&?3D - 3D).
If you attempt one of the operations marked with red in the table above, you will get an error
message, explaining that there is a mismatch of dimensionality between the Input and Oper-
and dataset. Similarly you will get another error message if the input and operator dataset
does not have the same size (i.e. different number of vectors horizontally and/or vertically).
16.101 Vector  Dewarping
In PIV and most other measuring techniques based on light-sheets and cameras, it is assumed
and/or required that the camera is oriented normal to the light-sheet.
In many experiments this is however not feasible, either because of restricted optical access
to the experimental setup or because the camera would thereby disturb the flow-field under
investigation. In such experiments measurements may have to be performed with an off-axis
camera, looking at the ligh-sheet at an angle instead of normal to it.
PAGE | 880
PAGE | 881
Images recorded with an off-axis camera will suffer from perspective distortion, meaning that
the scale factor is not constant, but varies across the cameras field of view. With numerical
models describing the perspective distortion ("warping"), it is however possible to com-
pensate and correct ("de-warp") the images themselves or (in the case of PIV) correct the vec-
tor maps derived from the warped images.
Due to perspective off-axis cameras generally cover a larger area of the flow-field than cor-
responding on-axis cameras, but instead of a square, each pixel covers an oblong trapezoidal
section of the flow-field, and this may cause loss of information due to smearing of features
within the cameras field of view. After dewarping of the image each pixel will again cover a
square section of the flow-field, but the information lost has not been recovered!
Similar considerations apply regarding the dewarping of vector maps.
For small off-axis angles (smaller than 30?-45?) the problem is small, but nevertheless it is
recommended to use on-axis cameras whenever possible.
If off-axis cameras cannot be avoided, keep the off-axis angle as small as possible.
16.101.1 Setting the z-value and w-value
Performing PIV-measurements with an off-axis camera it is possible to de-warp the images
prior to correlation, but dewarping vector maps is a possible (and usually faster) alternative.
The process is similar to the one used for dewarping images.
As for the dewarping of images the vector map is assumed to be recorded in Z=0 unless oth-
erwise specified by the user. Similarly nonzero Z-values should be entered in the Log Entry of
the vector map dataset properties, but beyond this you have the possibility to specify an over-
all W-velocity component (m/s).
It is well known from conventional PIV that flow through the light-sheet can severely disturb
the measurement of in-plane velocities, especially if the through-plane velocity component is
of the same or higher order of magnitude as the in-plane velocities. With increasing off-axis
angles PIV becomes even more sensitive to the effects of through-plane motion, but knowing
the through-plane velocity it is possible to predict and compensate for the resulting errors in
the calculated in-plane velocities.
When dewarping vector maps recorded with an off-axis camera you are therefore strongly
encouraged to enter a W-value even if all you have is an educated guess. If nothing is spe-
cified, the system will assume Z = 0 mm and W = 0 m/s.
Example
In this example the effect of through-plane motion has not been accounted for, so the two
vector maps appear similar, but there are a few important differences:
l  First of all the effects of perspective is clearly visible: Vectors in the original vector map
are positioned in a rectangular grid, where neighboring vectors always share the same x-
or y-coordinate, and both horizontal and vertical distance between neighbors are con-
stant. In the de-warped vector map this is no longer the case; Vectors are not random,
but positioned in a non-uniform trapezoidal grid, where neighbors share neither x- nor y-
coordinates, and where both horizontal and vertical distance between neighbors vary
across the area covered.
l  Secondly the imaging model used is responsible for a change from image to object
coordinates, so positions and displacements are measured in mm instead of pixels, and
velocities are measured in m/s instead of pixel/s. Note also that the origin has moved
from the lower left corner of the original vector map to the center of the de-warped vec-
tor map. The new origin corresponds to the position of the zero marker in the calibration
images used for the imaging model fit.
l  Despite these changes there is a one-to-one correspondence between vectors in each of
the two vector maps, meaning that information regarding for example vector status
codes (Valid, Rejected, Outside, etc.) is maintained, and vector validation methods can thus
be applied either before or after the dewarping depending on user preferences.
PAGE | 882
PAGE | 883
Vector map before and after dewarping (top and bottom respectively).
The non-uniform grid of vector positions may impede further analysis such as the calculation
of vorticity and/or streamlines, since most algorithms for analysis of PIV data are designed for
rectangular grids. To overcome this problem you may wish to resample the de-warped vector
map to get back to a uniform grid, but please remember to validate the vector map before
doing so.
A re-sampled vector is a weighted average of four neighboring vectors in the parent vector
map. Assuming for simplicity that these four vectors are weighted 25 % each, it is obvious that
an undetected outlier among them will be much harder to detect in the re-sampled vector
map than it was in the original vector map.
16.102 Vector  Interpolation
Vector interpolation takes as input a mask and a vector dataset and reconstructs (interpolates)
vectors that overlay selected regions in the mask. The method of Thin Plate Splines (TPS) is
used to interpolate data. TPS is an algorithm for interpolating and/or fitting 2D data. As the
name implies, TPS essentially takes as input 2D data and "bends" a flat plate until all the points
pass through it. In the event of too much noise and the possibility of singularities, the user can
apply "relaxation". Zero relaxation forces the plate (surface) to pass through all the input
points, a large value reduces the result to a least squares approximation.
Interpolation is controlled by the following two parameters:
1.   Radius: the distance in pixels about a point of interest when collecting data points for an
interpolation.
2.   Relaxation: the degree of relaxation, that is, requirement, that all points pass through the
resulting surface created during interpolation.
Application of the mask above results in the following given vector map. Interpolated vectors
are marked as substituted (green).
PAGE | 884
PAGE | 885
Overlay comparing original and interpolated vectors.
16.103 Vector  Masking
This method is used to mask velocity vectors in user-defined regions of a vector map. Note
that the vectors are not changed or removed from the map but simply tagged with status
code 'Outside', 'Disabled' or 'Rejected'. Based on the status code, these vectors can then be
hidden from the vector map display and/or excluded from further analysis.
To apply masking you must first define a Mask, using either the analysis method "Define
Mask" (on page?474) or a regular single-frame image with the Custom Property 'Mask'
enabled (See"Custom Properties" (on page?419)). The mask ensemble must contain either one
static mask or N dynamic masks, where N equals the number of vector maps to be masked.
Dynamic masks are often derived from the same parent images as the vector maps e.g. using
the "Image Processing Library (IPL)" (on page?525), but please note that vector masking
require single-frame masks.
If you use regular images for masking, nonzero pixels in the Mask image will identify regions
in the vector map that are to be left untouched, while Zero-valued pixels in the Mask image
identify regions where vectors will be tagged 'Outside'. It is the vector location (i.e. center of
Interrogation Area) that determine whether or not the vector is masked no matter if the cor-
responding IA extend into masked areas.
To apply a mask to a vector map, pre-select the mask (See "Selection (Input to Analysis)" (on
page?419)) and then select the ensemble containing the vector maps to be masked. Look for
Vector Masking in the category ’Masking’.
No matter what kind of mask you use, the vector masking recipe is the same and has no set-
tings or options:
The resulting masked vector ensembles are labeled with the icon       and thus differ visually
from the parent vector ensemble icon      .
The following examples are based on a top-down view into a square water tank with a mag-
netic stirrer at the bottom. The light sheet is horizontal and just above the spinner. In each
image the (static) tank walls can be seen as well as the (moving) spinner at the bottom of the
tank.
PAGE | 886
PAGE | 887
Using "Define Mask" (on page?474) we can create a mask to identify and hide the noisy vectors
from outside the tank:
...please note there is only one mask, which is applied to each of the parent vector maps suc-
cessively.
Using the "Image Processing Library (IPL)" (on page?525) we can create a series of masks to
remove vectors overlapping the spinner:
PAGE | 888
PAGE | 889
...please note there are 20 masks here, one for each of the parent vector maps. The parent
image and thus the 'Derived Mask' are both double-frame and thus cannot be used directly
for vector masking. Therefore we extract a single frame mask using "Make Single Frame" (on
page?663).
To remove both (static) walls and (dynamic) spinner we can apply the two masking operations
successively or we can merge the two masks into a hybrid mask by applying the static mask to
the dynamic ones (Using "Image Masking" (on page?508) with the 'Black-out areas' option):
...applying these hybrid masks to the vector maps we can remove both walls and spinner:
As stated above Vector Masking tags invalid vectors with a status code 'Rejected', 'Outside' or
'Disabled', but does in fact not remove or change any vectors.
In the examples above vector map display options have been set to hide the masked out vec-
tors:
In the default vector display all vectors are shown in which case the masked vector map could
look something like this:
PAGE | 890
PAGE | 891
16.104 Vector  Resampling
As indicated by the name, this method re-samples velocity vector maps by interpolating
between neighboring vectors. A re-sampled vector maps is labeled with the icon       and, if
needed, it can be edited, but changes in the data-sheet will be updated on the vector map.
Typically, re-sampling is done to refine the spatial resolution of the velocity vector map(s). The
method is flexible enough to enable vector map "dilatation" too (which would lead to a loss of
resolution) to e.g. match CFD grids.
To use this method, select the map(s) of interest and look for the method named 'Resampling
of vector map' in the Coordinates?? category.
Select then the appropriate option:
l  Automatic re-sampling of vector map
l  User-defined re-sampling of vector map
16.104.1 Automatic re-sampling
With automatic re-sampling, the user just needs to define the over-sampling factor. Velocity
vector maps are spatially refined when this factor is greater than 1.00, whereas spatial res-
olution is lost when this factor lower than 1.00 (see examples below).
By default, the grid is square but non-isotropic re-sampling can be made if necessary by enter-
ing X- and Y- over-sampling factor values.
PAGE | 892
PAGE | 893
Example of velocity vector map re-sampled with (Bottom, left) (2 x 2) over-sampling factor and
(Bottom, right) (?, ?) over-sampling factor. The top image shows the reference velocity vec-
tor map.
16.104.2 User-defined re-sampling
User-defined re-sampling grid can be applied too: specify the region of the map to consider in
the (X, Y), (Min, Max) boxes and enter the step size desired. For anisotropic re-sampling,
uncheck the 'Square grid' option and give a value to the Y-step size.
Example of velocity vector map re-sampled manually on the region [(50.5, 50.5);(500.5;
700.5)] with anisotropic (2 x 3) grid step-size. (Left): Reference vector map and (Right): Re-
sampled vector map.
16.104.3 Edit data
To further access raw data of the refined velocity vector map calculated, open the vector map
and select the menu 'Open as numeric'.
PAGE | 894
PAGE | 895
Move the cursor of the mouse over the top, left cell and click the right button of the mouse to
get 'Display options' and 'Export as file' capabilities. Comparison with the velocity data of the
raw vector map can be made easily by opening the data-sheet of this velocity vector map.
In this example, the first 2 columns give the position in CCD pixel coordinates whereas the
next 2 other columns give the U- and V-components (in m/s) of the velocity at the selected pos-
itions.
16.105 Vector  Rotation/Mirroring
This method is used to rotate or mirror vector maps.
To rotate a vector map, look for the method 'Rotate / Mirroring' in the category 'Coordinates'
and select the option(s) of interest; i.e.
l  Rotate: of 0?, 90?, 180? or 270?
l  Mirror around Y- or/and X-axes
The result vector map is then labeled with the icon       , clearly showing that a rotation/mirror
has been applied to the raw vector map (      ) or masked vector map (      ). Note that images
can be rotated (by any degree) as well using the 'Rotate' method of the Image Processing
Library module.
16.106 Vector/Scalar  subtraction
As indicated by the name, the Vector/Scalar subtraction method enables the user to subtract
a vector or scalar from a scalar or vector map. The result is another vector/scalar map, which
can be examined and/or used for further processing.
In the following the method is described and applied to a vector map, but it can be applied to a
scalar map in exactly the same manner.
You can subtract a fixed vector from all vectors in your vector map(s) or you can subtract one
vector map from another. The latter is commonly used to f.ex. subtract the mean vectors
from instantaneous vectors in order to reveal f.ex. vortices conveyed with a bulk flow...
This requires that you first identify the (mean) vector map that you wish to subtract from the
other vector map(s):
To do this you must of course calculate the mean velocity vector using 'Vector Statistics'. Hav-
ing done that right-click the Vector Statistics and choose 'Select' from the context menu.
Alternatively left-click the statistics and press the Space-key. Either way a small checkmark will
appear beside the vector statistics icon indicating that this dataset has now been selected for
use in an upcoming analysis.
You can now return to the ensemble containing the vector maps from which you wish to sub-
tract the mean. Right-click it, select 'Analyze...', and in the resulting dialog choose 'Vect-
or/Scalar Subtraction' in the category 'Vector & Derivatives':
PAGE | 896
PAGE | 897
In the resulting analysis recipe you can choose to either subtract a fixed vector or subtract
another vector map. If you've previously selected another vector map such as the mean vec-
tor map as outlined above this option will be chosen by default and the name of the ensemble
will be listed as shown below
If you did not pre-select another vector map for subtraction you will only have the option of
subtracting a fixed value, which you may choose to have the system calculate for you as the
mean of all vectors in the instantaneous vector map.
Performing the vector subtraction will give you a new ensemble of vector maps all calculated
as the original vector maps minus either a fixed vector or a chosen vector map. An example is
shown below:
Instantaneous vector map.
PAGE | 898
PAGE | 899
Temporal (ensemble) average vector map.
Instantaneous vector map minus the ensemble mean.
If the size of the 2 vector maps is not identical, DynamicStudio will issue an error message and
stop all further calculations. Most likely this is a mistake, but if not you may overcome the prob-
lem by resampling one of the vector maps so it matches the size of the other one. Refer to
the help file of the Re-sampling of vector map method (found in the Coordinates category) to
get further information on this calculation.
Note
'Vector/Scalar subtraction' is a legacy analysis method and we recommend the use of 'Vector
Arithmetic' instead. ("Vector Arithmetic" on page?878).
16.107 Vector  Statistics
As indicated by the name, the Vector statistics method calculates statistics from multiple velo-
city vector maps. Graphically results are presented as a vector map of mean velocity vectors,
PAGE | 900
PAGE | 901
but a lot of other statistical quantities are calculated as well. These can for example be
accessed via the numerical display and include mean velocities, standard deviations, variances
etc. For each position (i.e. interrogation area) in the vector map, the number of vectors
included in the statistical calculations is also stored.
Vector statistics data are labeled with the icon       to be easily located later.
To use the Vector statistics method, select an ensemble containing at least 2 vector maps of
equal size. Vector statistics supports 2D-2C, 2D-3C (stereo) and 3D-3C PIV data.
In the category Analyze/Statistics, look for the method called Vector Statistics and select it.
Among the 3 different options available, select the one corresponding to your needs:
l  The All vectorsmethod includes all the vectors calculated (i.e. valid, invalid and substituted if
any calculated by the PIV algorithm used) for each interrogation area.
l  The All valid vectorsmethod excludes invalid vectors.
l  The All valid, non-substituted vectors method includes only valid vectors and is thus the
most stringent options. This is the default method.
Press the Apply button to calculate vector statistics and view the results (Click on the Display
button to access further visualization methods.) and OK to accept the calculation.
Vector Statistics from PIV measurements on a jet. Vectors show mean flow, scalar map is tur-
bulent kinetic energy.
A Vector statistics box always appears on the resulting map. This box (which can be resized and
moved over the map using the mouse) contains statistical information at every interrogation
area. The format of the data can be modified too: When the mouse hover over the box, right
click and select the data format wished. (Select the Hide option to remove this box from the
map.)
PAGE | 902
PAGE | 903
16.107.1 Visualization  methods
With the mouse, double-click on the image (or Right click on the image with the mouse and
select Display option: scaling options, color codes and other advanced data representations
are now available. "Vector Map Display" on page?1004
16.107.2 Numeric data  display
To access raw data of the vector map calculated, open the map and select the menu Open as
numeric.
Move the cursor of the mouse over the top, left cell and click on the right button of the
mouse to get the Display  options.
The first columns show x-/y-coordinates for each vector. For all vector maps coordinates will
be available in mm or as a simple index number, while for 2D vector maps, coordinates will
also be available in pixels. The next columns show mean velocities. For all vector maps velo-
cities will be available in m/s, while for 2D vector maps pixel displacements are also available.
Next is the standard deviations on each of the velocity components (2 or 3) followed by TKE,
the Turbulent Kinetic Energy. Standard deviations are in m/s, TKE in (m/s)2 is computed from
the sum of variances. If all 3 velocity components are available the sum is scaled by 0.5, if only
2C data is available the third is approximated assuming isotropic turbulence and the sum is
scaled by 0.75. The following columns contain the Reynolds stresses (just one for 2C data,
three for 3C data) and correlation coefficients between the various velocity components and
finally Skewness and Kurtosis for each of them.
The column labeled N gives the number of vectors used to calculate the statistics. If for
example 20 vector maps were used for the statistical calculations N = 18 in the numerical dis-
play means that 2 of the 20 vectors were excluded from the statistics calculation, typically
because they were marked as invalid. The last column shows the status for each vector pos-
ition. The status is numerically coded, but can be shown as text by right-clicking inside the
numerical display and selecting 'Show Status as Text'.
Data format can be modified too so as to e.g. match a typical format when using the export
function. Read more about column formatting in "Numeric Display" on page?1048.
As stated above the turbulent kinetic energy is proportional to the sum of variances and sim-
ilarly the Reynolds stresses are proportional to the Covariance between the various velocity
components. But please remember that PIV produce velocity estimates as a spatial average
over the interrogation area/volume. This acts like a spatial low-pass filter and computed values
may thus be significantly lower than measured by other techniques or predicted from classic
fluid mechanics that operate in an infinitely small fluid element (i.e. a point).
16.107.3 Formulas used
Classic formulas for statistical quantities are used. Expressions below use symbols un and vn to
describe velocity samples, and corresponding formulas are of course used for U, V, W and
their combinations.
Mean velocity:
Variance:
Standard deviation:
Turbulent Kinetic Energy:
Reynolds Stress:
Correlation coefficient:
Skewness:
Kurtosis:
The Turbulent Kinetic Energy is computed using the first (exact) formula if all 3 velocity com-
ponents are available. If only 2 velocity components are available the second (approximated)
formula is used, assuming isotropic turbulence.
PAGE | 904
PAGE | 905
The formulas for Reynolds stress and Correlation coefficient is shown for velocity components
U &?V. Similar formulas area of course used to combine U-W and V-W in a 3C dataset.
A total of N data samples is assumed, so all sums are from n = 1 to n = N. Generally you would
expect N to match the number of vector maps used for the calculation, but the user may
choose to exclude vectors that have e.g. been identified as invalid. Consequently N may be
smaller than the # of vector maps for some of the positions in the resulting vector statistics
map.
For the calculation of variance and Reynolds stress please note the division with (N-1) instead
of N. These quantities are based on deviations from the mean, but the mean is itself derived
from the same data samples, so using N would bias results towards zero. For sufficiently large
values of N this will be of little importance, but with PIV you may have limited amounts of data.
Please remember though that reliable statistics typically require at least 20-30 samples that
are independent of one another. It is mathematically possible to calculate statistics on just two
samples, but results have little or no physical meaning.
16.108 Vector  Stitching
Vector map stitching merges vector maps of different sizes and locations into one large vec-
tor map. Basic requirements are: user?defined positions for each vector map and pre-
determined scale factors.
Select vector maps from the database tree menu. You can select to have the software auto-
matically determine the dimensions of the final vector map, or define a region and only vec-
tors that fall into this region are accepted.
Automatic determination of final vector dimensions: the software will determine the
dimensions of the output vector map on basis of the input ensembles.
Vector map dimensions: the user supplied minimum and maximum dimensions of the out-
put vector map. Any vectors outside these limits will be ignored.
Calculated area: the total area of all input vector maps.
Number of vectors: the total number of input vectors.
Above: Result of two vector maps of same size position adjacent to each other.
16.109 Volume  Grid
The Volume Grid Method quickly converts Lagrangian data from PTV to a regular Eulerian grid
based on a inverse distance interpolation.
PAGE | 906
PAGE | 907
TOMO PTV experiment to measure the flow in the silicon model of an aortic root; Particle
Tracks (left) and Transformed vectors (right).
16.109.1 Inputs
The Volume Grid Method works on all Time Resolved PTV data from 2D and 3D. Of course it
also can be applied on smoothed tracks.
16.109.2 Outputs
The output dataset is a 3D vector map that can be treated and post-processed as any 3D vec-
tor map.
PAGE | 908
PAGE | 909
16.109.3 Settings and how  to
The settings you can choose for the function volume grid are the number of vectors along the
x, y, and z-axis.
Number of vectors: x y z- Define the number of vectors you want to have along the x, y and z-
axis. Note that you should not oversample the Tracking data. That means if you have about
300 particles per time step, your volume should also not contain more than 300 vectors per
time step.
The volume in which the interpolation will be based on is defined by the positions of the
particle tracks itself. Hence the method will figure out the volume for the interpolation on its
own.
The velocities are calculated based on an inverse distance interpolation where a closer particle
the centroid of a vector position get a higher influences the value of the vector.
16.110 Volumetric  calibration  refinement
16.110.1 Introduction:
When performing Volumetric Velocimetry measurements a calibration is the key to success!
However, during or in between measurements it can occur that cameras lose their precise
alignment used during the initial calibration procedure. Even small misalignments will induce
calibration errors which must be considered.
One needs to distinguish between two different possible errors:
1.   Static errors: due to an accidental movement of one or several cameras
2.   Temporal calibration errors: due to vibrations or a change of the fluids refractive index
during a measurement.
Both of these will invalidate the initial calibration.
Static errors are often the result of a change in camera location or angle caused by acci-
dentally hitting a camera or the entire camera rig. Temperature variations can cause the rig to
shrink or expand, which leads to a misalignment of the cameras. Alternatively the water tem-
perature of your channel might have changed a few degrees, which results in a different
refractive index of the fluid compared to the initial calibration. This affects the line-of-sights
for each pixel, which can cause the calibration to become biased.
Temporal errors can occur in experiments where the test object induces vibrations or e.g. in
combustion applications where rapid changes in temperature or gas composition can occur.
In order to correct slight static changes of the line-of-sight between the calibration and the
measurements, the so called “Volumetric Calibration Refinement” or short VCR can be applied
to correct the calibration. This method is based on the initial calibrations and some acquired
particle images. The method is able to correct deviations from the original calibration, dis-
parity errors, of up to 7 pixels which is more than twice the typical particle image diameter.
Note: in combination with the “Volumetric Calibration Refinement” we recommended to reg-
ularly perform a proper and careful Multi Camera Calibration! Use the “3rd Order Polynomial
Method” as soon as there is any change of refractive index between the lens and the Field of
view. i.e. any type glass!
At the moment the only calibration model the Volumetric Calibration works with in Dynam-
icStudio is the 3rd order polynomial calibration model!
16.110.2 Theory of the Volumetric Calibration  Refinement.
The Volumetric Calibration Refinement consist of nine steps. A detailed overview of these
steps is found below. In short, the method uses particle reconstructions and their projections
back onto the camera sensor to compute the disparity between the recorded and the back-
projected particles. Based on this disparity the original calibration records are then corrected.
PAGE | 910
PAGE | 911
1.   Preliminary camera calibrations and a set of particle images are used (one from each cam-
era) to reconstruct a voxel volume with particle images. Ordinary background image pro-
cessing should be applied to make the image background black. Additionally, if the
disparity is large, an enlargement/blurring of the particle images can increase the chance
of a successful reconstruction even if current camera calibrations have a larger offset
than the typical particle diameter. Therefore, 3x3 Gaussian blur is applied for the iteration
and used until the maximum disparity errors for all cameras is below 1 pixel.
2.   Divide the voxel volume into possibly overlapping Interrogation Volumes (IVs). The IVs
are typically larger than normally used for 3D-LSM.
3.   Taking IVs one at a time and apply an (optional) spatial mask and project the IVs back to
each of the cameras using the current calibrations. This approach is very similar to the
SMART reconstruction process. In this particular case we’re not projecting the entire voxel
volume, but a sub volume thereof (the IV).
4.   From each of the back-projected IVs an Interrogation Area (IA) on the corresponding
image plane is created and a cross-correlation with the originally acquired image is per-
formed. The resulting correlation maps describe the disparity (error) of the
corresponding interrogation volume as seen from the camera being evaluated. The dis-
parity or correlation map from a single snapshot is expected to be very noisy, with the sig-
nal peak not necessarily higher than randomly located noise peaks. For better statistics
several recordings should be used with a lower seeding density (0.001 to 0.0025 ppp) to
avoid overlapping particle images.
5.   For different recordings repeat steps 1-4 above and add up the disparity maps over time
(similar to Average Correlation). If errors are indeed constant in time the ‘true’ signal peak
in the disparity map will remain in the same place every time, while noise peaks jump
around randomly. Adding up the disparity maps will thus increase the S/N ratio allowing us
to identify the ‘true signal’ peak and its location.
6.   From each interrogation volume the result is N disparity maps (N=# of cameras in the sys-
tem). If for example one uses 7x5x3 interrogation volumes, and there are 4 cameras in
the setup, the result will be a total of 7x5x3x4=420 disparity maps, each with its own dis-
tinct peak location (provided a well-defined peak can be found, which is not guaranteed
for all interrogation volumes).
7.   Create new virtual calibration data by using the center of each interrogation volume as
nominal marker position and using corresponding disparity peak location as the cor-
responding image position. Perform a new calibration based on these virtual calibration
points. You may choose to recalibrate all cameras at a time, or only the one where dis-
parity maps indicate the largest average errors.
8.   With the new modified camera calibration(s) return to step 1 and repeat the process.
9.   Stop if/when disparities are below a certain threshold or a certain number of iterations
have been performed without reaching a final result.
16.110.3 The Static Volumetric Calibration  Refinement in  DynamicStudio
To perform a static “Volumetric Calibration Refinement”, acquire a set of images with all cam-
eras simultaneously using a low seeding density of about 0,001 to 0,0025ppp. The require-
ments for the acquisition of the image to be used in the refinement step are similar or the
same as for a normal VV measurement:
l  A carefully performed calibration with calibration errors below 0.5 pixels
l  Homogeneous illumination, free of reflections
l  Homogeneously distributed particles through the entire field of view
l  High signal to noise ratio between the particles and the background
PAGE | 912
PAGE | 913
For the Volumetric Calibration Refinement method it is furthermore important that the
particle positions are statistically independent from each other. This means that, in case of
time resolved measurements, it is important to reduce the frame rate for the recording or to
only use a fraction of data acquired at a high frame rate. It is therefore recommended to
acquire about 50-250 images even if afterwards only statistically independent images, e.g.
every 10th image, are used for the actual refinement.
Since the calibration refinement is based on particle reconstruction, the images need to be
pre-processed such that only true particles have a non-zero gray value and the background is
black. This ensures a better reconstruction quality and thereby also a better quality of the
refined calibration. To get more information on this please see HELPVIDEO and the different
passages on the various “Image Processing” methods that DynamicStudio offers.
Volumetric Calibration Refinement in DynamicStudio
After the initial Multi Camera Calibration, please select the calibrations of each camera as well
as the separate recorded images and highlight them using the spacebar. Right mouse click on
one of the calibrations and select Volumetric Calibration refinement:
A new window for the “Volumetric Calibration Refinement” method will open, this pop up win-
dow features two main tabs:
1.   Voxel Reconstruction tab
2.   Voxel Reconstruction tab
The different recipes can be seen on the 2 following images
PAGE | 914
PAGE | 915
Recipe of the reconstruction tab from the
“Volumetric Calibration Refinement” method
Recipe of the refinement tab from the “Volu-
metric Calibration Refinement” method
In the Voxel Reconstruction tab, the volume in which the calibration refinement is going to
take place needs to be defined. These parameters are very similar to the known Voxel Recon-
struction tab known from the VV methods.
Resolution: The resolution describes the measure of how many voxels are needed to fill one
mm this is ideally the same as the mm to pixel ratio. By using the suggest button the software
will automatically figure out the needed value to have the mm/voxel set to the same value as
mm/pixel, in the center plane of the reconstruction.
Origin and Size: The origin of the bounding box is the left (min x), lowest position (min y)
corner, that is the furthest away from the viewer (min z). From this position with a right
handed coordinate system the volume is spanned in positive x, y, and z-direction. The size
determines how larger the volume extents from the origin. In the given example image the
volume will extend to x=+21.5mm y=+12.5mm and z=+7mm.
The reconstruction Method: will always be the SMART since this offers the best balance
between reconstruction accuracy and needed processing time. User just need to specify the
following:
SMART?Method: This parameter determines whether to use the Raytracing or Sparse
SMART?implementation. The Sparse SMART is generally faster for low seeding densities and is
therefore recommended for Volumetric Calibration Refinement. The Suggest button can be
used to automatically select between the Raytracing and Sparse SMART?based on the source
density of the input images. For more details about the two different
SMART?implementations, please see the help for Voxel?Reconstruction.
SMART Iterations: The desired amount of SMART iterations. For lower seeding densities of
up to 0,0025ppp, around 10-15 SMART iterations will be sufficient.
SMART Relaxation: This parameter defines how much change is applied between each
SMART iteration. This value will typically be set 1. If the reconstruction of a volume does not
looks good, try reducing this value to 0.75.
SMART Threshold: This parameter defines the amount of threshold that is applied after each
SMART iteration. 0.05% means that the lowest 0.05% of average voxel gray-value is set to
zero.
The Calibration Refinement tab is separated in two main groups.
The settings for the refinement and the settings for the sub volumes in which the disparities
are calculated. The settings for the refinement have the following options:
Iterations: Describes the maximum amount of iterations for the Calibration refinement
Number of images: defines how many images are used for the VV calibration refinement.
The used images are distributed evenly through the number of recordings in the selected
run.
Max. Disparity: Defines the maximum threshold per sub volume, below which the iterations
will stop. Ideally the maximum disparity is below 0.4 pixels, but of course the smaller the bet-
ter. Note that both criteria, the maximum and average disparity, need to be below the
defined threshold to stop the refinement.
Avg. Disparity: Defines a threshold for the average disparity per sub volume, below which
the iterations will stop. Ideally the average disparity is below 0.1 pixels, but of course the smal-
ler the better. Note that both criteria, the maximum and average disparity, need to be below
the defined threshold to stop the refinement.
Restrict one camera at a time: If selected, only one camera is corrected per iteration. Note
that calibration refinement will usually converge faster if the correction is applied to all cam-
eras simultaneously.
Use Universal Outlier Detection: If selected, a Universal Outlier Detections will be applied on
each plane of the disparity vectors with the following standard settings:
Neighborhood size: 3 by 3
Detection Threshold: 2.0
Normalization Level: 0.1
Rejected vector will be substituted.
The settings for the sub volumes have the following options:
IV  count: Defines how many sub volumes shall be created in the defined reconstructed
volume. Typically the smallest one is the depth of the reconstructed volume. To properly cor-
rect the 3rd order polynomial calibration at least 3 sub volumes in the reconstruction direction
are needed. It is recommended to keep the aspect ratio of the sub volumes consistent with
the aspect ratio of the voxel volume.
Manual Configuration: will enable users to select the select the size and the shift manually.
IV  size: Defines the x, y, z - size of the sub volumes in voxel
IV  step: Defines the shift of the sub volumes in voxel in x, y, z ? direction.
Clicking O.K. will start the refinement process and during the refinement the following inter-
active window will show the progression:
PAGE | 916
PAGE | 917
The user can switch between the average or the maximum disparity to be displayed by the
graphs for each camera over the amount of iterations. The graph also highlights the best iter-
ations so far. Furthermore by right clicking into the graph it is possible to make a print screen
or to copy the numerical data to the clipboard. If “close on completion” is unselected, this win-
dow will not close automatically. This makes it possible to copy the graph and numerical data
after the refinement is complete. Please note that method will not finish and the results of
the calibration refinement is not saved before the window is closed.
Within this display also other selections can be made:
Pause and display  iteration results: Will stop the process after each iteration and show the
disparity maps in a separate window. If not activated, the refinement will run until a stop cri-
teria is reached.
Close on completion: If selected, it finishes the method after a stop criteria is reached. Note
that in this case the data and the graph shown is lost after the method finishes.
The following image is shown at the end of each iteration if “Pause and display iteration res-
ults” is activated.
Here it is possible to select the disparity map for each sub volume and camera. Furthermore,
it is possible to select the total disparity image or the ones from each time step separately.
The last dropdown menu selects between the Disparity (only if total is selected), or altern-
atively the projections of the sub volumes or the real images.
On the right hand side the position of the Disparity peak is shown as well as if it is corrected by
the UOD (if applied). In the given example we are looking on the disparity map 8x1x1 mean-
ing the 8th map in x-direction in the 1st row and the 1st plane. It is from camera 1 and the dis-
parity added from all the images is shown. The disparity in x-direction is -0.0027 pixels and
0.1901 pixels in y-direction. The status is original meaning that this vector was not replaced by
the UOD.
16.110.4 Example of the VV calibration  refinement
The example database is based on synthetic data with a low image resolution in order to also
follow the ideas, and calculations on a laptop. The sample as well as the raw data can be found
on the DynamicStudio Demo Database that is provided with the installation of DynamicStudio.
An Initial set of calibration images are simulated and stored in the Project (Initial camera Cal-
ibration Images). Based on these calibrations it is possible to apply an initial “Multi Camera Cal-
ibration” An additional run is added from which the recording settings can be loaded again.
The data structure can be seen in the following image.
PAGE | 918
PAGE | 919
For a second Project in the database the camera positions were shifted for all cameras and
lens distortions were added with the following properties:
1.   Movement:
a.   For Camera 1 and Camera 2 = +0.6mm in X- direction = 3.5 pixels in X- direction
b.   For Camera 3 and Camera 4 = +0.6mm in Y- direction = 3.5 pixels in Y- direction
2.   Distortion model switched on = Pinhole camera model
a.   Increases distortion linearly starting from the center up to 2 pixels in the outer
parts of the image
This second project is named “Camera Moved +distortions”, it contains as well 2 runs. The first
run is used for the calibration refinement. The second run is the actual measurement where
the refined calibrations are used.
In order to start the Calibration refinement, checkmark the involved images as well as all the
initial calibrations. In the Pop up window showing the different analysis methods select cal-
ibrate and choose “Volumetric Calibration Refinement”.
The following 2 Windows show the settings that need to be made.
First of all, the volume boundaries used for the reconstruction need to be entered. Here the
“Show Projection” button helps to know which parts of the different camera images are
involved for the reconstruction. With the help of this feature the start of the volume was set
to the following coordinates Xorigin?=?-30mm, Yorigin?=?-20mm, Zorigin?=?-10mm. The size of
the volume is defined to X?=?60, Y?=?40, Z?=?20mm. (For more information on how to select the
volume please see the help of the method “Voxel Reconstruction”). The voxel resolution was
set to 0.15?mm/voxel.
In the tab for the Calibration refinement, the amount of refinement iterations is defined.
How many are needed strongly depends on the data. But as described DynamicStudio will
stop iterating if the max. disparities and the average disparities of all cameras are below the
defined thresholds.
PAGE | 920
PAGE | 921
During the calculations the following disparity graph is shown: This also highlights the best iter-
ations until this point and for the given example the Volumetric Calibration Refinement will
automatically stop after the next iterations because the “Avg. disparity” and the “Max. dis-
parity” falls below the defined thresholds of 0.05pix and 0.4 pixels for each camera.
In the next steps some reconstructions and LSM analysis are carried out to highlight the
impact of the VCR
PAGE | 922
PAGE | 923
Green = Simulated original Voxel spaces with no camera shift applied
Red = Voxel reconstructions of the shifted cameras with the initial calibrations
Green = Voxel reconstructions of the shifted cameras with the refined calibrations
Comparison of the signal to noise ratio from the reconstruction with the initial calibrations
(red curves) and the refined calibrations (yellow curves)
Green = Simulated original Voxel spaces with no camera shift applied analyzed with 3D LSM
Red = LSM analysis based on the Voxel reconstructions of the initial calibrations
Green = LSM analysis based on the Voxel reconstructions of the refined calibrations
16.110.5 The Temporal Volumetric Calibration  Refinement in  DynamicStudio
At the moment a temporal correction for the VV calibration refinement is not part of Dynam-
icStudio. It is planned to be introduced with a later version.
16.110.6 Side effects of the Volumetric Calibration  Refinement
The working principle of the Volumetric Calibration Refinement is warping the 3D space to cor-
rect the calibrations. However, this correction cannot be traced back and thereby the original
coordinate system as well as scaling is lost by introducing the correction parameters!
That means that on 2 consecutive images where the refractive index locally changed, one will
analyze velocities even though not particle movement took place! Hence we strongly recom-
mend to take calibration serious and to perform it with great care and as regularly as possible.
And in cases where the optical path changes during a measurement one needs to keep in
mind the shift applied to the initial calibrations. Generally one can say that the warping (or cor-
rection) of the volume is in the range of the added up average disparities from the involved
cameras.
PAGE | 924
PAGE | 925
16.111 Volumetric  Velocimetry (Legacy method)
This analysis method is obsolete and included for backward compatibility only.
The recipe is read only and no further development or maintenance will be made.
It will be removed completely in a future release of DynamicStudio.
We recommend using "3D Particle Tracking Velocimetry" on page?428 instead.
DynamicStudio provides three methods for analyzing volumetric imaging data:
l  3D Particle Tracking Velocimetry (3D PTV)
l  Tomographic Particle Tracking Velocimetry (TOMO PTV)
l  3D Least Squares Matching (3D LSM)
The selection of which technique to use depends on the hardware configuration and the par-
ticular physical nature of the measurement. The following table outlines the operating field
for each of the above techniques:
Technique                           Configuration-
min. cameras
Time data                   Media                      Seeding
density
[ppp] for 4
cameras
3D Particle Tracking
Velocimetry
3 cameras +             Time resolved
(single frame)
Water / air            max. 0.01
Tomographic Particle
Tracking Velocimetry
2 cameras +
(4-recom-
mended)
Time resolved
(single frame) &
double shutter
Water / air            max. 0.02
3D Least Squares
Matching
2 cameras +
(4-recom-
mended)
Time resolved
(single frame) &
double shutter
Water / air            max. 0.05
with
MTE?0.07
For both the PTV methods single image recordings are needed with no more the 8-10 pixels
of particle displacements.
Additionally for the TOMO?PTV and the 3D LSM approach the 3D gray-value distribution needs
to be reconstructed from the different images.
"3D Particle Tracking Velocimetry" on page?428
Tomographic Particle Tracking Velocimetry
3D Least Squares  Matching
Least Squares Matching (LSM) is a method for determining 3D velocity fields in highly seeded
flows in water and air. In contrast to Volumetric PTV and TomoPTV, the output data are equally
spaced vectors and the input data consists of double-frame images. Regular cuboids from two
or three reconstructed voxel volumes are analysed to determine local affine transformations.
"3D Least Squares Matching" on page?624
16.111.1 References
J. Kitzhofer, P. Westfeld, O. Pust, H. G. Maas and C. Brucker. Estimation of 3D deformation
and rotation rate tensor from volumetric particle data via 3D Least squares matching. In: Pro-
ceedings of the 15th Int Symp on Applications of Laser Techniques to Fluid Mechanics. Lisbon,
Portugal, 05-08 July, 2010.
T. Nonn. Application of high performance computing on volumetric velocimetry processing.
In: Proceedings of the 15th Int Symp on Applications of Laser Techniques to Fluid Mechanics.
Lisbon, Portugal, 05-08 July, 2010.
P. Westfeld, H.-G. Maas, O. Pust, J. Kitzhofer and C. Brucker. 3-D least squares matching for
volumetric velocimetry data processing. In: Proceedings of the 15th Int Symp on Applications
of Laser Techniques to Fluid Mechanics. Lisbon, Portugal, 05-08 July, 2010.
H.-G. Maas, P. Westfeld, T. Putze, N. Botkjaer, J. Kitzhofer, C. Brucker. Photogrammetric Tech-
niques in Multi-Camera Tomographic PIV. In:?Proceedings of the 8th International Symposium
on Particle Image Velocimetry - PIV09. Melbourne, Victoria, Australia, August 25-28, 2009.
J. Kitzhofer, C. Brucker and O. Pust. Tomo PTV using 3D Scanning Illumination and Telecentric
Imaging. In: Proceedings of the 8th International Symposium on Particle Image Velocimetry -
PIV09. Melbourne, Victoria, Australia, August 25-28, 2009.
16.112 Voxel  Reconstruction
This section gives you an overview about what voxel reconstruction are, what methods exist,
and how this is done in DynamicStudio.
16.112.1 Introduction
When performing reconstruction based 3D velocity measurements, the analysis procedure
consists of two different parts:
1.   Volumetric Reconstruction
2.   Velocity Analysis
The voxel reconstruction methods handles the first part, by generating a voxel-based 3D grey
value representation of the measurement domain for each time step. This representation is
called a voxel space or voxel volume. A voxel can be imagined as a pixel extended into 3D
space with each voxel having a gray value information similar to pixel. Typically all edges of a
voxel are of the same length. To generate these voxel volumes from different time-syn-
chronous images, many different approaches exist. The most common ones will be described
here:
PAGE | 926
PAGE | 927
1.   MinLOS Reconstruction
2.   SMART Reconstruction
A)?As Raytracing
B)?As Sparse
3.   Motion Tracking Enhancement (MTE)
Afterwards the voxel based reconstruction, the velocities can be calculated by applying 3D
Least Squares Matching (LSM) or Tomographic Particle Tracking Velocimetry.
16.112.2 The different reconstruction  techniques in  theory
Minimum Line of Sight (MinLOS)  reconstruction
MinLOS is an easy and computationally quick technique. For each voxel, the different line of
sights passing through it are compared and the voxel inherits the lowest grey value. A sim-
plified example of this is shown on Figure 1, where the line of sight from a few pixels in each
camera and their intersection in the voxel volume are shown from a top-down perspective.
Consider the case where camera 2 is not available. The line of sights from the left and right
camera intersect in the two particle centers and the left particle is assigned the value of 197
counts, as this is the smallest grey value that intersect with the particle, while the right particle
is assigned the value of 124 counts. Since the lines of sight also intersect in another location,
so-called ghost particles are formed with an intensity of 124 counts (top) and 143 counts (bot-
tom). These ghost intensities become more regular with an increased seeding density and in
some cases can even outnumber the true particles. One way to overcome the issue with
ghost particles is to add another camera. When camera 2 is added to the given example, the
two ghost particles are eliminated as the line of sight from the new camera intersect the
ghost particles with zero intensity.
Figure 1 Simplified example of intersection of line of sights
The term seeding density is often used in volumetric measurements, which is a measure for
how many particles are in a given volume. The seeding density is measured in the sensor
plane and is therefore measured in particles per pixel (ppp). The maximum seeding density
recommended for a MinLOS reconstruction is about 0.02 ppp with a four-camera set-up.
Typically the seeding density is measured on the sensor plane, with a seeding density of
0.02ppp and a 1MP camera this means we see 20,000 particles. This also means that the num-
ber of particles is constant in the volume no matter if the volume is very thin or very thick.
Hence the deeper you make the volume, the lower the spatial resolution will become.
A MinLOS reconstruction is computed very fast, because it is a single pass comparison, how-
ever the number of ghost particles produced the MinLOS makes it mostly suitable to use it for
quick tests, initial guess for iterative reconstruction methods, and measurements with very
low seeding density.
MART & SMART Reconstruction
(S)MART stands for (Simultaneously) Multiplicative Algebraic Reconstruction Technique.
The MART algorithm was originally presented by Elsinga et al. in 2006, while the SMART
algorithm is a slightly alternative approach to better thread the calculations and make a better
use of the multicore CPU architecture. The general idea of MART is, that an observed pixel
intensity in the camera images, should equal the integration of the reconstructed voxel intens-
ities along the pixels line of sight. If the pixel intensity and projected intensity differs, all
voxels along the line of sight are corrected.
The amount of correction applied to each voxel depends on the weight that the voxel has on
the pixels intensity.
The schematic of updating the voxel entries can be seen on Figure 2.
Figure 2 MART, from Elsinga et al. (2006)
The figure show a representation of the imaging model used for MART reconstruction. Here a
simplified top view in 2D is shown from two cameras and the voxel volume in one slice. The
PAGE | 928
PAGE | 929
gray level indicates the value of the weighting coefficient ( wi,j ) in each of the voxels with
respect to the pixel I(x1, y1). Note that this process is an iterative one, so that each voxel is
updated multiple times. The schematic clearly shows the necessity for this, because after the
first iteration the line of sights in the voxel domain of each non-zero pixel are filled with gray
values. Those gray values will be removed with an increasing amount of iterations. The resid-
ual is defined by the difference in the gray values between a voxel and a pixel in the line of
sight and it decreases with the number of iterations as the voxel volume converges towards
the solution.
In DynamicStudio, the SMART uses an initial guess approach for the voxel space, which is
based on the MinLOS. Additionally there are two types of SMART reconstructions available in
DynamicStudio.
The first method is the raytracing-based SMART, which is the original implementation of
SMART in DS. This method stores a list of non-zero pixels and raytraces each line of sight
through a dense voxel volume.
The second method is the sparse SMART, which takes the other opposite approach and stores
a list of the non-zero voxels and projects each voxel to the camera sensors. This increases the
computation cost of each voxel, but reduces the number of voxels processed if the seeding
density of voxel volume is low, thus this method is recommended for voxel volumes with low
seeding density.
Using SMART reconstruction, it is possible to reconstruct high seeding densities of up to
0.05ppp with a four-camera set-up. In case this is not enough, the Motion Tracking Enhance-
ment can be applied.
Motion Tracking Enhancement (MTE)  Reconstruction
The Motion Tracking Enhancement is a technique developed by Matteo Novara [3] based on
the assumption that ghost particles, in the presence of a flow, appear in different locations.
Together with the velocity information, one can enhance the reconstruction capabilities for
high seeding densities. However this adds another iterative step in the form of a velocity ana-
lysis.
The reconstruction is achieved by reconstructing two consecutive voxel fields, Vt and Vt+1, as
the so-called first guess reconstruction. Once the velocity field is calculated, the velocity field is
used to deform the voxel space at timestep t+1 onto the time-step t. Afterwards the voxel
spaces can be compared, since the ghost particles in each time-step should not be at the same
locations, one can reduce the influence of their intensities while at the same time the particle
intensities are enhanced. This can be done by adding or multiplying the different voxel-
spaces. These new voxel space are used as a new input for another SMART reconstruction,
which further improves the reconstruction quality. This sequence can be shortly described by
the following points:
1.   SMART-reconstruction of Vt and Vt+1
2.   Velocity analysis between the two reconstructions gives the velocity U at time t+?
3.   Deformation of the two voxel spaces to one common time step Vt+? and Vt+1-?
4.   Combination of the two voxel spaces to calculate an enhanced reconstruction V't+?
5.   Warping the enhanced reconstruction back into their original time step V't and V't+1
6.   Use V't and V't+1 as new input for an additional SMART reconstruction
Point 2-6 in the sequence can be repeated until the result converges. The number of repe-
titions are defined by the user in the UI, where they are called MTE iterations. The cuboid size
using the LSM algorithm (Least Squares Matching) to calculate the velocity-fields needed for
the MTE is also set in this user interface. This technique has very high computational demands
due to its two interlinked iterative processes that also includes a velocity calculation. Seeding
densities up to 0.075ppp for a four-camera set-up can be properly reconstructed.
Memory  requirements  and computational costs
The size of a voxel volume that needs to be kept in RAM or written to disk is independent
from the amount of particles (unless the Sparse SMART is used.)
To estimate the size you can multiply the amount of voxels with 4 bytes (single-precision float
value) SizeVol = Xvox*Yvox*Zvox*4 bytes. For the example of a 4 MP camera with a resolution of
2000 x 2000 pix, and an aspect ratio of 2:1(between the x and z-axis dimensions), the SizeVol =
2000 * 2000 * 1000 * 4 bytes = 16 GB for one time step. If you are dealing with double
images, you need to multiply this by two. This is just the size of the voxel space.
For the SMART reconstruction methods, weighting matrices may be required to store as well,
which are precalculated values needed to speed-up the calculations. These weighting matrices
can get very large, especially for the raytracing method. The table below shows when weight-
ing matrices are used.
MinLOS                                                        SMART                                                                MTE
Raytracing                                    Sparse
CPU    Weighting
matrix not
needed
Weighting matrix
always stored
Weighting matrix always stored     Depends on
reconstruction
settings
GPU   Not imple-
mented on
GPU
Weighting matrix
calculated on the
fly
Weighting matrix stored if
memory available, otherwise cal-
culated on the fly
In order to keep the needed amount of memory computations as low as possible, proper
image pre-processing is necessary. The goal of the pre-processing is to set all pixels from all
cameras that does not contain any particle grey value information to zero. Care needs to be
taken in order to keep the true particles in the image and to not truncate them to the size of
one pixel only, since this biases the position accuracy.
16.112.3 The Voxel Reconstruction  in  DynamicStudio
The voxel reconstruction recipe, presented in the following picture, is divided into two sec-
tions that correspond to the two steps of the reconstruction procedure:
l  see Voxel space setup
l  see Reconstruction setup
In order to apply this analysis method in DynamicStudio, at least two cameras with time syn-
chronous particle images, as well as the calibration files of the cameras need to be check-
marked, as seen in Figure 3 (use the spacebar to checkmark the datasets and calibrations).
Note that Motion Tracking Enhancement can only be applied on double images.
The recipe is located in the "Volumetric" category of the analysis method menu:
PAGE | 930
PAGE | 931
Figure 3 Input ensembles required for voxel reconstruction
The recipe for the Voxel Reconstruction method is shown on Figure 4. Each of the options will
be explained in the following sections.
Figure 4 Recipe for Voxel Reconstruction
Voxel space setup
As already mentioned, a voxel is the equivalent for a volume of a pixel for an image. Each
voxel representing a gray value from the 3D particle distribution. In order to perform the
reconstruction, the user has to provide the recipe some geometrical parameters regarding
the voxel space this includes the x,y and z dimensions in mm, their origins with respect to the
calibration and the resolution that defines the physical size in mm/voxel.
Resolution of the voxel space
The voxel resolution will determine the number of voxels that will be used to discretize the
voxel space, and corresponds to the physical size of a voxel. This number should be equivalent
to the camera's resolution, i.e. the physical size in mm per pixel. The smaller this number the
higher the number of voxels, and thus the amount of data to be computed. Using the “sug-
gest” button next to the resolution calculates the optimal size for it.
Center / Origin of the voxel space
Enter the coordinates (x, y, z) of the origin of the desired voxel space are centered around
the middle of the desired reconstructed volume. here, with respect to the calibration. Entries
are in mm. The next image shows a target image with the overlaid calibration. Now we want to
start our volume at the x|y|z positions of: -20 | -20 | -10 mm, the starting point of x and y can
directly be seen in the image. The z position results from the first target calibration image
with a right hand coordinate system, in this case behind the imaged plane.
Figure 5 Calibration overlayed on calibration images
You can always control your calibrated volume, by overlaying the calibration onto the cal-
ibration images (compare to last image).
Size of the voxel space
PAGE | 932
PAGE | 933
Indicate here the size of the voxel space in millimeters. In the later example, the values to
enter would have been (40|40|20) which for the example would lead to the seen above end
points in the x- and y direction. The end point of z would be in a plane in front the image.
The resulting voxel space dimensions are calculated and displayed accordingly to the specified
dimension of the voxel space and the voxel resolution.
For more information regarding the calibration see also Multi Camera Calibration & Volu-
metric Calibration Refinement.
These settings can be checked when pressing the “Show projections” button in the recon-
struction recipe. When pressed, an extra window opens that shows the first of the particle
images and a green bounding box representing the volume that will be reconstructed. Note
you can switch through the different cameras via the drop-down menu. For a proper recon-
struction it is also needed, that for all cameras the bounding box is within the particle image!
The bounding box is linked to the projection of the cameras, hence the box enlarges, if the
depth of the volume is enlarged. Also please note that for best reconstruction results, always
reconstruct the complete illuminated depth of the volume.
Figure 6 Projection bounds dialog
Reconstruction setup
This part defines the reconstruction method used to generate the voxel space. Three meth-
ods are applicable via a drop down menu. The MinLOS, the SMART reconstruction, which is
also divided into 2 subsections and the MTE can be selected. The different options and the
graphical user interface is shown below:
Figure 7 Reconstruction setup for each reconstruction method
MinLOS Reconstruction
In the Method for the reconstruction the MinLOS can be selected. In case this one is chosen
no specific parameter needs to be edited.
Normalize Images: This applies a histogram equalization between the different camera
images. This is sometimes necessary, especially if the cameras are positioned in different scat-
tering directions of the Laser illumination and the resulting images have big differences in
their brightness level.
SMART Reconstruction
To apply a SMART reconstruction, just select SMART in the drop-down menu and you have the
possibility to set the following parameters
SMART Iterations: This specifies how many iterations for the reconstruction will be made,
typically 10-30.
SMART Relaxation: Is a damping parameter for the reconstruction procedure that should be
between 0 and 1. The smaller the dampening is, the more iterations are necessary. Hence in
special cases with very good image quality and mild seeding one can also try to set the value
higher than one.
SMART Threshold: Applies a threshold on the gray-values between each SMART iteration. A
value of 2 means 2% of the smallest values are muted to 0 at the end of each iteration.
Normalize Images: This applies a histogram equalization between the different cameras.
PAGE | 934
PAGE | 935
Use GPU acceleration: This enables the computation on the GPU. Note that at least NVidia
Compute Capability 3.5 is necessary and GPU with over 8 GB of memory and more than 2k
streaming cores is recommended.
Suggest: This is a guide whether to use Sparse or Raytracing based SMART. In the image
plane for all involved cameras it calculated the source density (amount of pixels that are non-
zero) and if this is above 0.18 the Raytracing approach is selected. Please note that this is a
generalized approach that was considered for the CPU, Ram, and GPU combination we offer
for our standard PCs and workstations (as of August 2018). If you use a different combination
this number might differ.
Motion Tracking Enhancement
If MTE is selected as reconstruction technique, the following additional settings compared to
the SMART are available:
MTE Iterations: This specifies how many iterations for the Motion Tracking Enhancement will
be done. Typical values are between 2 and 5, depending on the seeding density.
MTE IV  Size: Defines the cuboid size for the LSM to calculate the predicted deformation of
the voxel spaces.
Enable pyramid scheme: Switches Pyramid Scheme on or off for the Least Squares
Matching.
Be aware that this process multiplies your computational cost for the SMART reconstruction
and additionally adds the time for the velocity-analysis. Hence, only use it in case of high seed-
ing densities.
Resulting voxel spaces
Once the voxel reconstruction has finished, the reconstructed voxel space is displayed in a 3D
display . In this display, it is possible to rotate, zoom and pan using simple mouse interactions.
Furthermore by double clicking into the 3D display, it is possible to change the histogram
(Color map and Histogram). Right mouse-click will open a context menu with different options
to operate the display, such as toggle between two frame if the voxel volume is from double-
frame images. Toggling between frames is also possible using the keyboard shortcut "t".
One important option in the context menu is the “Z Plane intensity”. This sums up all intensives
along the z-axis and normalizes them. If you have reconstructed slightly more than you illu-
minated and your reconstruction quality is very good, you can see a drop of your intensity
towards the non-illuminated area as seen in the sample image below. If the reconstruction
quality is not as good, there will be a lot ghost intensities in the non-illuminated regions of the
z-axis. The values of the profile can also be exported as numerical values.
Figure 8 Example of 3D display
For more information about the display, please see 3D Display.
For more information about tomographic reconstruction in general, please see:
[1] T. Nonn. Application of high performance computing on volumetric velocimetry
processing. In: Proceedings of the 15th Int Symp on Applications of Laser Techniques to Fluid
Mechanics. Lisbon, Portugal, 05-08 July, 2010.
[2] Atkinson C, Soria J: (2009): "An efficient simultaneous reconstruction technique for tomo-
graphic particle image velocimetry" Experiments in Fluid 2009, 47:553
[3] Matteo Novara (2013): "Advances in tomographic PIV" PHD thesis Delft University
16.113 Waveform  Calculation
The Waveform Calculation (formerly known as Rescale Analog) enables you to apply one or
more transformations to analog waveform (generic) data. It allows you to combine data from
any of the coincident data channels, with a number of build-in mathematical functions and
operators.
Each transformation consist of a number of formulas representing the returned output data.
Every output data must have a name defining the column and an optional unit for the value.
16.113.1 Formulas
The Analog transform analysis uses formulas to define the transformations between input
data (columns) and output data (columns). One formula represents one output data column (in
generic format) with a name and units. A formula can be created using build-in operators and
functions, along with available input variable names (see below).
Using the formula drop-down menu enables you to select between the build-in functions and
operators, in the two first columns, and the available input variables in the last column. The
input variable names are corrected using the naming conversion described later.
When an item is selected it is automatically added to the formula editor. This is especially use-
ful to identify and select the available input data (columns) for the formula.
PAGE | 936
PAGE | 937
16.113.2 Built-in  Functions
The following table gives an overview of the functions supported. It lists the function names,
the number of arguments and a brief description.
Name                    Arguments                                           Explanation
sin                            1                               sine function
cos                           1                               cosine function
tan                           1                               tangent function
asin                          1                               arcsine function
acos                         1                               arccosine function
atan                         1                               arctangent function
sinh                          1                               hyperbolic sine function
cosh                         1                               hyperbolic cosine
tanh                         1                               hyperbolic tangent function
asinh                        1                               hyperbolic arcsine function
acosh                       1                               hyperbolic arccosine function
atanh                       1                               hyperbolic arctangent function
log2                          1                               logarithm to the base 2
Name                    Arguments                                           Explanation
log10                       1                               logarithm to the base 10
log                            1                               logarithm to the base 10
ln                              1                               logarithm to base e (2.71828...)
exp                           1                               e raised to the power of x
sqrt                          1                               square root of a value
sign                          1                               sign function -1 if x<0; 1 if x>0
rint                           1                               round to nearest integer
abs                           1                               absolute value
if                               3                               if ... then ... else ...
min                           var.                          min of all arguments
max                          var.                          max of all arguments
sum                          var.                          sum of all arguments
avg                           var.                          mean value of all arguments
16.113.3 Built-in  Operators
The following table lists the binary operators supported by the transformation.
Operator                        Meaning                        Priority
=                   assignment*                               -1
and              logical and                                   1
or                 logical or                                      1
xor               logical exclusive or                     1
<=                less or equal                               2
>=                greater or equal                        2
!=                 not equal                                     2
==                equal                                            2
>                   greater than                               2
<                   less than                                      2
+                   addition                                       3
-                    subtraction                                 3
PAGE | 938
PAGE | 939
Operator                        Meaning                        Priority
*                   multiplication                              4
/                   division                                        4
^                   x^y, raise x to the power of y  5
*The assignment operator is special since it changes one of its arguments and can only be
applied to variables.
16.113.4 Naming Conventions
Variable names in the formulas must follow standard mathematical conventions. As a con-
sequence variable names cannot include characters like spaces or others that can be inter-
preted as an operator or such. Because data columns can be freely named by DynamicStudio,
renaming of the columns names are sometimes necessary. Valid variable names can be selec-
ted in the formula editor in the recipe.
Underscores are used as the preferred delimiter. The following characters are converted:
Input char                                       Converted char
(space)                                             _ (underscore)
-                                                         _
(                                                         _
)                                                         _
[                                                         _
]                                                         _
{                                                        _
}                                                        _
.                                                         _
,                                                         _
(any other non-alpha)                    _
#                                                       N
(digit as first char)                          X
All other non-alphanumeric characters are ignored and left out in the conversion. If the first
character is a digit an ‘X’ replaces it.
16.113.5 Syntax and Numerical Errors
During calculations both syntactical and numerical errors can occur. Syntactical errors are
reported with a precise description of where the actual error is encountered, with a clear
indication what to correct. Numerical error can happen during calculation and if often caused
by floating point domain and/or range errors. E.g. if a divide-by-zero calculation error is found
the analysis will stop, and report an numerical error in the formula.
16.113.6 Examples
Linearization
Following examples represent data transformation which converts data from a source data
format in voltages into a destination data set in a given unit. Most often analog data is read as
voltages from external instruments or equipment. These analog voltages represents a phys-
ical parameter like pressure or temperature. To get from the analog voltage to the physical
parameter a linearization is necessary. This linearization can typically be accomplished by a
polynomial transformation. Using the if-then-else build-in function also allows you to define dif-
ferent linearizations to different parts of the input data.
The following linearization example shows an instrument having a
relation between the analog voltage and the temperature. At
voltage 0 V the temperature is 20 °C etc. The curve shows that
the linearization trend changes at 4 V. The two linearizations can
be found as:
Trend 1: C = 5*V + 20, and
Trend 2: C = 2*V + 32.
The Analog Transformation can now be used to express this linearization relation using the for-
mula: if(V<4, 5*V+20, 2*V+32)
16.114 Waveform  Extract
The Waveform Extract makes an extract of multiple datasets in selected indices (i). This is par-
ticularly useful in connection with analog waveforms, but can also be used in other con-
nections.
The result from an extract can either be opened as Numeric (exported or copied via the Win-
dows Clipboard to i.e. MS Excel) or it can be view in DynamicStudio as XY plot.
16.114.1 Extracting Data
The picture below illustrates how the Analog Extract analysis works. When an ensemble con-
tains more than one analog or generic dataset, it extracts the data values from the same
index in all the datasets. The result is a curve representing all the data values from the same
index.
PAGE | 940
PAGE | 941
The order in which the data values are extracted is based on the current ensemble sorting. It
is also possible to specify more than one index to extract data from, within the same data, in
which case a two column data set is created.
16.115 Waveform  Statistics
Basic statistical properties for analog waveform data (generic data) can be calculated using the
Waveform Statistics analysis method. The analog statistics is useful when a trend, in the analog
data, is to be analyzed during a complete series of measurement.
16.115.1 Statistical Values
All the statistical values is returned in one dataset including the following data columns:
Column Name                                                                      Description
X, Y, Z                                                         Traverse position
Time                                                           Time when data was acquired
Count                                                         Number of sample values in dataset
Min                                                             Minimum value in dataset
Max                                                            Maximum value in dataset
Mean (arithmetic mean)                         Mean of all values in dataset
StdDev (standard deviation)                  Standard deviation of all values in dataset
Classic formulas for mean and standard deviation are used:
The X, Y and Z values are collected from the record properties for the dataset. These can be
manually specified or automatically added when using the Acquisition Manager. The Time
value corresponds to the acquisition timestamp, also found in the record properties. The
Count value is determined from the input dataset.
The Min, Max, Mean and StdDev are calculated for each selected input column. The results are
presented using the following naming rules: <Column name>_Min, <Column name>_Max,
<Column name>_Mean, and <Column name>_StdDev , for each of the data values.
16.115.2 Example
The Analog Statistics analysis method can be used to find the average value of an Analog Wave-
form for each acquired image, when images and analog data is acquired simultaneously.
Together with the advanced sorting option of ensembles, the analog statistics can be used to
sort a series of images based on an average analog value.
16.116 Waveform  Stitch
Waveform Stitch allows you to merge or combine a series of analog (generic data) datasets
into one data series. This is especially useful to combine a series of analog waveforms
acquired for each image into one analog time series for the entire measurement.
You can select which of the available input data columns to stitch, and how the time base of
the new data should be defined.
16.116.1 Stitching Data
The picture below illustrates how the Analog Stitch analysis works. When an ensemble con-
tains more than one analog or generic dataset, it stitches the data from all the individual data-
sets into one. The result is a curve representing all the data values from all the datasets. The
PAGE | 942
PAGE | 943
order the data is stitched is based on the acquisition time, meaning that stitching is not pos-
sible if the data is missing the acquisition time1 information.
The above datasets are stitched together to present one curve on a common axis.
1Can be seen with databases created with older versions of DynamicStudio.
If the common axis is set to the Time, the curve will be aligned to the acquisition times of the
individual datasets, see image to the left. If the common axis is set to index the "space"
between the individual datasets will become more evident.
You can stitch more than one data channel if available in the datasets, and then later define
which to display in the XY plot.
16.116.2 Analog Stitching
The way an Analog signal is stitched will depend of the ratio between the time between
images (pulses) and the analog sampling time. If the time between images is high compared
with the analog sampling time, the stitched curve will have longer periods with no data.
16.117 Correlation  option  Window/Filter
A number of correlation methods is offered for processing of PIV images:
Auto-correlation
Cross-correlation
Adaptive PIV
Average correlation
For all of these a processing option named Window/Filter is available, with a recipe as shown
below:
16.117.1 Window  functions
The correlations are calculated using Fast Fourier Transformation (FFT). This approach gives
much higher calculation speed than a direct implementation, but the method is based on the
assumption that the input particle patterns are cyclic and correlates across the interrogation
area boundary using this assumption. For example particles near the right hand edge of the
interrogation area may correlate with particles near the left-hand edge, but interpret this as a
PAGE | 944
PAGE | 945
small displacement to the right; -Due to the assumed cyclic behavior, the particles near the
left hand edge are assumed to be present also just to the right of the rightmost edge of the
interrogation area, and similarly particles near the right hand edge are assumed to be
present also just left of the leftmost edge. The exact same happens with top and bottom
edges. Obviously this is an error source, but with suitable interrogation area sizes and time
between the two particle image recordings, the "true" particle displacement will normally pro-
duce a dominant peak in the correlation map, while these so-called phantom correlations pro-
duce small noise-peaks that do not affect the final result.
It is however possible to reduce or even eliminate the cyclic noise from the correlation map
by using a window: The window function is a preprocessing of one of the interrogation areas
prior to performing the correlation, where pixel values are multiplied by a factor between 0
and 1 depending on their location within the Interrogation Area (IA). Pixels at the center of
the IA are typically multiplied by one, while pixels further away are scaled down.
The simplest window is the Tophat that masks out pixels near the edges of the IA by mul-
tiplying with 0, while the remaining central pixels of the IA are multiplied by 1 and thus remain
undisturbed. This way cyclic noise is eliminated, since there are no particles near the edge of
the interrogation area that can correlate with phantom particles across interrogation area bor-
ders.
The drawback is that reducing both height and width of the effective interrogation area signal
strength is reduced also. To avoid this it is generally recommended to use larger inter-
rogation areas than normal, when applying a window function. In order to recover the inform-
ation that is lost when masking the interrogation area, it is furthermore recommended to use
overlapping interrogation areas when applying window functions.
Clipping of particle images is another error source, that is present even when using the top
hat window: Particles straddling the interrogation area border will be clipped so only part of
the particle image is visible when calculating the correlation. The clipped particle will be inter-
preted as being closer to the center of the interrogation area than it actually is, and on aver-
age particle clipping will tend to bias average results towards lower velocities. A similar
problem exists with the top hat window, where particle images straddling the edge of the win-
dow will also be clipped and bias end results towards lower velocities.
The clipping of particle images can be avoided by using a window that is 1 in the center of the
IA and decrease smoothly as you approach the edge.
The simplest such filter is the Bartlett window, that increases linearly from 0 at the edge to 1
at the center of the IA. In 2D the Bartlett window has a pyramid shape.
In the frequency domain both Tophat and Bartlett windows produce ringing artifacts due to
the sudden jumps in value or slope (Tophat/Bartlett respectively). To avoid this the two
remaining window functions vary smoothly in terms of both value and slope: The Hanning fil-
ter is a raised Cosine that is zero at the IA?edges and one in the center, while the Gaussian is
also one in the center, but does not reach zero at the edge of the interrogation area. Since
the Gaussian does not reach zero at the edge of the Interrogation Area it reduces cyclic
noise, but does not remove it completely. It does however have the benefit that a Gaussian in
the spatial domain becomes a Gaussian in the frequency domain also and thus produce no
ringing artifacts except from the edges, where the Gaussian is truncated. Default windows
are shown below:
The windows are of course 2-dimensional like the interrogation areas to which they are
applied. The windows shown here are square, but they can be rectangular like the Inter-
rogation Areas to which they are aplied:
All windows are 1 in the center and all but the Gaussian reach 0 at the edges, but how they
behave in between differs.
The Bartlett and Hanning windows are fixed, but for the Tophat and Gaussian windows an
adjustable k-value allows the user to adjust where or how fast they approach zero.
For the Tophat the k-value specify directly where the window jumps from zero to one. The
default value k=0.50 shown above means that this happens halfway between the center and
the edge, while larger k-values shift the jump closer to the edge. For k=1.0 the jump is at the
interrogation area boundary equivalent to no windowing at all, while k=0.0 would set all but
perhaps the central 2x2 pixels to zero. Obviously k=0.0 and k=1.0 are meaningless for the
Tophat window, but values in between can be useful:
PAGE | 946
PAGE | 947
MxN is the size of the interrogation area, most often square so M=N, but it can be rectangular
as well. (m,n) describes positions within the interrogation area with (0,0) meaning the center.
For the Gaussian the k-value also specify window width and default k=0.75 is shown in the fig-
ures above.
As before MxN is the IA?Size and (m,n) describes positions within the IA with (0,0) being the
center. Again increasing k-values make the window broader (i.e. increases the spread of the
Gaussian).
Choosing a k-value the user can change the width of the Gaussian window applied and thus
achieve a suitable compromise between signal strength and cyclic noise; A narrow window will
for all practical purposes reach zero at the edge of the interrogation area and thus eliminate
cyclic noise. The signal strength will however be low, since only the pixels at the very center of
the interrogation area will contribute to the correlation, while all other pixels are multiplied
with a very small number. On the other hand a broad window will give high signal strength at
the cost of increased cyclic noise, since the window does not reach zero at the edge of the
interrogation area.
16.117.2 Filter functions
As explained above, correlations are performed using Fast Fourier Transforms, which trans-
form from the spatial domain to the frequency domain and back again. While the window func-
tions described above operate in the spatial domain by manipulating the interrogation areas
prior to correlation, the filter functions operate in the frequency domain just before the
inverse FFT, that generates the correlation map.
Apart from 'None', the user may choose one of the filters 'NoDC', 'LowPassGaussian' and
'PhaseOnlyGaussian'.
No DC
No-DC simply removes DC-components from the signal, setting the zero-frequency signal to
zero. Generally speaking the background and/or average grayscale intensity in interrogation
areas 1 and 2 will correlate and produce a background DC-correlation level, not related to the
flow being measured. Normally the mean grayscale values are subtracted from both inter-
rogation areas before processing, so the problem should be small, but any remaining DC-com-
ponent can be completely removed using the No-DC filter.
Low-Pass  Gaussian
To fulfill the Nyquist sampling criterion particle images should be at least two pixels in dia-
meter, but in practical PIV experiments this is often not possible, and particle image dia-
meters in the range 1-2 pixels are not unusual. Violating the Nyquist criterion will inevitably
produce pixel locking, where measured velocities are biased towards values that correspond
to integer pixel displacements on the images. This cannot be avoided completely, but the
effects can be mitigated by the use of a Gaussian low-pass filter that multiply each of the Four-
ier coefficients with a factor W depending on the frequencies (u,v) that it describes: Small
particle images produce narrow peaks in the correlation plane and in the frequency domain
this corresponds to high frequencies. With a low-pass filter, high frequency components are
damped, resulting in broader, but also lower correlation peaks. The broadening of the peaks
reduces the pixel locking effect, while the lowering of the peak corresponds to a reduction of
signal strength. Fortunately the low-pass filter also reduce the noise somewhat, so end-res-
ults are still OK. The filter width can be controlled by the user, by setting the k-value similar to
what is done for the Gaussian window; Small k-values produce broad filters, that broaden and
lower the correlation peaks very little, while large k-values produce narrow filters, that
broaden and lower the correlation peaks much more, until eventually the individual peaks can
no longer be distinguished.
-where (u,v) describe horizontal and vertical frequencies, while U & V is the Nyquist limit fre-
quency.
Please note that the frequencies discussed here describe variations in space rather than time,
and a low frequency thus corresponds to something with a long wavelength rather than some-
thing varying slowly.
To understand how the filters work, you should be aware that a narrow peak in the cor-
relation plane corresponds to a high frequency in the frequency domain, meaning that the
Fourier-Coefficients will be large for numerically large values of u and v. A wide peak in the cor-
relation plane on the other hand corresponds to a low frequency in the frequency domain,
meaning that the Fourier-Coefficients are large for small values of u and v, and the Fourier-
Coefficient at (u, v) = (0, 0) describe a DC-level in the correlation plane.
Phase-Only  Gaussian
The Phase Only Gaussian filter uses the exact same Gaussian as the one described above, but
rather than scaling Fourier Coefficients with a certain factor, the filter value W overwrites the
amplitude, keeping only the phase of each Fourier Coefficient. The Fourier Coefficients are
complex and interpreting them in polar form all information regarding peak locations are
stored in the phase, while the amplitude describe signal strength (i.e. peak height and noise
floor ripple). Keeping the phase information ensures that correlation peaks remain where
they are, while imposing a certain (Gaussian) amplitude distribution can impose a peak shape
well suited for subpixel interpolation.
16.118 Adaptive  Correlation  (Legacy method)
This analysis method is obsolete and included for backward compatibility only.
The recipe is read only and no further development or maintenance will be made.
It will be removed completely in a future release of DynamicStudio.
We recommend using "Adaptive PIV" on page?433 instead.
PAGE | 948
PAGE | 949
The adaptive correlation method calculates velocity vectors with an initial interrogation area
(IA) of the size N time the size of the final IA and uses the intermediary results as information
for the next IA of smaller size, until the final IA size is reached.
The Adaptive correlation can be used with the High Accuracy Module and Windows deform-
ation.
Additionally, local validation can be added to the adaptive correlation so that viewed on all the
calculations process, less 'bad' vectors are generated. To compensate for the loss of vector
field resolution during the processing, overlap of IA is often used with a typical value at 25 %.
(Post-processing the resulting vector map by re-sampling it with an oversampling factor
greater than unity can be done as well to enhance the spatial resolution - see help on the
method 'Resampling of Vector Map'.)
Example: (Left) Cross-correlation with (16x16) and 25% overlap, (Right) Adaptive correlation
(16x16) and 25% overlap with 3 steps refinement. Note that less bad vectors are generated
on the map to the right and that local refinement ("green vectors") is a realistic correction of
the flow field.
16.118.1 Interrogation  areas
When selecting N=3 and a final interrogation area of (16x16), the initial IA size is (128x128)
The parameter "Overlap - Horizontal/Vertical" defines a relative overlap among neighboring
interrogation areas, as illustrated in the figure below for (H-50 %, V-50 %). It can be set inde-
pendently for the horizontal and vertical, offering total freedom to increase vector map res-
olution in any direction. In the example, 5 vectors maps are created instead of 1 when H = 0 %
and V = 0 %.
Setting the various parameters of the adaptive correlation calculations is briefly explained in
the following. More technical information can be found in the PIV User's Manual.
More information on...
l  Window and Filter functions
l  Validation methods available with adaptive correlation
l  About Interrogation area offset
16.118.2 Window  and Filter
With the development of advanced algorithms, filters are very little used and nowadays pref-
erence is always given to setting the interrogation areas and validation methods adequately.
Basically, the "Window/Filter" options set a-priori functions on the signal processing.
PAGE | 950
PAGE | 951
See more in the help file regarding Windows and Filters.
16.118.3 Validation  methods
Validation parameters for the adaptive correlation method are various and can also be used in
combination to fine-tune the processing and, when needed, to remove spurious vectors.
In the "Peak validation" section, the user can set values for the minimum and the maximum
peak widths as well as the minimum peak height ratio (between 1st and 2nd peak) and
thereby put more stringent conditions on peak identification for the subsequent determ-
ination of vectors.
Peak validation can help identify invalid vectors, but is unable to produce an estimate of what
the correct vector might be. Consequently the invalidated vector will simply be substituted
with zero, which in many cases can be quite far from the truth. You are therefore strongly
advised not to use peak validation alone, but always combine it with a local neighborhood val-
idation, which based on neighboring vectors is capable of making a realistic estimate of what
the spurious vector should have been.
With "Local neighborhood validation", individual vectors are compared to the local vectors in
the neighborhood vector area, which size (MxM) is set by the user. If a spurious vector is
detected, it is removed and replaced by a vector, which is calculated by local interpolation of
the vectors present in the (MxM) area. Interpolation is performed using median or moving
average methodology (with n iterations).
Spurious vectors are identified via the value given to the "Acceptance factor". This factor
effectively allows a given degree of freedom on velocity vector gradient inside the (MxM)
area and if the calculated gradient is larger than set, the central vector is removed. The larger
this factor is, the less the velocity vector map is spatially corrected. On the other hand, with
low factor values, the vector map is smoothed at a level that removed all
Example of validation settings for PIV analysis using adaptive correlation methodology.
16.118.4 Interrogation  area  offset
Velocity vectors are estimated from mean particle displacement inside interrogation areas
(IA). Mathematically,                                                         , where 'D' is the displacement and 'u' is
the velocity, is the main formula used to calculate velocity vectors. This formula is transformed
into an algebraic equation either using a Central Difference Scheme or a Forward Difference
Scheme.
The Central Difference Scheme is equivalent to a three-point symmetric algorithm for the
evaluation of                 , with a reference 'point' created at the time t+. The Forward Dif-
ference Scheme, on the other hand, considers the temporal reference t0.
Note: The Central Difference Scheme is mathematically the most accurate methodology and
therefore shall be preferred with PIV measurements. When processing further advanced
measurements such as PIV/LIF, the Forward Difference Scheme shall be used because the
LIF-image will get the same temporal reference (t0) as the velocity vector map, which will not
be the case with the Central Difference Scheme.
PAGE | 952
PAGE | 953
16.118.5 High  Accuracy and Deforming Windows
The idea behind the High Accuracy and deforming windows PIV algorithm is to:
l  Use a signal analysis approach without image interpolation
l  Optimize the signal strength by window off-set
l  Optimize signal strength by capturing particle drop-out due to velocity gradients
l  Achieve bias free measurements through improved sub-pixel interpolation
l  Achieve high sub-pixel accuracy independent of correlation peak shape
l  Minimize displacement estimate errors by use of adaptive deforming windows
Establishment of high accuracy  sub-pixel interpolation
The benefits of Gaussian and Parabolic fitting of the correlation peaks were the fast com-
puting speed. However, both methods have in their nature limits, simply because of the pre-
assumption of the correlation peak shape. The result is a bias error, which is often described
as peak locking. Further when particle images convoluted with velocity distribution (the later
being the most important) produce non-ideal correlation peaks, the result is basically incor-
rect.
The high accuracy sub-pixel algorithm used is independent of particle image shape and cor-
relation peak shape. The method works on individual correlation peaks. The high accuracy is
achieved by using the full information in the correlation function and not just the nine highest
values in the correlation plane.
Adaptive and deforming windows
As in any signal analysis, windowing and zero-padding discrete data is required in order to
avoid aliasing etc. In standard Adaptive correlation the situation is like shown below:
A typical choice for PIV is a round Hanning window. However, the windowing does not take
into account that there are velocity gradients in the flow. Hence ideally cross correlation
should be between windows, which follows the flow gradients.
This can be adapted in an iterative loop, where interrogation area size and shape is chosen to
suit the velocity gradients. This procedure gradually builds up the signal strength and result in
successful reduction the interrogation spot to an absolute minimum.
When the adaptive deforming window is applied in non-integer steps, the iterative capture of
the two interrogation spots further ensures that particle images on the border of the inter-
rogation regions are equally weighted by the window function. This is particular important
when reducing the size of the interrogation spots to a minimum, because non-equal weighted
border particles, will slightly bias the measured displacement.
Flow diagram
In the standard Adaptive correlation, the two interrogation windows are discrete offset based
on an estimate from the previous displacement estimates with the same or larger inter-
rogation regions.
Using the adaptive deforming windows, the displacement estimates must be with sub-pixel
accuracy. We have chosen to use the Gaussian sub-pixel estimate for this iteration in order to
increase computing speed. The high accuracy is applied in order to have the bias free estim-
ate at the end.
Example, picture from the PIV Challenge 2003
Purple vectors show results from a previous pass in the iterative method. The dotted green
squares show nominal positions of the interrogation areas. Blue and orange squares show off-
sets on frames 1 and 2 respectively. Colored ellipsis shows the deformed interrogation spots
with each line representing 10, 50 and 90 % weighting values. Please note that windows are
not necessarily centered within the square areas; This allows for non-integer interrogation
spot offset.
PAGE | 954
PAGE | 955
Interrogation spots are overlapped showing only the 10 % window limits. Particles from
frame 1 are shades of blue, particles from frame 2 shades of orange. Overlapping particle
images become shades of gray. The resulting correlation plane; On account of the inter-
rogation area offset and non-centered windows, a peak near the center is expected. Peaks
outside the dotted gray line are considered outliers.
Achieving high accuracy  in practice
With the combined algorithm of adaptive deforming windows and high accuracy sub-pixel
interpolation, there is improved signal strength, hence opening the possibility to decrease
the interrogation spots. Decreasing the interrogation spot and having high accuracy allows
for small particle displacements, which combined will result in increased spatial resolution.
It is however evident that these new algorithms are pushing the results so far that the fun-
damental bandwidth limitations of the PIV signal is challenged and other errors in the signal
forming needs very careful attention and craftsmanship.
This need of investigating the accuracy can be illustrated in this experiment: A micrometer
stage rotates the target plate by means of a leverage arm. The arm is 1867 mm and the ver-
tical displacement is 2.5 mm. Mid.: Enlargement of the particle images, simulated by sand-
paper. The F-number was 2.8, 8 and 11, Left: Resulting displacement map.
Resulting PDF of the U displacement for #F2.8, #F8 and #F11 for the above setup. The dis-
placement is mechanical imposed and known (above figure). Top: 9-point Gaussian sub-pixel fit
(cyan). Bottom: High accuracy method (green). The single PDF on the right hand (blue) is
made from the curve fitted data and expresses the expected PDF.
PAGE | 956
PAGE | 957
Read more: Westergaard, Madsen, Marassi and Tomasini, Accuracy of PIV signals in theory
and practice, 5th International Symposium on Particle Image Velocimetry, Busan, Korea,
September 22-24, 2003, paper 3301.
16.119 Proper  Orthogonal  Decomposition,  POD  (Legacy
Method)
This analysis method is obsolete and included for backward compatibility only.
The recipe is read only and no further development or maintenance will be made.
It will be removed completely in a future release of DynamicStudio.
We recommend using "Proper Orthogonal Decomposition (POD Analysis)" on
page?718 or "POD Reconstruction" on page?742 instead.
Proper Orthogonal Decomposition is a powerful method for system identification aiming at
obtaining low-dimensional approximate descriptions for multi-dimensional systems. The POD
provides a basis for the modal decomposition of a system of functions, as in the case of data
acquired through experiments. It provides an efficient way of capturing dominant com-
ponents of a multi-dimensional system and representing it to a desired precision by using a rel-
evant set of modes, thus reducing the order of the system.
POD consists of two parts, taking the "snapshot" of a series of data, and then "projecting" the
data through a selection of modes.
POD can be applied to scalar- or vector maps (conventional 2-component or stereoscopic 3-
component).
16.119.1 POD Snapshot
The Proper Orthogonal Decomposition, POD, is closely related to Principal Component Ana-
lysis, PCA, from linear algebra and was first introduced in the context of Fluid Mechanics by
Lumley [1]. This implementation of POD applies the so-called "Snapshot POD" proposed by
Sirovich [2]:
Each instantaneous PIV?measurement is considered a snapshot of the flow. An analysis is then
performed on a series of snapshots acquired in the same position and under identical exper-
imental conditions. The first step is to calculate the mean velocity field from all the snapshots.
The mean velocity field is considered the zero'th mode of the POD. Subtracting the mean
from all snapshots, the rest of the analysis operates on the fluctuating parts of the velocity
components (umn, vmn, wmn) where u, v & w denote the fluctuating part of each velocity com-
ponent. Index m runs through the M positions (and components) of velocity vectors in each
snapshot and index n runs through the N snapshots so umn=?u?(xm,ym,tn).
All fluctuating velocity components from the N snapshots are arranged in a matrix U such that
each column contain all data from a specific snapshot:
If input is a series of 2-D vector maps there will be no w-components of velocity and the lower
third of the U-matrix is simply omitted. Similarly if input is a series of scalar maps only the
upper third of the U-matrix is used, where the fluctuating part of the scalar values replaces
the u-components of velocity while v- and w-components are omitted.
From the U-matrix create the NxN autocovariance matrix C as:
-and solve the corresponding eigenvalue problem:
-where λi and Φi are corresponding eigen-values and -vectors.
Solutions are ordered according to the size of their eigenvalues:
Since the subtracted mean value was calculated from the data itself the N'th eigenvalue will
always be zero (ignoring round-off errors) and can in practice be discarded.
The eigen-vectors corresponding to each of the eigen-values can be combined with the U-mat-
rix to compute the eigen-functions, which in turn are normalized to get the POD modes:
-where Φi is the i'th eigenvector corresponding to the eigenvalue λi and modes are nor-
malized by the discrete 2-norm defined as:
PAGE | 958
PAGE | 959
POD Snapshot analysis is implemented in DynamicStudio as a separate analysis method, that
take no input parameters and require a single ensemble of scalar or vector maps as input:
As described above the Snapshot analysis will generate all the POD modes and their cor-
responding energies (=the eigenvalues) and the Modal energy distribution is shown graph-
ically when the analysis is complete:
It can be shown [3] that the amount of total kinetic energy from velocity fluctuations in the
snapshots associated with a given POD mode is proportional to the corresponding eigenvalue.
The ordering of the eigen-values, eigen-vectors and eigen-functions therefore ensures that
the most important modes in terms of energy are the first modes. This usually means that the
first modes will be associated with large scale flow structures.
In the example above modes 1 &?2 are clearly dominating, while modes 3-8 may be relevant
and the drop and subsequent flattening of the curve suggests that modes 9 and up may
simply describe noise. This means that modes 1 & 2 will be suitable for describing "typical" fluc-
tuations, but if a specific snapshot contain some kind of unusual fluctuation this might be bet-
ter described by a higher order mode.
16.119.2 POD Projection
Each of the snapshots from which the POD?modes were determined can be expanded in a
series of the POD?modes with expansion coefficients an for each POD mode n. The expansion
coefficients, also called POD?coefficients, are determined by projecting the fluctuating part of
the velocity field onto the POD modes:
-where each of the POD modes φi occupy a column in the mode matrix Ψ:
Knowing the POD?coefficients an we can reconstruct the corresponding velocity vectors by
expansion:
-since the first modes contain the most energetic parts of the flow, we may choose to exclude
the higher modes from the reconstruction (assuming they represent noise), or we may
exclude the lowest modes from the reconstruction if we are looking for medium or small scale
structures in the flow.
In DynamicStudio POD Projection is implemented as a separate analysis, that require two
inputs, the parent vector or scalar maps and a set of POD modes from POD Snapshot onto
which you wish to perform the projection.
Normally the input scalar or vector maps will be the same as the ones from which the POD
modes were generated, but any scalar or vector map can be projected onto a set of POD
PAGE | 960
PAGE | 961
modes provided it has the same grid size and dimensionality (scalar, 2-D or 3-D vector) as the
modes. (This will of course make sense only if the input data and the POD?modes describe sim-
ilar flows).
The POD Projection contain two groups of analyses, one to perform actual reconstruction (pro-
jection) of the input data and another to investigate the modes and their contributions to
reconstructions over time (i.e. the datasets in the parent ensemble).
Reconstruction
When reconstructing input scalar or vector maps you may choose to specify exactly which
modes to include in the reconstruction or you may choose to have a specified fraction of the
total energy reconstructed:
l  Enter a comma separated list of modes (f.ex. 0-2, 4-5, …). Please remember mode 0 if you
want the mean flow included.
... OR ...
l  Specify an energy fraction (the software will include as many modes as required to
recover ? the requested energy fraction).
-When projecting by energy fraction the default is to include mode 1 and up until the reques-
ted energy fraction has been recovered, but you may also choose to 'Sort by dominance',
meaning that for the reconstruction of the current input the software will take the mode with
the largest POD?coefficient first even if it is not mode 1. If this does not provide the reques-
ted energy fraction the mode with the second largest POD?coefficient will be included, then
the third largest and so on until the requested energy fraction has been recovered.
The example above shows an input vector map (with the mean subtracted) and the same vec-
tor map reconstructed from the first two modes of a POD?Snapshot.
Modal investigation
Instead of reconstructing the input data you may choose to investigate the modes them-
selves or their contributions to the reconstruction of individual snapshots:
l  'Extract modes only' will extract the modes from the snapshot dataset and store them as a
conventional ensemble that you can browse trough.
With this option the parent data are in fact not used, but even so it is important that the
number of input datasets match the number of modes.
This is easily ensured by choosing the ensemble from which the modes were generated as
parent.
Choosing a different parent is possible, but makes little sense since the parent is in fact
not used when extracting the POD modes.
Examples:
l  'Time history of POD?coefficients' will project the parent datasets onto the modes, but
return the POD?coefficients instead of performing the reconstruction.
The results are shown in an X/Y-plot, allowing you to investigate how much each mode con-
tributes to each snapshot over time or how modes relate to one another:
PAGE | 962
PAGE | 963
The example above illustrates the POD?coefficients for modes 1 & 2 when a series of vector
maps are projected onto the POD?modes. In this case the input data are obviously time-
resolved since a cyclic behavior can be clearly seen. The POD as such does not require Time-
Resolved input, but a plot as the one above makes little sense if it is not.
The example above shows the exact same data as before, but plots POD coefficients for
mode?1 vs mode?2 instead of plotting them both vs time.
This so-called phase portrait will help identify mode pairs describing cyclic phenomena even if
the input data is not time-resolved.
16.119.3 References
[1] J. L. Lumley (1967):
"The structure of inhomogeneous turbulent flow".
-In A. M. Yaglom and V. I. Tatarski, editors:
"Atmospheric Turbulence and Radio Wave Propagation", pages 166-178.
[2] L. Sirovich (1987):
"Turbulence and the dynamics of coherent structures. Part I: Coherent structures."
Quart. Appl. Math., 45(3):561-571.
[3] K. Fukunaga (1990):
"Introduction to Statistical Pattern Recognition".
Academic Press, 2nd edition.
[4] P. Holmes, J. L. Lumley & G. Berkooz (1998):
"Turbulence, coherent structures, dynamical systems and symmetry".
Cambridge monographs on mechanics. Cambridge University Press.
[5] J. M. Pedersen (2003).
"Analysis of Planar Measurements of Turbulent Flows".
PhD thesis, Department of Mechanical Engineering, Technical University of Denmark.
[6] K. E. Meyer, D. Cavar & J. M. Pedersen (2007):
"POD as tool for comparison of PIV and LES data".
7th International Symposium on Particle Image Velocimetry. Rome, Italy, September 11?14, 2007.
PAGE | 964
PAGE | 965
17 Data Exchange
You can import images into DynamicStudio using the Image Import option from the File
Menu.
Data can be exported from DynamicStudio to disk through different methods:
l  Image export: The dataset is exported as an image that can be viewed and processed by
external programs
l  Numeric data: The data is exported as a numeric representation to a file. The data rep-
resentation is similar to that shown in the numeric view.
l  Ensemble zip export: The selected ensembles can be zipped into zip file representing a
sub set of the existing database.
l  Dataset export: Specific datasets can be selected from the content list to a zip file.
l  Dataset export with dependencies: The selected dataset and its dependencies are com-
pressed in a zip file.
17.1 Image  Import
Available from the context menu of an existing database, project, run and image ensemble. It
is not possible to import images into a project containing acquired images.
17.1.1 Formats
The import function allows to import images in the following formats:
File extension          File type
.bmp                          Windows Bitmap file
.tif, .tiff                      TIFF Tagged Image File Format file
.jpg, .jpeg                  JPEG File Interchange Format file
.png                           PNG?Portable Network Graphics
.raw                           Raw file format
.im7, .imx                  DaVis Binary Image File
The selected record from where the import is initiated determines the location of imported
images in the database:
l  If the selected record is the database root, a new project and run will be created.
l  If the selected record is a project, a new run will be created.
l  If the selected record is a run, a new ensemble will be created.
l  If the selected record is an image ensemble, the images will be appended, if size and pixel
depth matches existing images.
JPEGs, PNGs, and Windows Bitmaps are always converted and imported as 8-bit grayscale
images. The accepted input formats are 8 bpp indexed, 24 bpp RGB, 32 bpp RGB and 32 bpp
ARGB. TIFF images are imported as either as 8-bit or 16-bit, depending on the input format.
For the ".raw" file format, the size and pixel depth of the image are usually not stored in the
image. DynamicStudio will try to retrieve this information from the file by reading the "Image
Width", "Image Height" and "Image Bit-Depth" properties. If the properties are not available,
the user will be asked to manually input them.
17.1.2 How  to Import Images
Right-click on the database, project, run or ensemble in the database. Expand theImport
images  > option from the context menu and select either  Import as  single-frame... or
Import as  double-frame..., depending on whether you want images imported to a single-
frame or double-frame image ensemble. A file dialog window will appear, where you can
choose the images you wish to import. Once you have selected the images you wish to import,
press Open, and the import images dialog will be shown. Depending on whether you choose
Import as  single-frame... or Import as  double-frame..., the dialog will differ slightly.
Single-frame import
The single-frame image import dialog is shown on Figure 1. The selected images to import are
shown in the list on the left side of the dialog. You can use the Add images to open another
file dialog to add additional images to import, as well as the Add?folder to add an entire folder.
The Images  properties in the lower left corner shows the image size, pixel depth and num-
ber of frames in the each of the images chosen for import. It is only possible to add additional
images, which have the same properties as the already chosen images. The Remove images
button will remove the images selected in the list. You can select images in the list by clicking
on them with the mouse. You can use the Shift key and/or Ctrl key to select multiple images. It
is also possible to drag-and-drop the selected images in the list to reorder the images, which
will affect the order they have in the ensemble as well as their timestamps. You can also press
the Name header to sort the images alphabetically in ascending or descending order.
PAGE | 966
PAGE | 967
Figure 1)  Single-frame import images  dialog.
When you have added and ordered all the images you want imported to the list, it is time to
choose the camera settings. This is described in the section called Camera selection further
down in this document. If you are importing to an existing ensemble, the camera selection
option will be disabled.
Double-frame import
The double-frame image import dialog is shown on Figure 2.
Figure 2)?Double-frame import images  dialog
This dialog has an additional list of images labeled "Frame 2". When importing, the images on
the same position in the two lists will be matched together to create a single double-frame
image. Note that if the imported images are already double-frame images, like double-frame TIFF
files, then the "Frame 2" list will not appear and the two frames will be taken from the same file.
Above each list of images, a text is added with "Image count:?" to show how many images are
in the list below. The text will be red, if there are not an equal amount of images in each list
When images are added, they will initially be added at the end of the frame 1 list. There are
several ways to move the images between frame 1 and frame 2 to balance the two lists. The
images can be drag-and-dropped by selecting one or move images from a list and dragging
them to the other list. Alternatively the buttons between the two lists can be used to move
images between the two lists. They are described in Table 1 below.
Move selected images in frame 1 to frame 2.                                                   Flip the
content of
the two
lists.
Move selected images in frame 2 to frame 1.                                                   Move all
images in
frame 2 to
frame 1.
Move the second half of the images to frame 2.                                              Move
every
second
image to
frame 2.
Table 1)?Buttons  for moving images  between frame 1  and 2.
PAGE | 968
PAGE | 969
Finally, it is possible to write an expression in the text box below the "Frame 2"?list and clicking
Apply. All the images which match the expression will be placed in the frame 2 list, while the
images which do not match will be placed in the frame 1 list. Wildcard matching is used per
default, where "*" matches any sequence of characters including no characters, "?"?matches
any single character, and any other character matches that exact character. For instance, the
expression "*-2.png" will match any filename ending with "-2.png", which would properly sort
the images shown in Figure 2. Alternatively, regular expressions can be used by checking the
check box below the text field. Regular expressions are beyond the scope of this help, but
there are many guides available online. The supported syntax is described on Microsoft web-
site.
Camera selection
Once you have selected the images for import, it is necessary to select the camera to be asso-
ciated with the created image ensemble. The drop down menu under camera selection can be
used to choose the camera source as shown on Figure 3.
Figure 3)?Selecting camera source.
You can choose to create a new custom camera, choose an existing camera from the same
project or database, or create a new camera based on a device library template. If you want
to create a new camera, you must fill in the cameras properties in the property sheet below
the drop down menu, including the cameras name, pixel pitch and sensor size as shown on Fig-
ure 4. If you want to choose an existing camera, the name option is changed to a drop down
menu where you can select the camera you wish to use and you will not be able to change the
camera properties as shown in Figure 5. Note that when importing to an existing ensemble, the
camera selection will be disabled as the camera from the existing ensemble is used.
Figure 4)?Creating a new camera.                       Figure 5)?Choosing an existing camera.
Completing the import
Once the images and camera is chosen, you must set the right timing parameters and the
ensemble name.
There are two different timing parameters, "Time between pulses" and "Trigger rate". Note
that the time between pulses is only available for double-frame import and only when creating a
new run. If you are importing to an existing run, the time between pulses for the existing run is
used. Similarly, if you are importing to an existing ensemble with at least two images in it, the trig-
ger rate will be taken from the existing ensemble by looking at the timestamps of two consecutive
images.
The ensemble name is automatically named after the camera, but it is possible to manually
input an alternative name. Note that if you are importing to an existing ensemble, it is not pos-
sible to change the ensemble name here.
Finally, press the import button to begin the import of the images.
17.1.3 Note
If you want to use your imported images as calibration images you can move them to a cal-
ibration ensemble afterwards.
17.2 Image  Export
Image export function can be opened by right-clicking an ensemble or a collection of images
and selecting export.
PAGE | 970
PAGE | 971
17.2.1 Formats
The export function allows to export data in the following formats:
File extension          File type
.bmp                          Windows Bitmap file
.tif                               TIFF Tagged Image File Format file
.jpg                             JPEG File Interchange Format file
.avi                             Video for Windows Movie file
.emf                           Windows Enhanced Metafile file
.raw                           raw format, only pixel data is saved to the file
When one of the image or movie formats is selected the display representation is exported;
for Images this means the image itself, and for Vector plots an image of the vector plot.
Image Export
When JPEG or Windows Bitmap formats are selected the images are represented in only 8-bit.
This means, that when an Image with a higher resolution is exported the original image res-
olution is reduced by resampling the pixel values, resulting in loss of information. If loss-less
export of Images is required, choose the TIFF format. Exported images in the TIFF format will
contain full pixel information in 8-bit, 16-bit and 32-bit floating point using LSbit-first fill order,
- currently suitable for all image formats. If double frames are exported in this format the
both frames will be contained in a single TIFF file.
Note
Most image viewers and tools in Windows does not support 16-bit integer or 32-bit floating
point tiff images. An example of a image processing tool that do support these tiff formats is
ImageJ (http://rsbweb.nih.gov/ij/)
If double frames are exported in Bitmap or JPEG a image file will be created for each frame
and signed with the letter a or b indicating frame a or frame b.
When exporting tot eh "raw" format, information on the image size and pixel depth is stored
as properties on the file. These properties is not visible in Windows, but can be retrieved by
use of special tools found on the Internet. The property names are "Image Width", "Image
Height" and "Image Bit-Depth".
Pixel data is stored as a stream of pixel values, row by row, starting from the upper left corner
of the image. If pixel depth is 8 bit, then only one byte per pixel is used. If pixel depth is more
than 8 two bytes per pixel is written to the file(little endian format).
Please note that some of the export formats takes very long time for exporting; especially
when large Images are exported as Text format.
AVI Export
When exporting images or vectors to AVI, movies are always saved as 8-bit using the current
LUT information.
Vector Export
When exporting Vector plots it is possible to save images as Windows Enhanced Metafiles.
17.2.2 File Format
The exported data sets are saved into a Destination path, using a Base file name, File type and
Index to construct the resulting export file name.?
<Destination path>\<Base file name>.<Ensemble id>.<Index>.<File type>
The index is prefixed with 0, zeros are added until 6 digits is reached. This format is useful
when listing and sorting in the Windows Explorer. The Ensemble id uniquely identifies the
ensemble record, ensuring unique output file names.
17.2.3 Enhanced image quality
Prior to exporting images, it is possible to enhance image contrast or to change color for
example.
l  From the images to export, open the color map and histogram (right mouse click)
l  Adjust the ColorMap(contrast), select grayscale or color display
l  Close the "color map and histogram" window and save the new display settings as Global
color map.
l  The images can now be exported with the new display settings
PAGE | 972
PAGE | 973
17.2.4 How  to Export Data
Single data sets can be exported from within an expanded ensemble by right-clicking on the
data set index or name. All data sets in an ensemble can be exported by right-clicking on the
ensemble record.
1.   Select the ensemble or data set inside the ensemble to export.
2.   Right-click on the record and select Export....
3.   Fill the Export dialog
Destination path is remembered between sessions and is default set to \<database
folder>\Exports\
Base file name is preset by the name of the selected ensemble
File type is remembered between sessions and is default set to .tiff format.
Start index is preset to 1.
4.   Press Ok to start exporting.
5.   During export a progress bar is displayed. It is not possible to perform other tasks in
DynamicStudio while exporting, but the export can be aborted by pressing Cancel.
When exporting in Movie format it is furthermore possible to specify the Playback and Com-
pression rates. The Playback rate determines how fast the movie should show the images or
frames pr. second (fps). The Compression rate determines the quality of the movie, a high
compression (close to 100%) gives poor quality but a small movie file, a low compression rate
(close to 0%) gives high quality and a large movie file. Default the Frame rate is set to 10 fps.
and the Compression rate to 70 %.
17.3 Numeric  Export
Numerical export function can be opened by right-clicking an ensemble or a collection of
images and selecting export.
Numerical export allows you to export datasets to the following formats:
File extension                                                                File type
.CSV                                                                         Comma separated values.
.TAB                                                                         Tab delimited text file.
.DAT                                                                        Tecplot data file.
.XML                                                                        Extensible Markup Language
Basics
In order to export the following information must be specified:
l  Path (path to the directory to export to)
l  Base name (first part of the name of the export file)
l  Index (second part of the name of the export file)
l  Export type (the type of export to perform)
A Preview button is available to see the export result of a given export setup on the first data-
set.
Includes
When exporting numerically it is possible to add properties to the export file. The properties
are as follows:
l  Originator (what ensemble does the export come from)
l  CameraInfo (camera name and index)
l  Custom Properties (properties that have been added by the user to the project, run and
ensemble)
l  TimeStamp (timestamp of the data)
Columns  select
It is possible to deselect the column data that is not needed.
Here it is also possible to select number of decimals for the individual values.
Export Types
l  CSV
l  TAB
l  DAT
l  XML
CSV
The csv file contains a header section ">>*HEADER*<<" and a data section ">>*DATA*<<".
These strings denote the start of each section.
The header section will always contain a line for file id and version.
If properties has been added to the export. Additional lines will be added to include these.
One line pr. property.
>>*HEADER*<<
FileID:DSExport.CSV
Version:1
Originator:deleteme1.New Project.New Run.Cross 32 50%
CameraInfo:Cam.9
Custom Properties:AnalysisEnsemble.Coordinates.X.Double:;
AnalysisEnsemble.Coordinates.Y.Double:;
AnalysisEnsemble.Coordinates.Z.Double:;
AnalysisEnsemble.Variables.Concentration.Double:;
AcquiredImageEnsemble.Variables.pH.Double:;
AcquiredImageEnsemble.Variables.Temperature.Double:;
AcquiredImageEnsemble.Variables.Integrationtime.Double:;
TimeStamp:0,0005
PAGE | 974
PAGE | 975
>>*DATA*<<
x;y;x (pix);y (pix);x (mm);y (mm);U pix;Length;Status
0;0;15,5;15,5;0,1178;0,1178;0,000689571882168438;
3,89431493223103E-05;0
0;1;15,5;31,5;0,1178;0,2394;0,000510268292648681;
1,06871219396756E-05;0
.
.
.
The Custom properties is actually one line in the file and the format is:
name:value;name :value ...
TAB
See CSV for details on header.
the data is arranged the same way except that they are separated by a tabulator character.
DAT
this format is a tecplot format that includes that columns that are selected and the properties
are added to the DAT file as DATASETAUXDATA values.
XML
this format is structured as follows:
<?xml version="1.0" encoding="UTF-8"?>
<Export>
<info id="DSExport.XML" version="1">
.
.
.
</info>
<data>
.
.
.
</data>
</Export>
Every property is located in the info tag.
All data columns are located in the data tag.
A property is constructed as follows:
<property name="XXXXX">
<type>XXXXX</type>
<value>XXXXX</value>
</property>
The name attribute denotes the name of the property.
Type can be any of the microsoft defined typecodes1
Value corresponding to the type.
1Empty, Object, DBNull, Boolean, Char, SByte, Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64,
Single, Double, Decimal, DateTime, String
The following shows how the custom properties are arranged:
<property name="Custom Properties" array="true">
<param recordtype="AnalysisEnsemble"
category="Coordinates" name="X">
<type>Double</type>
<value>0.034</value>
</param>
<param recordtype="AnalysisEnsemble"
category="Coordinates" name="Y">
<type>Double</type>
<value>9.455</value>
</param>
<param recordtype="AnalysisEnsemble"
category="Coordinates" name="Z">
<type>Double</type>
<value>6.6</value>
</param>
.
.
.
</property>
The property has the attribute array to indicate that this property is an array of several para-
meters.
The parameters are NOT necessarily the same type.
Data are arranged in the following way.
<datacolumn title="x">
<values type="Int32"
seperator=";">
95;98;98;98;98;98;98;98
</values>
</datacolumn>
<datacolumn title="y">
<values type="Int32"
seperator=";">
0;57;58;59;60;61;62;63;64;65
</values>
</datacolumn>
<datacolumn title="x (pix)">
<values type="Double"
seperator=";">
15,5;1583,5;1583,5;1583,5;1583,5;
</values>
</datacolumn>
The title indicates the name of the column
The type is specified by microsoft typecodes.
The separator attribute indicates the character used to separate the values.
PAGE | 976
PAGE | 977
18 Displays
Displays are windows for plotting, graphing and visualizing data on the screen. Displays are
shown in multiple windows inside DynamicStudio.
There are multiple display-types designed to visualize different types of data. A few are ded-
icated to show results from a specific analysis, while many are general purpose displays, used
to show results from several different analysis methods.
Examples frequently used in DynamicStudio are: ...
l  Image map
l  Vector map
l  Scalar map
Most data can also be shown numerically by right-clicking the data ensemble in the database
tree and picking 'Open as Numeric' in the context menu, or by selecting the ensemble and
clicking the     -icon in the toolbar. The actual content of the resulting Numerical Display will of
course depend on the parent data-set and -type.
For most display types there are 'Display Options' allowing you to adjust or modify the visual
appearance of the display. These are typically accessed by right-clicking somewhere in the dis-
play window and then picking 'Display Options...' from the context menu. For many display
types you can also access the Display Options by simply double-clicking somewhere inside the
display window.
18.1 General  Display Interaction
There are several ways to interact with a display.
Quick access features for the display are zooming and moving the image around.
Zooming is done by holding the left mouse button down while moving the mouse. This will dis-
play a rectangle around the area that will be zoomed to when releasing the mouse button.
Moving the image around is useful when zooming has been done. This is done by holding the
Ctrl key and the left mouse button down while moving the mouse.
Right clicking the mouse makes a context menu pop up.
The context menu makes it easy to access specific dataset display options, coloring, zooming,
ruler settings, recipes, analysis and other general display options.
For more details please refer to "Displays" above or to the Help Page for the Analysis Method
in question.
18.2 Zoom  and  Pan
In most 2D displays you can zoom in on specific areas and out again to get back to the over-
view. While zoomed in you can also pan the display left, right, up and down. Examples below
PAGE | 978
PAGE | 979
show images, but you can zoom and pan in e.g. vector and scalar maps the same way.
18.2.1 Zooming
There are several ways to zoom in:
You can left-click and drag with the mouse to draw a box around the area you wish to zoom in
on, the zooming will take place when you release the left mouse button.
You can also point with the mouse somewhere within the display window and then use the
scroll wheel on the mouse while pressing Ctrl on the keyboard. This will zoom in or out around
the location of the mouse cursor.
Finally you can right-click the inside the display window and pick one of the zoom options from
the context menu:
The current zoom level is shown along with various options to change it (characters to the
right are hotkeys):
Taking it from the bottom and up you can zoom in or out in steps of 5% by pressing Ctrl and
PgUp/PgDn simultaneously.
Then there are a number of fixed zoom level you can choose from (for images 100% means
that one pixel in the image is shown as 1 pixel on the screen, for vector and scalar maps 100%
mean that vectors or scalars that are for example 16 pixels apart are also shown 16 pixels
apart).
'Fit content to window' will adjust the zoom level of the display to make it fit within the current
display window .
'Fit window to content' will increase or reduce the display window size to match the size of the
dataset at the current zoom level (limited by screen size and/or the program window in which
DynamicStudio itself is running).
If you zoom in far enough on an image numerical grayscale values will appear:
It happens when displayed pixels are large enough for the grayscale value to be shown inside
them, so there is no fixed zoom level at which this occurs. It is sooner for an 8-bit image with 3
digits than for a 10- or 12-bit image with 4 and later still for a floating point image requiring
even more digits. The numerical display of pixel values can be switched off (or back on) in
Image Map Display Options if you don't want to see them.
No similar functionality exist for Scalar Maps even if they resemble floating point images, but
you can see the scalar value that the mouse is over in the info bar at the bottom left-hand
corner of the DynamicStudio program window.
Please note: For image displays horizontal and/or vertical artifacts may appear for 'odd' zoom
levels even if no such artifacts are present in the actual image data. If an image looks 'strange'
please try changing the zoom factor (ideally to 100%) to see if it's just a display issue or in the
actual data.
18.2.2 Panning
If you're zoomed in so far that the data cannot fit inside the display window, scrollbars will
appear in the bottom and/or right-hand side of the display (See the figure above).
The recommended way to pan left, right, up or down is to click and drag with the mouse some-
where inside the display window while pressing Ctrl on the keyboard.
You can also pan by clicking and dragging the scrollbar handles, click the scrollbar arrows
(small steps) or click in the bright area between arrow and handle (large steps).
PAGE | 980
PAGE | 981
Finally you can pan using the scroll-wheel on the mouse: If you do it while pressing Shift the dis-
play will pan left or right and if you do it without pressing any keys it will pan up or down.
18.3 Display Rulers
For most 2D displays you can switch on rulers at the bottom and left hand side of the Display
window.
Simply right-click inside the display and pick the Ruler option that you want from the context
menu:
For acquired images and data derived directly from them 'Pixels' is a valid option, while mean-
ingful 'Metric' rulers require that you have calibrated your camera, using either "Measure
Scale Factor" on page?1056, or "Define Axes" on page?1055.
Examples below show Pixel rulers on an image display and Metric rulers on a vector map
derived from it:
In these examples the origin (0,0) are in the default lower left corner of the image / vector
map, but it is possible for it to move to somewhere inside or even completely outside of the
image.
Especially when performing Stereo PIV or dewarping an Image, Vector or Scalar Map it is
quite normal for the origin to move to somewhere inside the display window (See "Image
Dewarping" on page?501, "Vector Dewarping" on page?880 or "Stereo-PIV" on page?838, that
rely on dewarping two image or vector maps to a common grid). All of these rely on an "Ima-
ging model fit (Legacy Method)" on page?556 to describe the mapping from object space (met-
ric) to image plane (pixel).
Results from these analysis methods can no longer be related directly to specific pixels on a
specific camera and 'Pixel Rulers' are thus no longer an option.
18.4 Using  the  display from  within  an  analysis  method
The view of an image can be controlled through the context menu that appears when right
clicking inside the image. In this popup menu the zoom level, the active frame (for a double
frame exposure) and visual appearance (Color map and histogram) can be adjusted. This func-
tionality is not implemented for the main display component
PAGE | 982
PAGE | 983
18.4.1 Zoom
The zoom level can also be adjusted by scrolling the mouse wheel button or by dragging a rect-
angle around the desired area to view. The context menu provides some fixed zoom levels as
well as an option to fit the image to the available window area.
18.4.2 Pan
If holding the <Ctrl> key while dragging inside the image the view area can be moved
(panned) around. The window scrollbars can also be used to pan the view of the image.
18.4.3 Magnifying glass
If holding down the <Alt> button while moving the mouse over the image, a "magnifying
glass" is shown. The magnifying glass can be used to quickly inspect details, without changing
the zoom level. The image below is showing the magnifying glass. Grayscale pixel values are
shown in each pixel, if zoom level is sufficiently high.
18.4.4 Color map
The color table used to display an image onto the screen can be adjusted by selecting the
menu item "Color map and histogram" from the context menu. The Color map dialog is
described in "Color map and Histogram " on page?992.
18.4.5 Adjusting the ellipse
The position, size and rotation of the ellipse can be changed by mouse interaction. The entire
ellipse is moved by using the left mouse button to drag the ellipse to its new location. When
the ellipse is selected or while hovering the mouse over the ellipse its adjustments handles is
displayed as illustrated in the image below.
By using the mouse to drag these handles to a new location it is possible to change the ellipse
size and rotation. If the <Shift> key is pressed while dragging a corner handle, the center of
the ellipse is kept at its current location. If the <Shift> key is pressed while dragging the rota-
tion handle (the right circle in the image below) the rotation angle is kept to a multiple of 45°.
When rotating the ellipse, the center of the rotation is the pivot handle seen in the middle of
the ellipse. The pivot handle can be moved to an other location to change the point around
which the ellipse is rotated.
If the <Shift> key is pressed while moving the pivot handle the pivot handle will return to the
ellipse center.
PAGE | 984
PAGE | 985
Setting the ellipse properties  using the property  dialog.
If right clicking the ellipse its context menu will be shown (left image below). When selecting
the ‘Properties…’ menu item the property dialog is shown (right image below):
In the property dialog entering the desired values of the position, rotation and size will adjust
the ellipse accordingly.
18.4.6 Adjusting the polygon
The position, size, rotation and shape of the polygon can be changed by mouse interaction.
The entire polygon is moved by using the left mouse button to drag the polygon to its new loc-
ation. When the polygon is selected or while hovering the mouse over the polygon its adjust-
ment handles are displayed as illustrated in the image below.
The polygon adjustment handles can be categorized in two groups.
1.   Handles connected to the bounding box of the polygon:
Adjusting these handles will affect the entire polygon, by changing the location, size or
rotation.
2.   Handles connected to polygon itself:
These handles are point/vertex handles and line segment handles. Adjusting these
handles will only change part of the polygon.
By using the mouse to drag these handles to a new location it is possible to change the poly-
gon shape, location, size and rotation. If the <Shift> key is pressed while dragging a corner
handle (on the polygons bounding box), the center of the polygon is kept at its current loc-
ation. If the <Shift> key is pressed while dragging the rotation handle (the right circle on the
bounding box in the image below) the rotation angle is restricted to a multiple of 45°.
The center of rotation (the pivot handle) is represented by two concentric circles (as shown in
the center of the image below). The pivot handle can be moved to another location to change
the point around which the polygon is rotated.
If the <Shift> key is pressed while moving the pivot handle, it will return to the center of the
polygon's bounding box.
Deleting a point
When right clicking on a polygon point/vertex a context menu is displayed as shown in the
screen shot below.
Selecting the menu item "Delete point" will delete the actual polygon point.
Inserting a point in the polygon
When right clicking on a polygon line segment (the line connecting two polygon points) a con-
text menu is displayed as shown in the screen shot below.?
Selecting the menu item "Split line segment" will create a new polygon point midway between
the line segments start and end points.
PAGE | 986
PAGE | 987
18.4.7 Adjusting the rectangle
The position, size and rotation of the rectangle can be changed by mouse interaction. The
entire rectangle is moved by using the left mouse button to drag the rectangle to its new loc-
ation. When the rectangle is selected or while hovering the mouse over the rectangle its
adjustments handles is displayed as illustrated in the image below.
By using the mouse to drag these handles to a new location it is possible to change the rect-
angle size and rotation. If the <Shift> key is pressed while dragging a corner handle, the cen-
ter of the rectangle is kept at its current location. If the <Shift> key is pressed while dragging
the rotation handle (the right circle in the image below) the rotation angle is kept to a multiple
of 45°.
When rotating the rectangle, the center of the rotation is the pivot handle seen in the center
of the rectangle. The pivot handle can be moved to another location to change the point
around which the rectangle is rotated.
If the <Shift> key is pressed while moving the pivot handle the pivot handle will return to the
rectangle center.
Setting the rectangle properties  using the property  dialog.
If right clicking the rectangle its context menu will be shown (left image below). When select-
ing the ‘Properties…’ menu item the property dialog is shown (right image below):
In the property dialog entering the desired values of the position, rotation and size will adjust
the rectangle accordingly.
18.5 Image  Map  Display
The Image Map Display is used to show images acquired, imported and/or processed by
DynamicStudio.
Acquired (or imported) images are typically 8-, 10- or 12-bit integer with pixel values ?0, while
processed images may be floating point with arbitrary pixel values.
DynamicStudio supports monochrome grayscale images only, but you can use various
colormaps to assign pseudo-colors to the onscreen display (See "Color map and Histogram "
on page?992). It is possible to import color images (See "Image Import" on page?965), but
they will be converted to grayscale in the process.
PAGE | 988
PAGE | 989
Grayscale and PseudoColor representation of the same image
For Double-Frame images you can press 'A' to see frame 1, 'B' to see frame 2 and 'T' to toggle
between the two. For a Single-Frame image these keys have no effect.
The Info box below the image display shows various details about the image shown:
In this case the image is 616 pixels wide and 628 pixels high, the lower left corner has pixel
coordinate (0,0), the image has a grayscale depth of 8 bit and we're presently looking at
frame 2 of a double-frame. The second line locates the image in time, relative to the other
images in the current ensemble; It is the second image in the ensemble and also the second
image acquired (the two index numbers may differ if you did not store all images after acquir-
ing them). The Time-stamp shown is relative to the start of the acquisition.
For floating point images the info box will also contain a legend illustrating which colors rep-
resent which floating point grayscale values:
Note that the grayscale depth in the first line is now 32-bits, a single precision floating point
with 24-bit mantissa and 8-bit exponent (both signed). Double precision 64-bit floating point
would have been possible, but takes up twice as much storage space and is rarely justified with
the limited 8-, 10- or 12-bit resolution of the acquired images.
The info box can be turned off (or back on) via the Context menu (right-click somewhere in the
display window). This can be useful when multiple display windows are open and you wish to
use more of the screen to display actual data.
From the context menu you can also switch on or off various rulers at the bottom and/or left-
hand edge of the display window (See "Display Rulers" on page?981 for details).
The context menu also offer a number of predefined zoom levels, please refer to "Zoom and
Pan" on page?978 for further details.
Other tools accessible from the context menu are: ...
l  "Color map and Histogram " on page?992 to see grayscale histograms and adjust how pixel
values are shown on screen.
l  "Correlation Map Display" on page?1001 to see Auto- or Cross-Correlation maps (Single- or
Double-Frame images respectively).
l  "Particle Density Probe" on page?997 to estimate particle image size, seeding density and
similar.
Follow the links above for further details on each of these topics.
Double-clicking inside an image display is a shortcut to the Color Map. In most other displays
double-clicking will open the Display Options dialog, but for images the Color Map is more fre-
quently used.
From the context menu you can also change the Background Color of the display window and
switch on/off the display of a grid on top of the image:
PAGE | 990
PAGE | 991
Background Color and Grid details are both accessible also from the Image Map Display
Options, the topmost entry in the context menu:
Background Color is common to all display windows and for Image displays it is the color above
and/or to the right of the image if height and/or width of the image shown is smaller than the
display window. By default the background is white, but if the image itself is very bright it can
be difficult to see where the image ends and the background begins. In that case using a dif-
ferent background color can be useful.
The next three control the display of a grid that can be shown on top of the image;
With the checkbox you can turn the grid on and off the same as described above for the con-
text menu item 'Show Grid'.
'Grid Color' obviously allow you to change the color of the grid-lines, while 'Grid Spacing'
determine how far apart the lines are. Default spacing of 64 pixels correspond to a typical PIV
Interrogation Area (IA) and gives you an indication of the number of particles/IA. Reducing
Grid spacing to e.g. 32 pixel will help you estimate whether seeding density is high enough to
justify a smaller PIV?Interrogation Area.
When the grid is turned on there will always be a horizontal and vertical line through the cen-
ter of the image and from there further lines are drawn above, below left and right, using the
chosen Grid Spacing. If you make the grid spacing large enough the lines through the middle
will be the only ones. This can be useful for example when lining up a multi-camera system to
ensure overlap between Field of View from each of the cameras involved. In such setups cam-
era calibration is often required anyway, so the calibration target is commonly used to focus
each camera and line up their Fields of View. (See "Imaging model fit (Legacy Method)" on
page?556 and/or "Multi Camera Calibration" on page?585 for details about camera calibration).
The last checkbox in the Image Map Display Options allow you to switch on or off the numer-
ical display of grayscale values inside each pixel when zoomed in far enough. Example and
details about this can be found on the Help page about "Zoom and Pan" on page?978.
Apart from 'Copy to Clipboard' the remaining items in the context menu are 'Transparent' and
'List of Layers'.
These are both related to "Overlays" on page?1017 and are described there (follow the link
for further details).
18.5.1 Color map and Histogram
The colormap determines the color and intensity with which a particular pixel value is
displayed. It does not affect the actual greyscale values in the image, only the way
they are shown on the screen. The Color map and histogram dialog is used to manip-
ulate the colormap and thus the on-screen appearance of images.
To bring up the Color map and histogram double-click anywhere within the "Image
Map Display" on page?988 or right-click and pick 'Color map and histogram' from the
context menu:
This will open Color map and histogram dialog as shown in the example below:.
PAGE | 992
PAGE | 993
l  The x-axis represents all possible pixel values in the image.
l  For regular greyscale images X goes from 0 to the maximum value supported
(i.e. 255/1023/4095 for an image with 8/10/12 bits per pixel).
l  For floating point images X goes from the minimum to the maximum greyscale
value present in the current image.
l  The colors on the x-axis shows how a particular pixel value will be displayed. In
this case pixel value 156 will be shown black, 866 will be shown white and values
in between will be shown with shades of blue or purple.
l  You can change the upper and/or lower limits by clicking on a value and enter a
new one or by clicking and dragging the square blue handles.
l  The colors on the y-axis show all 256 available colors in the colormap used.
l  You can change the upper and/or lower limit by clicking the number and enter a
new one or by clicking and dragging the square blue handles. This allows you to
use only part of the colormap. Setting the upper and lower limit to 170 and 85 in
the example above would for example show colors in the range from blue to
magenta, and never use black or white.
l  The histogram in the background shows the distribution of greyvalues in the
image
l  The Histogram is scaled so the maximum value is at the top of the y-axis.
l  By default the y-axis is linear, if you want it to be logarithmic check the "Log-
arithmic histogram" checkbox.
l  Statistical info shows information about the image:
l  Minimum: The minimum pixel value found in the image.
l  Maximum: the maximum pixel value found in the image
l  Number of different values: The number of different pixel values found in the
display
l  Mean: The mean pixel value
l  RMS: The Root Mean Square of the different pixel values found in the image
l  Peak index: Indicates the pixel value that has the highest pin in the histogram,
and the number of pixel found with this pixel value.
Use Global Color map
Each camera in the database has its own unique Colormap settings used when first
opening an image from that camera. By default all images from the same camera
share this common colormap and changes to one specific image will thus affect all
other images from the same camera. It is however possible to assign specific
colormap settings to each (ensemble of) image(s). To do so simply unselect 'Use global
Color map' at the top of the Colormap and histogram display.
Color coding
It is possible to select what color is to be displayed when the pixel value falls above or
below the limits.
This functionality is enabled using the check box color coding. The colors can be selec-
ted in the color coding section.
Palette selection
It is possible to chose between a number of different color palettes. To select a color
palette click on the selection box in the bottom right corner and choose a palette:
PAGE | 994
PAGE | 995
Mapping functions
Upper and lower limits on the x- and y-axes determine limit values and corresponding
display colors, while a mapping function determine the color of intermediate values.
The default is a linear mapping , but two nonlinear alternatives are available via the
context menu (right-click anywhere in the histogram window):
Selecting either Hyperbolic or Sigmoid mapping a slider will appear to the right of the
colormap display. With the slider in the middle position the mapping remains linear,
but moving it up or down will make the mapping nonlinear.
Hyperbolic mapping can be useful if/when the histogram peaks near the left or right
edge of the range and has a single-sided 'tail':
Sigmoid mapping can be useful if/when the histogram has a peak in the center and
'tails' on both sides:
Taking the slider to the top can simulate the effect of a hard threshold and taking it to
the bottom can help identify where the brightest and darkest pixels are by turning all
other pixels gray (or whatever is in the middle of the colormap used):
Invert
Clicking 'Invert' in the context menu will swap the upper and lower limits on the y-axis,
effectively reversing the order of colors in the colormap.
If you have bright particle images on a dark background 'Invert' will show a 'Negative'
image with dark particles on a bright background, which may be useful if for example
you wish to overlay display of other data.
PAGE | 996
PAGE | 997
Fit/AutoFit to Histogram
The colormap can be auto-adjusted to adapt to grayscale distribution of the image.
The adjustment sets the colormap limits so 0.1% of all pixels are below the lower limit
and 0.1% are above the upper limit, while the remaining 99,8% of all pixels are
between the upper and lower limit. 'Fit to Histogram' will adapt colormap to the cur-
rent image only, while turning on 'Autofit to Histogram' will adapt to each new image
when browsing through an image ensemble and/or toggling between images in a
doubleframe.
Shortcut: AutoFit can be turned on or off by simply double-clicking in the histogram
area.
Auto-Hide
By default the Colormap and Histogram display will remain open until actively closed
or until the corresponding parent image is closed. If you have multiple images and cor-
responding Colormap/Histograms open at the same time you may however loose
track of which belong together. In that case you may choose to hide a Colormap/His-
togram when the corresponding parent image is unselected (by clicking one of the
other image displays).
To do so right-click somewhere in the gray area of a Colormap/Histogram display and
turn on 'Auto-Hide'. (Right-click again to turn it off).
18.5.2 Particle Density Probe
Particle Density belongs to a group of image 'Probing Tools' alongside the "Correlation
Map Display" on page?1001 (=Auto-Correlation Map for Single Frame parent images) .
These probing tools all analyze subsections of the full parent image, provide live feed-
back in response to mouse movements etc, but do not store any results in the Dynam-
icStudio database.
As the name implies the 'Particle Density' probe tries to identify seeding particles
within the probing area, show the ones found and compute local particle density.
It aims at finding bright particles on a dark background and will thus not work for
shadow images. Nor will it work for floating point images, where output will be a blank
screen.
To access this tool right-click anywhere within the parent "Image Map Display" on
page?988 to bring up a context menu and select 'Particle Density':
This will open the Particle Density Display and overlay a white square on the parent
image to indicate the region currently being probed:
PAGE | 998
PAGE | 999
The Particle Density output window will show the probing area and overlay small
circles (O5 pixel) where particles have been found. The circles are saturated (=255 for
an 8-bit image, 1023 for a 10-bit image and so on), while the parent image is threshol-
ded at one grayvalue lower. This allows you to have the circles shown in a different
color, by manipulating the Color-Map for the Particle Density display. Right-click it and
select 'Color map and histogram':
The color map settings used in the example above are:
Above the probing area display there are two tabs, named 'Output' and 'Settings':
'Output' shows the number of particles found within the current probing area as well
as the corresponding average 'Seeding Density'.
The 'Source Density' indicate the fraction of pixels above the detection threshold, and
assuming particles are round we get the 'Mean Diameter' of each particle image by
combining Seeding and Source Density. Finally 'Mean Spacing' describe the average
distance between neighboring particles based on the Seeding Density.
The 'Settings' tab allow the user to influence the processing, by specifying the 'Probe
Area Size' and the 'S/N Threshold':
PAGE | 1000
PAGE | 1001
Probe Area Size defaults to 64x64 pixel, but may be set to 32x32, 48x48, 96x96,
128x128, 192x192 or 256x256 instead. Processing time increases with the size of the
probing area and with the grayscale depth of the parent image, so the largest probing
area, 256x256, may respond slowly especially for images with grayscale depth of
more than 8. For the smallest probing areas you risk finding very few particles (or
none at all), meaning that f.ex. estimated 'Average Size' may not be representative as
it describes a few distinct particles more than an actual 'Average'.
The S/N Threshold defaults to 2.5, but can be set to any value in the range 1.0 to 9.0.
Working principle…
Internally Particle Density Probe uses grayscale normalization as described in "Image
Processing Library (IPL)" (on page?525) as 'Pixel Normalization':
where gIn and gOut are Input and Output grayscale values of the pixel in question and
MEDΩ and MADΩ are the Median and Median Absolute Deviation in the spatial neigh-
borhood Ω around this pixel. The minimum noise level ε is included to avoid division by
(almost) zero in areas with more or less constant grayscale values and K is a scale
factor converting Median Absolute Deviation to Standard deviation σ (assuming back-
ground noise is Gaussian).
Both the MED (Median) and the MAD?(Median Absolute Deviation) are computed by
applying a 13x13 median filter twice. Applying it once with a rectangular neigh-
borhood may leave horizontal and/or vertical artifacts, which the second pass then
removes. The size, 13x13 pixel, has been chosen as a suitable compromise between
robustness and processing speed. Theoretically the conversion factor K should be
1.4826 if the background is Gaussian, but we use 1.5, which is good enough for prac-
tical purposes.
The minimum noise level ε is chosen as roughly 1/4 of the bits available:
Grayscale depth                                  Noise bits                             Epsilon
8-9                                                     2                                          4
10-13                                                   3                                          8
14-16                                                   4                                         16
The number of pixels that exceed the user defined S/N Threshold leads to Source
Density and greyscale peaks above the same threshold identify particles, which in turn
leads to Seeding Density.
For display purposes we return to the parent image, threshold it at one grayvalue
below saturation and superimpose saturated O5 circles around each particle found.
The particles are identified for counting purposes only, no attempt is made to locate
them with subpixel accuracy, but we strive to ensure that each particle is counted only
once.
18.5.3 Correlation  Map Display
Right-clicking anywhere within a "Image Map Display" on page?988 will bring up a con-
text menu.
From this menu it is possible to select the menu-item Cross-Correlation Map (-or
Auto-Correlation Map in the case of a single frame image). This will bring up a win-
dow that displays a surface plot of the normalized cross-/auto-correlation of a given
interrogation area within the image (the window is shown in the picture below).
A white rectangle in the image display is used to indicate where the correlation is cal-
culated. By moving the mouse inside the image display the interrogation area will fol-
low the mouse position and the surface plot will constantly be updated to reflect the
correlation at that current position.
Manipulating the view of the correlation map.
The view of the correlation map can be altered in different ways.
By clicking inside the window and dragging the mouse, the surface is rotated around
the transparent green ball seen in the center on the display above.
PAGE | 1002
PAGE | 1003
The surface can also be zoomed in and out by using the mouse wheel button. Finally
the position of the surface can be changed by holding CTRL button while dragging
inside the window.
The context menu
Right clicking the mouse inside the correlation window will display the context menu
shown below.
From this menu various settings can be adjusted.
A brief description of each setting is given below.
l  Auto-hide - will hide the surface plot if the parent display loses focus.
l  Interrogation Area Width and Interrogation Area Height is used to specify
how large the interrogation area will be.
l  Set View - will change the angle at which the surface is viewed. (options: default,
top down view)
l  Colors - can change the look of the surface or the background by applying dif-
ferent color tables.
l  Coordinate Axes - shows or hides the coordinate axes.
l  Zero Level - shows or hides a transparent plane perpendicular to the z-axis at
z=0.
l  Surface as  Wires - toggle whether surface is drawn as solid or as wires.
l  Shiny  Surface - toggle whether a shiny look is applied on the surface.
l  Animate - starts rotating the surface plot around the Z-axis of the eye-coordinate
system. The rotation speed and direction can be changed by using the mouse.
l  Close will close the window.
Normalized Cross  Correlation formula
F1() and F2() are the interrogation areas from frame 1 and 2 in the image.
F1 and F2 are the mean value of the interrogation area.
σF1 and σF2 are the standard deviation of the interrogation area.
N is the number of pixels.
The resulting correlation map will have values in the range [-1, 1], where 1 mean a perfect cor-
relation/match
18.6 Vector  Map  Display
The Vector Map Display is used to display the results obtained via a standard 2D-2C PIV or a
stereo 2D-3C stereo PIV.
18.6.1 Vector Map display options
Options menu for the vector map display is opened by double clicking the vector map, or via
the context menu entry "Display Options..."( opened by right clicking on the vector map).
Display Options for the Vector Map consists of several different tabs, that will be described in
the following.
PAGE | 1004
PAGE | 1005
Scaling?/  Subtraction
Scaling is necessary to display vectors from the selected variables. Scaling of the vectors can
be done in two different ways:
l  Auto-Scaling
l  Fixed scaling
It is possible to subtract values from the two components of the vectors. For example, con-
vective motion velocity can be removed from display in order to highlight existing vortices in
the vector map. The spatial mean value of the vector map can also be used, by checking the
corresponding or clicking the "Mean" button, as a specific value to be removed form the dis-
play.
Colors
Different colors for the vectors can be chosen. It is possible to select to have Rejected, Sub-
stituted, Outside, and Disabled vectors drawn in special colors.
It is also possible to have the vectors drawn with a color representing the length of the vector
Color Vectors
The selection of "Color vectors" will color vectors by its length, thus all other coloring of vec-
tors will be disabled. Different color maps are proposed to color the vectors with.
It is important to notice that the color is set by the length on the displayed vector, for
example, if one desires to plot only one component then the color is set in agreement with
the length of the displayed vector. This feature is very useful especially when the selected vari-
ables for the vector plot differs from the usual U and V components. ("Examples of realizable
displays" on page?1012).
To set a range for the coloring of the vectors there are three different possibilities :
l  Use fixed min & max found during browsing the ensemble
Here the minimum and maximum for the range is found during browsing the ensemble.
When ever a larger or a lower value is found compared to min and max, the value will be
used as min or max for the range. Outlier vectors will have an influence on the finding of
min and max, and will properly make the range to wide.
l  Use individual Vector map min max
Each individual vector map will de displayed with its own range found in the dataset.
As above Outlier vectors will have an influence on the finding of min and max, and will
properly make the range to wide.
l            Manual
The entries Minimum and Maximum will be used for the range. Vectors length
PAGE | 1006
PAGE | 1007
outside the range will either be given the color of the minimum or the maximum
value.
Vector map
By default U vs. V is displayed in the vector map, but it is possible to select other variables for
the vector components. This is done in the Vector map tab.
The number of different variables available will vary depending on the type of vector map dis-
played (e.g. Standard 2D-2C PIV or Stereo 3D-3C PIV).
From this tab it is also possible to suppress one or more components by checking the cor-
responding box.. This results in a pseudo profile plot.
The vector's anchor can also be changed to Tail point or Mid point using the dedicated radio
box.
Finally, it is possible to not display all the vectors by using "Index skipping". If optimized Auto-
skipping is enabled, vectors will be automatically be hidden in order to keep the frame rate
high when showing more than 50,000 vectors.
Reference Vector
A reference vector can be display in the lower left corner of the display. The vector size can
either be set manually or set to the average vector of the vector dataset.
Scalar Map
A background scalar map can be enabled by selecting a Variable from a list of available vari-
ables in the vector map. The list can vary depending on the recipe used to calculate the velo-
city field. For example the 2D LSM directly evaluates gradients from the grayscale particle
images, thus gradients will also be available for display.
PAGE | 1008
PAGE | 1009
When selecting a variable different from None it becomes possible to set a number of dif-
ferent levels for coloring.
Range for coloring the background scalar can be fixed to individual dataset maximum and min-
imum values, by checking the check box “Use full range”.
Un-checking “Use full range” makes possible to manually specify a minimum and a maximum
value that will be used for the entire dataset.
Scalar Map Style
Selecting a Variable different from None in the Scalar Map tap will enable settings in the tap
Scalar Map Style
In Scalar map Style it is possible to select how the Scalar map in the background is displayed.
The following possibilities are available:
l  Discrete
l  Follow contours
l  Only contour lines
Several different color maps are available for the display of the background scalar map, and
can be changed in "Color use".
Streamlines
Streamlines now must not be computed any more. They can just be visualized in the Display
settings. The display for the streamline display is divided into four different section. in the fist
section a grid for seed points is defined where the number of points in x-, and y-direction are
specified. In the "Streamline integration" section the direction of the integration is specified,
which means if the streamlines only go with the flow, against the flow or both. In "Streamlines
Colors" the visualization properties are defined, like in which color is the specified streamline
to be color coded in.
PAGE | 1010
PAGE | 1011
Scalar Map Interpolation
Selecting "Follow contours" or "Only contour lines" in the "Scalar Map Style" tab, will enable set-
tings in the "Scalar Map Interpolation" tab.
18.6.2 Examples of realizable displays
Vectors  only  displays
A pseudo profile plot of a measurement in boundary layer is shown in the figure below.
Only one velocity component is displayed and colored by its length ("Colors" on page?1005 and
"Vector map" on page?1007 ). Note that for a better visualization, all the measured vectors are
not plotted using the "Index skipping" feature.
The figure below represents a pseudo profile plots of RMS values of the main velocity com-
ponent in a boundary layer.
The first component of the displayed vector has been set to "Std dev (U)" and no second
PAGE | 1012
PAGE | 1013
component is used ("Vector map" on page?1007). The color is displayed according to the
length of the displayed vector (in that case the RMS value of U), thus highlighting the location
of the maximum turbulence intensity locations.
Super Imposed Vectors  and Scalar Map
The examples below shows Vector plots of mean velocity values super-imposed on Scalar
Maps of RMS values of velocity, in a boundary layer and in a subsonic jet.
18.7 Scalar  Map  Display
18.7.1 More visualization  methods…
Spatial coordinates system and scale bar for the scalar values can be added to the
mean/RMS maps. With the mouse, click on the map and select 'Info box' to get the
scalar bar.
Scaling properties are accessed via the 'Display option' or by double-click with the
mouse (Left button) on the scalar map. Adjust the range and the number of levels
according to needs.
Any variable that is present in the ensemble can be displayed by selecting it the drop
down list.
PAGE | 1014
PAGE | 1015
Additional drawing methods are also available in the 'Style' section, where drawing
style 'Follow Contours' is the default. The 'Discrete' style will typically show a pattern of
rectangular tiles of uniform color corresponding to the scalar value in each point. The
other styles interpolate between discrete scalar values to produce a smoother dis-
play, where contour lines can be used instead or added to the contour plot.
A number of different color schemes are available for showing the scalar map on the
screen.
Except with the discrete display style, the scalar map display will interpolate between
scalar values to produce a display that varies smoothly. This is accomplished by aver-
aging over a neighborhood around each and every point in the display. The neigh-
borhood size is defined by the so-called integration step size, where large values will
produce smoother displays than small ones. The software can determine a suitable
step size automatically, or let the user select one. The user defined step size can be
entered directly or relative to the average spacing between neighboring values in the
scalar map.
PAGE | 1016
PAGE | 1017
18.8 Overlays
Overlay is a way to show multiple datasets in the same display window. A classic example is to
show a vector map on top of (one of) the particle image(s) from which it was derived:
+
=
In practice it is done by opening a display of one of the datasets (e.g. the image) and then
click, drag and drop the other dataset (the vector map) from the database into the newly
opened image display.
A 'List of Layers' will be opened in the right hand side of the display window:
The topmost entry in the Layer List is drawn on top of the datasets below. In this case the vec-
tor map (Adaptive PIV) is drawn on top of the image map (Cam 1). In the gaps between the
vectors you can "see through" to the image underneath.
PAGE | 1018
PAGE | 1019
If you had dragged the image on top of the vector map the vectors would initially be hidden
underneath the image so you would not be able to see them. In such a case the arrows at the
top of the Layer List can be used to move layers up or down in the list: If the vectors are hid-
den underneath the image, click the Vector Map in the Layer List and then click      to move it
up, or click the Image Map in the Layer List and then click      to move it down.
Any of the display layers can be hidden by selecting it in the list and then clicking      at the top
of the Layer List. Clicking it once more will show the corresponding dataset again.
With the       at the top of the Layer List datasets can be removed from the list completely
(except the parent dataset onto which the other(s) have been drag/dropped, it can be hidden,
but not removed).
Please Note: When a dataset is first overlaid on top of another, the overlaid dataset will always
be the first dataset from its ensemble, regardless if the parent dataset onto which it is over-
laid is not the first in its ensemble. As soon as you browse forward or back in the parent
ensemble the system will however try to synchronize the overlaid dataset(s).
You can overlay datasets that have no relationship to one another, it is your own responsibility
that the overlays performed make any sense.
You can drag and drop further datasets into the display, they will be shown on top of what is
already there and be added at the top of the Layer List.
The example below shows 3 datasets overlaid:
In this example the master/parent dataset is a scalar map of Vorticity, derived from the vector
map at the top of the Layer List, which in turn is derived from the image in the middle of the
list. Initially the vorticity map will be hidden underneath the Image map, so we've had to
modify the image map display in two ways in order to see the vorticity again.
First the "Color map and Histogram " on page?992 of the image display was inverted so we see
dark particles on a bright background instead of bright particles on a dark background.
Secondly the image is made 'Transparent' meaning that white pixels become completely trans-
parent and 'bright' pixels partially transparent so we can see what's underneath; Click 'Trans-
parent' in the context menu.
PAGE | 1020
PAGE | 1021
Double-Clicking in a display window with overlaid datasets is ambiguous, so DynamicStudio
asks which dataset you intended:
Pick the dataset you wanted to access and click OK.
Similarly the context menu (right-click) is slightly different:
All common things are displayed as usual (not all shown above), but near the top of the con-
text menu there is an entry for each of the layers, from where you can access the context
menu entries that are specific for that dataset. In the example above the image map entry
has been chosen.
You can also overlay two (or more) images on top of one another, which can be useful to
check e.g. overlap;
In the example below two cameras have acquired simultaneous images of the same cal-
ibration target. Based on those a calibration ("Imaging model fit (Legacy Method)" on
page?556) has been made and finally used to dewarp the two images to a common grid. Mak-
ing both images 'Transparent' and picking two distinctly different colormaps (a blue and a red)
we can overlay them and check that they align:
In this particular case we are already looking at dark dots on a bright background so we need
not invert the "Color map and Histogram " on page?992, but we do need to adjust them so the
background becomes truly white (and thus transparent) instead of just pale red or pale blue:
PAGE | 1022
PAGE | 1023
Zooming in on the edges we can see small regions where the red and blue dots do not over-
lap perfectly, possibly due to lens distortion, but in most of the area they align nicely.
When you are satisfied with the overlap display the 'List of Layers' can be hidden via the con-
text menu (right-click inside display window), thereby releasing screen area for other displays.
To release even more space you can also hide the info box at the bottom. Via the context
menu both info box and Layer List can of course also be turned on again.
So far examples have overlaid datasets that were spatially aligned, but you can in fact overlay
datasets that are not, thereby stitching together neighboring displays:
Please note: There is no stitching of data in this example, datasets remain separate! It is only the
displays that are shown side by side to achieve visual "Stitching".
This example shows PIV?measurements in a nozzle; Velocities are fairly small in the inlet, quite
high in the throat and somewhat lower in the outlet, so three separate experiments were per-
formed with varying time between the laser pulses, adapting to the local velocities; 100μs for
the inlet, 10μs for the throat and 50μs for the outlet. In all cases full DoubleFrame images
were acquired, but subsequent PIV?analysis was set to produce vectors only in the region
where laser pulse separation was suitable for local conditions. Since the datasets were not
acquired simultaneously it makes no sense to show instantaneous vector maps side by side,
but we can show the mean flows, so above results from vector Statistics are overlaid on a back-
ground image showing the nozzle (Masking was used to remove vectors outside the nozzle).
For the purpose of this example vector colors have been modified to visualize the three
regions; Inlet vectors are Light blue, vectors in the throat are purple and finally outlet vectors
are yellow. In this particular example it is tricky to ensure comparable scaling of the mean vec-
tors from each of the three experiments, but it can be done.
With a similar approach it is possible to show images and/or vector maps from two (or more)
cameras side by side; If the upstream and downstream cameras Field of View are lined up and
the cameras synchronized to acquire images simultaneously it will again make sense to show
instantaneous datasets side by side. Assuming they share a common laser to illuminate the
flow all cameras will necessarily have the same time between pulses, so getting comparable
vectr scaling also become easier.
But again: Overlay is a visualization technique only, it does not affect the actual data in any way.
18.9 3D  Display
The 3D Display is used to display volumetric datasets using vectors, iso-surfaces, and con-
tours. The 3D data can be probed, animated, exported to other windows applications, and
viewed in different stereo rendering modes, depending on the dataset being displayed.
The 3D displays can generally display three different results:
PAGE | 1024
PAGE | 1025
l  3D velocities (resulting from 3D LSM or Volume Grid)
l  3D voxel spaces (resulting from the Voxel Reconstruction technique or Volume Math)
l  3D trajectories (resulting from 3D PTV or 3D TOMO PTV)
Two Visualisations of a Jet flow, left: Three contour slices are added at different plane normals,
while vectors are displayed but in y axis only every 15th is shown. Right: Vectors, iso-surfaces using
different color maps.
Multiple volumetric datasets can be added as layers to the "master" dataset by dragging data-
set from the database onto the active display window. All datasets will use the same coordin-
ate system, but each dataset will have separate display options.
To be able to create a 3D Display you must generate a volumetric dataset representing scalar
data in X, Y and Z dimensions. The 3D Display can display both voxels and 3D vectors.
By using the Display Options for each display (layer) respectively, visual properties can be
changed:
In case of the voxel volume, the palette and slice sub volume can be defined.
In the case of vectors, one can add iso-surfaces and place contours slices using the build-in
probe. The following will explain the different display options for either dataset type:
18.9.1 3D?Voxel Display
The voxel volume display shows the extent of the voxel volume using a semi transparent box
representing the volume or a slice of it. With a right click you can go to the Display Options and
define the sub-volume. The volume be moved freely, scaled and rotated to fit a desired
frame.
18.9.2 Interacting with  the voxel volume display
The volume slicer can be moved freely by pressing the Shift key while dragging the volume
with the left mouse button. The widget can be rotated around the normal of a selected face of
the widget by pressing the Ctrl key while dragging the mouse up or downwards on the face of
the widget. The faces of the slice volume can be moved by pressing the Altkey and dragging a
selected face of the widget. Pressing the Shift key while doing this will drag the entire volume
in the direction of the face normal.
18.9.3 The display options for voxel volumes
The display options for the voxel volume enables the user to adjust the settings for the display
for better visualization of the dataset.
PAGE | 1026
PAGE | 1027
The “Display  option” opens the voxel volume setup. Here you can set whether you want to
show a portion, a probe, set a slice or show the whole volume, and define if the grey values
should be interpolated or not, if activated the display is more responsive but slightly simplifies
the actual volume display.
The “Z-plane Intensity” plots the average intensity of all grey values in the volume along the
z-axis. This helps to judge about the reconstruction quality since in the non-illuminated parts of
the voxel space the grey values shall be 0. Note that they will rarely really show 0 because
ghost values are always introduced during the reconstruction. By a right mouse click the
numerical values of the z-intensity profile, or the diagram can be exported
The “Show probe” hides the voxels in the volume, and can also be selected from the displays
right click menu.
The “Use transparency” makes voxels more and more transparent the small the value is.
Un-checking the Use transparency makes all voxels solid.
The “Slice” widget can be reset to a known state from the drop down menu, the thickness of
the slice can be set in the text box to the left of the drop down menu. It is possible to choose
to align the slice widget with any plane spanned by the coordinate axies, or to have the widget
fit the whole volume. It is also possible to select the same option from the displays right click
menu.
In the “View” tab different fixed views can be selected and the perspective projection can be
switched on or off.
In the “Axis  and Grid” you can switch on and of a coordinate system and define a grid to it
In the “Background color” tab a color for the background can be set.
The “Animate Camera” animates the a circular camera path around the display
The “Stereo Rendering” switches between different stereo rendering views (not that
depending on your hardware some might not work)
The “Camera” lets you select for a input method how to rotate and view the volume
With “Export” the image or the animation can be saved as image or video.
“Copy  to clipboard” generates a print screen of the 3D display
“List of Layers” opens or closes the view of different layers. This automatically activates if
multiple 3D displays are overlapped
It is possible to switch between the two frames of a double frame voxel volume using the "t"
button, while having the voxel volume display selected.
Selecting "Color map and histogram" from the right click menu brings up the color map dia-
log that is similar to the color map dialog used for images
It is also possible to set the color look up table to a given palette or change the range of values
to display. Any value under the minimum value will be set to transparent and any value above
the maximum value will be clamped to the maximum palette color
It is possible to switch between the two frames of a double frame voxel volume using the "t"
button, while having the voxel volume display selected.
PAGE | 1028
PAGE | 1029
18.9.4 Images
Imaged that have been dewarped can be show in the 3D view by either right clicking on the
image and choosing 3D view or dragging the image into an existing 3D display window.
The image will be represented to scale (defined by the dewarping). Dewarped source images
can therefore be used as a reference when interpreting for example vector or voxel data.
18.9.5 Vectors
The 3D display can show both true 3D vectors and vectors derived from stereo PIV.
The default display for stereo PIV vectors is the 2D view, but right clicking on the stereo
PIV?dataset and choosing 3D view will show the data using the 3D view. Alternatively the ste-
reo PIV dataset can be dragged onto an existing 3D display.
By default volumetric data is displayed as vectors, and the length of the vectors are nor-
malized and adjusted to the size of the display. By using the Vector page in the Display Options
dialog the look and representation of the vectors can be changed.
Rejected and substituted vectors can be removed form the display by unchecking the cor-
responding boxes.
By changing the size of vectors by using the Vector size slider, they become more visible.
PAGE | 1030
PAGE | 1031
Left: A typical default vector display when a new 3D Display is opened. Right: By sizing the vectors
they become more visible.
The Index skipping option can be used to trim down the number of displayed vectors to get a
better look of the data. The display can be translated (rotated, moved and zoomed)?by drag-
ging the mouse inside the display window.
Left: The indexing option reduces the number of visible vectors allowing other displays to become
more visible. Right: By using the mouse the display can be interactively rotated, zoomed etc.
based on the camera mode.
The length of the vector will always represent the actually size of the velocity, but the color
can be changed to represent any scalar available in the dataset. By default the color rep-
resent the length of the vectors, but by changing the Scalar vector data property it can e.g
the U, V or W-component along with a number of derivatives. By changing the color map from
the default Rainbow style, the interpretation of the vectors can be made more clear.
PAGE | 1032
PAGE | 1033
Left and right: By changing the color map, information and structures in the data can be made
more visible.
For comparing vectors from multiple volumetric datasets both the vector color, size and size
can be set to fixed. The fixed vector scale factor maps the size of the vector in mm relative to
the axes to a velocity of 1 m/s.
18.9.6 Iso-surfaces
As an alternative (or supplement) to the vectors, the volumetric dataset can be displayed
using iso-surfaces. As for the vectors the color and the color map of the iso-surfaces can be
changed to represent any scalar value available in the volumetric dataset. The number and
range of iso-levels can be specified, and the iso-surfaces can be combined using the smooth-
ing and transparency options.
PAGE | 1034
PAGE | 1035
Iso-surfaces from the q-cirtera from a jet flow showing the rollup from the shear layer and the
breakup of the vertical structures. Left: raw data: right: by the smoothing feature, the surfaces
blend into each other better and show les noise.
18.9.7 Contours
As an third display option planar contour slices can be added to the display. The easiest way to
add contours slices is to use the probing feature. The probe is an interactive contour plane
that can be positioned (moved and resized) by using the mouse within the display window.
When the probe is in the correct position, it can be added as a contour slice, either by using
the display options dialog or directly from the context menu. The probe can optionally be
restricted to move in a fixed plane, or it can be moved freely. The probing feature is also avail-
able directly from the context menu of the display window.
PAGE | 1036
PAGE | 1037
Left: The probe is displayed with resizing handles in each corner and a rotation and tilt handle in
the center of the plane. Right: When converting the probe to a contour slice, the handles are
removed.
Contour slices can also be added or updated manually by selecting a fixed plane perpendicular
to one of the 3 axes, or arbitrary by typing in the coordinates of 3 of the corners defining the
plane in the display options dialog. Like the other displays the color and the color map of the
contour slices can be changed to represent any scalar value available in the volumetric data-
set. The color map used by the probe will follow the color map for contours.
Any number of contour slices can be added to the display, and vectors, iso-surfaces and con-
tour slices can be displayed separately or combined for advanced displays. During invest-
igation of your data you may need to reset the camera using the context menu, to bring all
data into the display.
18.9.8 Streamlines
The Streamline property tab is divided into four different parts. In the first you define the
number of seed points along the x-, y-, and z-axis, from these points the streamline integ-
ration starts. in the integration type, it is defined if the streamlines are integrated in positive
or negative vector direction or in both directions. in the "Streamline colors" sections the color
option is defined. here you can select the variable to use for the color coding, also the color
scheme and if needed a manual data range can be selected. in the last tab, you can switch on
or of "Arrows" as well as define their size and amount along the streamline.
PAGE | 1038
PAGE | 1039
Example of a steamline visualisation of the flow velocity from a jet flow.
18.9.9 Stereo Rendering
The 3D Display supports displaying the data in stereo rendering on the screen. There are 3 dif-
ferent modes supported:
l  Red-Blue is supported for legacy use. This format can be used for displaying stereo on a
standard monitor, but the effect is limited and the color spectrum is limited.
l  Checkerboard requires active glasses synchronized to the monitor. Select this if your hard-
ware driver runs in checkerboard 3D mode.
l  Shutterglasses requires active glasses synchronized to the monitor. Select this if your
hardware driver runs the shutterglasses mode. This mode gives the best effect and is
fully color neutral.
18.9.10 Animation
The 3D Display can be animated, automatically rotating the display to view the data from all
sides. This feature is especially informative when used together with the stereo rending fea-
ture.
18.9.11 Camera
Data can be viewed interactively using different camera modes:
l  Trackball is a motion sensitive mode where motion occurs when the mouse button is
pressed while the mouse pointer is moved. This is the default mode.
l  Joystick is a position sensitive mode where motion occurs continuously as long as the
mouse button is pressed.
l  Flight is an animated mode where you interactively fly through you data.
Use Reset view to place your data in the center of the display window.
18.9.12 Export
The default export as image and display from the main menu of DynamicStudio is extended
with a local export option. This allows for more advanced export formats besides the standard
bitmap formats. The following formats are supported:
PNG             Portable Network Graphics (PNG) bitmap format. Provides lossless
data compression, popular for Web contents.
BMP             Bitmap Image File (BMP) bitmap format. Includes all pixels uncom-
pressed. This is probably the most compatible format, but gen-
erates large files.
JPEG             JPEG (JPG, JPEG) bitmap format. Provides lossy compression gen-
erating very small images only suitable for e.g. E-mail and Web.
TIFF              Tagged Image File Format (TIF, TIFF) bitmap format. Used for sci-
entific imaging mainly.
EMF              Enhanced Meta File (EMF) vector and bitmap graphics format. Used
under Windows as the file containing calls directly to the GDI.
EPS               Encapsulated PostScript (EPS) text and vector graphics format.
Widely used and compatible with any dtp program.
VRML           Virtual Reality Modeling Language (VRML) 3D interactive vector
graphics format. This format is suitable for the Web and for back-
ward compatibility.
X3D              X3D (X3D) 3D computer graphics XML-based file format. This
format is part of international ISO- and Open standards.
18.10 XY Display
The XY display is used to display 2D column based datasets using line, scatter or bar rep-
resentation.
The XY display has several properties to ease the investigation of the dataset, all described
below.
18.10.1 Graphical user interface
There are different toggle possibilities making it easy to manage the display.
PAGE | 1040
PAGE | 1041
The following possibilities are available from the context menu shown when right-clicking
inside the display:
l  Chart type gives the possibility to show the display as either line or bar (bar chart can't
show logarithmic x-axis).
l  Zoom Out will zoom out to either dataset defined range or user defined range. This can
be set in “Display Options…”.
l  Show Legend can be toggled to show the legend.
l  Probe is a feature that makes it possible to show the x and y values for the data point
closest to the mouse or the index of the data in the dataset.
l  Show Grid is a toggle option to have a grid shown in the display.
l  Show Markers is a toggle option to show markers defined by the dataset (can for example
be mean value). Some datasets don’t have markers.
l  Info Box shows info defined by the dataset.
18.10.2 Legend
From the legend it is possible to highlight a line in the display by clicking on the line in the
legend. This can be useful if multiple lines are shown, to distinguish between the lines. To dis-
able highlighting right click on the legend and select “Deselect data”:
The selected dataset will be in red.
18.10.3   Info Box
The info box will show information (if available) from the dataset.
PAGE | 1042
PAGE | 1043
18.10.4   Zooming
There are two ways of zooming the display. The mouse can be used, by marking a rectangular
area in the display, or by scrolling the mouse wheel button1. The other way is to specify a
range in the “Display Options…” The Zoom Out from the display context menu will zoom back
to full range. The image below shows the rectangle drawn when zooming with the mouse:
18.10.5   Probe
The Probe feature can be used in two different ways. If no line has been selected then the
probe works by showing the x and y value of the plotted point closest to the mouse, dis-
regarding any particular line. If a line has been selected the probe will show values for only
that line.
18.10.6   Default setup
For a newly created XY Display the selected menu items will look like this:
1The mouse wheel button can be used in combination with the <SHIFT> or <CTRL> button, to restrict
the zooming to either horisontal og vertical direction.
18.10.7   Display Options
The display options have 4 tabs in order to simplify the user interface.
l  Data selection
l  Plot setup
l  Axis setup
l  Line style
All settings can be applied without closing the display options dialog.
18.10.8 Data  Selection
The data selection is for choosing what lines should be shown in the plot. It is only possible to
select one x-axis data source.
PAGE | 1044
PAGE | 1045
18.10.9 Plot Setup
The plot setup is for setting general plot properties. A title can be written, some datasets sup-
ply its own title, but it can also be edited. The title is a drop down control that makes it possible
to always select the default title (if supplied by the dataset). The editable colors are axis color
which include axis lines, numbers and labels. The title & legend color applies to the title and
the legend text. The plot back color applies to the background color of the entire display. The
legend back color applies to the background color of the legend area. Font type applies to all
text in the display except the info window. The currently selected type is shown as an example
beside the drop-down. The font sizes used in title, legend, probe text and axis labels can be
set individually.
18.10.10 Axis Setup
The axis setup provides the possibility to edit the axis labels the same way as the title. It is
always possible to get the default axis label from the drop-down. Here it is also possible to edit
the range that should be displayed by the plot. If the Full Range is checked then the full range
of the dataset will be used, otherwise the range specified here will be shown. If toggling the
Zoom Out item from the display context menu, the display will zoom out to the selected
range.
The origins of the axis are by default set to the border of the plot area, so that they never
interfere with the plot. This can be changed so the axis cross-section is shown at {0;0}, by
checking the Axis origin at zero checkbox. If the plot is then zoomed and the axis lies outside
the zoom area, then the axes are shown at the border of the plot area. The axis can be selec-
ted to be shown as logarithmic if necessary for the x- or the y-axis or both (bar chart can't
show logarithmic x-axis).
PAGE | 1046
PAGE | 1047
18.10.11 Line Style
The style of each line can be changed with the following parameters:
l  Line style
l  Line color
l  Line thickness
l  Point symbol
l  Symbol color
l  Symbol size
By default the first time a plot is shown, the color of the first line is black.
18.11 Numeric  Display
DynamicStudio is an imaging system and images are essentially just a collection of pixels, each
with its own grayscale value, i.e. a number. For color images each pixel has RGB-intensities,
i.e. still numbers, just 3 times as many. When PIV image pairs?are analyzed to produce vector
maps of average displacements/velocities, each vector has a position plus vector components.
For classic 2D-PIV each vector will thus have a (x,y)-position and (u,v) displacement/velocity
components, i.e. 4 numbers.
No matter how they are normally displayed all datasets are essentially a collection of numbers
and to see those numbers you can use the Numeric Display.
To see numerical values from a dataset select it in the database and then pick 'Open as Numer-
ic¨' from the Toolbar, in the Context Menu or in the File Menu:
PAGE | 1048
PAGE | 1049
The Numeric Display will show the numbers from a dataset in a spreadsheet style display with
rows and columns of cells, each with a number (or short text) in it. The exact content and lay-
out of this spreadsheet display will of course depend on the type of data shown. It is beyond
the scope of this text to describe them all, but we will show a few examples and highlight
things that are typical for the Numeric Display regardless of the data shown. Things that are
specific for a specific datatype will normally be described in the Help page for the Analysis
Method generating that data.
The Numerical Display will usually have headers explaining what is in each column.
For example Numeric Display of an image will by default look something like this (right half of
the figure below):
The leftmost (grey) column with the header # is simply a continuous number providing a
unique ID for each row. The next two columns with headers x & y are integer grid numbers,
identifying each vector's location as the x'th column and the y'th row. The next two columns
are x &?y again, but this time floating point coordinates, describing the metric location of each
pixel. In this example neighbor pixels are 0.1mm apart. The grid indices for both x &?y will
always increase in steps of one, but the metric positions may change if you move the origin or
change the Scale Factor (See "Define Axes" on page?1055 for details about this).
The last column with the header 'Pixel' contain the actual grayscale values for each pixel. In
the figure above some of the grayscale values have been highlighted in the Numeric Display
and a zoomed in Image Display, so you can see how the Numeric Display simply lists grayscale
values one row at a time starting in the bottom left corner of the image.
Specifically for images you can have the Numeric Display 'Show as Grid'; Right click somewhere
in the Numeric Display and pick 'Show as Grid' from the Context Menu:
Numeric Display changes to something like this:
Rows and column of the numeric display now match rows and columns of the image (but
upside down since the image has origin in the lower left corner, while the spreadsheet display
has origin in the top left corner). The pixels highlighted are the same as before.
For vector maps the Numeric Display typically looks something like this (again showed side by
side with a zoomed in graphical display of the same data):
PAGE | 1050
PAGE | 1051
Each row corresponds to a specific vector and like before the Numeric Display lists vectors
row by row starting in the bottom left corner. In the figure above some of the vectors have
been highlighted along with the corresponding line in the Numeric Display to illustrate this
row-by-row approach.
Again the leftmost (grey) column with the header # is simply a continuous number providing a
unique ID for each row in the table (i.e. each vector).
The next two columns are grid indices identifying vector location as the x'th column and the
y'th row in the vector map.
As implied by the names/headers x(pix) and y(pix) identify vector locations in pixel. Vectors are
located at the center of an Interrogation Area (IA), the size of which (Width & Height) is always
an even number of pixels. The center of the IA will thus always be at the corner of a pixel and
not at the center, which is why the vector pixel coordinates always ends on '.5'.
The next two columns x(mm) and y(mm) provide metric coordinates for each vector. These
depend on "Define Axes" on page?1055 settings.
The remaining columns refer to the vector components;
Upix and Vpix specify the average particle image displacements in pixels as measured on the
camera sensor.
U and V are the same, but converted to metric velocities based on known scaling and time
between the two laser pulses that illuminated frame 1 &?2 respectively. Length is the Euclidian
(metric) length of the vector, i.e. the square root of the sum of squared vector components.
The Status Code is a binary coded integer with each bit having its own distinct meaning:
l  0: Vector is valid and derived from a specific peak in a correlation map
l  1: Vector is Rejected automatically (by software)
l  2: Vector is Rejected manually (by user; -legacy option)
l  4: Vector is Outside (by masking)
l  8: Vector is Disabled (by masking)
l  16: Vector is Substituted
The first and the last are considered valid vectors the others are not. Code 1 (Rejected by
software)?is assigned by a validation routine if for example a vector differs too much from its
nearest neighbors. Code 2 (Rejected by user) is a legacy option from a time when the user
could manually click a specific vector and manually reject it. This is no longer supported and
code 2 should never be on. Status codes 4 & 8 can be assigned by applying a mask, where the
definition of the mask determine which of the codes to assign. 'Outside' is intended to deal
with walls and objects in the Field of View, while 'Disabled' is intended for areas such as shad-
ows behind objects, where the laser light does not reach and the flow present thus cannot be
measured.
In practice it makes little difference if a vector is 'Outside' or 'Disabled', it is considered invalid
in both cases. In fact most analysis methods that check the Status Code care only if a vector is
valid or not and do not distinguish between different types of 'Invalid'. Code 16 is used to
identify a vector that is considered valid, but is not derived from a specific peak in a cor-
relation map. Validation routines that compare each vector to its neighbors can for example
replace (/substitute) an invalid vector with the average of its valid neighbors.
Status codes add up so a vector can for example have status code 17=1+16, meaning that it
was first rejected by a validation routine (1) and then replaced/substituted with a more likely
vector (16).
Status codes can be inherited by derived data such as scalar map of e.g. Vorticity or they can
be assigned directly to various datasets by various Analysis Methods. They are used the same
way and mean the same in all contexts.
From the (right-click) context menu of a Numeric display you may choose to show the status
codes as text:
The result is that the numeric Status code is replaced by text such as this:
In case multiple codes are 'on' at the same time the one with the highest number is shown
(which is typically also the most important one).
Display Options for the Numeric Display depends on the data, but resembles this one:
PAGE | 1052
PAGE | 1053
Each row in the Table Setup above refers to a column in the corresponding Numeric Display.
'Unit' and 'Description' are for information only, while the user can interact with 'Quantity' and
'Decimals'. Adding or removing checks from the checkboxes you can add or remove columns
from the Numeric Display. By default they are all selected, but you can for example remove
grid and pixel position and see only metric vector informations (Press OK or Apply to see the
effect):
Under the heading 'Decimals' in the right hand side you can modify how floating point values
are shown in the Numeric Display. Default is 'Auto', but if you click either of them a small drop-
down menu will appear with possible alternatives:
'0 decimals' mean that the number will be rounded to the nearest integer and shown as such.
The options 1-6 decimals obviously show the number with that exact number of decimals,
while 'Exponential' will switch to exponential notation. 'Engineering' is also exponential nota-
tion, but with the 10-exponent always a multiple of 3 and the mantissa fixed at 5 significant
digits. The example below is the same dataset with 'Length' set to 0, 2, 4, 6 decimals and then
to 'Exponential' and 'Engineering' at the far right:
Finally you have the option to sort the Numeric Display by the values in a specific column: To
do so simply double click the header of the column by which you want the rows of the table
sorted. If you want to reverse the sort order double-click the column header once more. To
get the Numeric Display back to the original row-order right-click inside it and pick 'No sorting'
in the context menu.
PAGE | 1054
PAGE | 1055
19 Dialogs
You can get help on dialog by pressing the F1 button.
19.1 Define  Axes
19.1.1 to change the coordinate system  for a  project within  a  DynamicStudio
Database.
the "Define axes" can be reached by a right mouse click on the calibration record similarly as
the settings for "Field of View" as shown in the image below.
Ones the "Define axes" is selected, the following window will open:
In this menu, you can define your axis based on the image definition and the "Axis Definition"
drop-down menu on the right-hand side of the window. With "Origin [mm]" you can define an
offset to 0,0,0 coordinate shown in the sketch from the main window. Note to make the off-
set work properly you need also need to define the sale factor in the Field of View menu.
Please note if you have measured a scale factor by "Measure Scale Factor" below and refined
your 0,0 coordinate here, this 0,0 will also be applied to the new axis definition. Furthermore,
if you have traversed calibrations, if you define several measurements via the acquisition man-
ager, please take care to follow the specified coordinate system. Hence, it is best to already
define the coordinate system before image acquisition. see more info in the "Acquiring
images" section of the "User Interface" on page?58 description.
"Measure Scale Factor" below
19.2 Measure  Scale  Factor
19.2.1 Scaling of measurements to metric units.
The Measure Scale Factor dialog is used to determine the scale factor converting pixel units
into metric units. For information on how to create a calibration record please refer to "Cal-
ibration Images" on page?102.?
To display the dialog, right click an image ensemble inside a calibration record and select the
menu option "Measure Scale Factor..."
PAGE | 1056
PAGE | 1057
The Measure Scale Factor dialog will now appear:
If the calibration image appears dark it's possible to enhance the image by right-clicking inside
the image and selecting 'Colormap and Histogram'.
Three reference markers labeled O, A & B must be set to determine both origin and scale
factor. The markers are picked and moved around using the mouse; Point a marker, press
and hold the left mouse button while dragging the marker to the new position and release
the left mouse button. If two markers are very close to one another you can pick a specific
one by pressing O, A?or B on the keyboard while you click the left mouse button.
To position markers accurately you can zoom in on a detail. There are several ways to zoom;
Right-clicking inside the display area will open a context menu, from where a number of pre-
defined zooms can be chosen. Especially 'Fit to Window' is useful to return to a zoom where
the entire image is scaled to fit inside the dialog. You can also zoom by clicking and holding the
left mouse button while drawing a rectangle around an area of interest, or you can use the
scroll-wheel on the mouse to zoom in or out around the current mouse position. While
zoomed you can pan the image around by pressing the Ctrl-key while left-clicking and drag-
ging the mouse. Finally you can press the Alt-key to convert the mouse cursor to a magnifier
performing a temporary local zoom while you move the mouse around.
The 'O'-marker determines the location of the origin, which by default will be in the lower left
corner of the image. If (0,0) is outside the image or cannot be identified you can pick some
other reference point and specify it's nominal coordinates as the origin.
Markers 'A' & 'B' determine the scale factor. Simply position the markers at identifiable pos-
itions within the image and specify the nominal distance between the markers. This will auto-
matically compute the scale factor and show it as read-only in the upper right hand part of the
dialog.
Tip:
The precision of the calculated scale factor is proportional with the distance between ref-
erence marks A and B. A large distance will give a better precision.
Note:
The value of the scale factor represents the magnification of the lens system. Therefore the
physical size of a pixel element (pixel pitch) is needed to perform the conversion into metric
units. The formula is : unitsmm = ScaleFactor * Pixel_Pitchmm.
To leave the dialog and save the current settings press the OK -button.?
To discard any changes made select the Cancel button.
19.3 Sort
Sorting enables you to change the sorting criterions for an ensemble. The sort option is avail-
able through the context menu of an ensemble. The sort option will not be available if the
selected ensemble contains sub-ensembles or is analyzed.
By default all datasets in an ensemble are sorted after the acquired timestamp. Alternatively
it is possible to sort after a user selected dataset or by a user-defined property value. All sort-
ing options allows for descending sorting as well.
19.3.1 Sorting by Timestamp
By default sorting by timestamp is selected. The sort will look at the timestamps of all the
included datasets. Using this option is useful to restore the sorting if another sorting option
has been applied or to reorder the datasets if the ensemble was constructed by merge of two
or more ensembles.
19.3.2 Sorting by Data  Column  Value
As an advanced option it is possible to sort the ensemble using an external dataset as Selec-
tion. The only restrictions to the external dataset is that it must be generic curve data, and
that the number of data points must match the number of datasets in the ensemble to sort.
The external dataset must be made as a user selection prior to using the sort option. In the
sort option it is possible to specify the generic column name to sort by.
This sorting option is very useful for sorting an ensemble based on analyzed data. As external
datasets the Extract, Fit, and Profile analyzes are useful to define the sort order.
PAGE | 1058
PAGE | 1059
Example
This example shows how Analog Statistics and an Analog Transformation can be used as
external input to the Sort option. The Analog Transformation curve is selected as external
user input to the Sort. A duplicate is made of the original analog data, and the curve is used
for sorting. As a result the data corresponding to the smallest values are now listed on top of
the sorted list.
19.3.3 Sort by Property Value
Sorting by a property value enables you to sort an ensemble based on property inputs. Cus-
tom Properties can be dynamically added during saving data to the database or when using
the acquisition manager. The property type must be defined as a number value.
After sorting a special sort property is added to the record properties for the sorted
ensemble. The sort property contains the value used for the sorting, either from a
timestamp, data column value, or property value. The Split option uses this value to perform a
custom split, and the value can be read and used by analysis methods.
19.4 Split
Splitting enables you to split an ensemble into two or more ensembles. The split option is avail-
able through the context menu of an ensemble. The split option will not be available if the
selected ensemble contains sub-ensembles or is analyzed.
The splitting will split the ensemble either on indexing or based on the current sort order. The
current ensemble will be included in the split as the first ensemble.
19.4.1 Split at Index
By default the ensemble is indexed starting at #1. Splitting at index defines a number of split
widths corresponding to number of datasets.
The table in the dialog will show where in the current ensemble the splitting will be per-
formed.
Table contents:
"3: [1;4[ | 3" (meaning; the first split ensemble will hold 3 datasets. These datasets will be data-
set index 1, 2 and 3. Index 4 is not included in this ensemble)
"3: [4;7[ | 3" (meaning; the second split ensemble will hold 3 datasets. These datasets will data-
set be index 4, 5 and 6. Index 7 is not included in this ensemble)
"3: [7; 9]| 3" (meaning; the last split ensemble will hold 3 datasets. These datasets will be data-
set index 7, 8 and 9. Index 9 is included in this ensemble)
19.4.2 Split at Sort Property Value
The sort property value is added to the record properties of the ensemble, when an
ensemble is sorted using the sort option. Splitting at sort property value defines a number of
split widths corresponding to the range of the sort property values.
The table in the dialog will show where in the current ensemble the splitting will be per-
formed.
Table contents:
"1000: [200;1000[ | 4" (meaning; the first split ensemble will hold the first dataset up to sort
value 1000. The datasets will be datasets that has the sort value 200 to 1000. Datasets that
has the Sort value 1000 and above in not included)
"1600: [1000;1600[| 3" (meaning; the second split ensemble will hold the first dataset up to
sort value 1600. The datasets will be datasets that has the sort value 1000 to 1600. Datasets
that has the Sort value 1600 and above in not included)
2000: [1600; 2000]| 2 (meaning; the last split ensemble will hold the first dataset up to sort
value 2000 (highest sort value). The datasets will be datasets that has the sort value 1600 to
2000. )
19.4.3 Automatic
By default the split widths are calculated automatically defining a linear distribution of the
widths over the range. Using this option will ensure an equidistant and valid splitting.
19.4.4 Custom
Using the custom split widths, you can define your individual split widths in the list. In this case
you must ensure that the range is within the total range.
PAGE | 1060
PAGE | 1061
Select each of the width values in the list to set new values, press F2 to enter new split width
value.
As a help defining the split widths the total range and the current range is automatically cal-
culated and displayed. If the current split is invalid, the conflicting position will be marked with
a red color in the list. Typically error situations are when too many splits are defined, or that
some of the splits become empty.
19.5 Merge
Merging enables you to combine results from two or more ensembles into one. The merge
option is available through the context menu of multiple selected ensembles. A number of lim-
itations are applied to merging ensembles; the data in the ensembles must be of similar type
and size.
PAGE | 1062
9040U1872 This document is subject to change without notice. Copyright c 2019. Dantec Dyna-
mics. All Rights Reserved. www.dantecdynamics.com
